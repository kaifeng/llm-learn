# 检索增强生成 (RAG)

大语言模型（LLM）虽然在其训练数据中编码了海量的知识，但它们存在两个固有的、难以解决的问题：

1.  **知识陈旧**：LLM 的知识被“冻结”在其训练数据截止的那个时间点。它们无法获知此后发生的新事件或出现的新信息。
2.  **幻觉 (Hallucination)**：在被问到其知识范围之外或不确定的问题时，LLM 倾向于“编造”听起来合理但实际上是错误的答案。

为了解决这些问题，同时让 LLM 能够基于私有的、特定的知识（如企业内部文档）来回答问题，**检索增强生成（Retrieval-Augmented Generation, RAG）** 范式应运而生。

## 核心思想

RAG 的核心思想非常直观：**与其完全依赖模型内部的“记忆”，不如在回答问题之前，先让模型去一个外部知识库中“开卷查书”。**

具体来说，当用户提出一个问题时，RAG 系统并不直接将问题发送给 LLM。相反，它会执行以下步骤：

1.  **检索 (Retrieve)**：系统首先将用户的问题作为一个“查询”，在一个外部的知识库（例如，公司的 PDF 文档、网站内容、数据库）中检索最相关的信息片段。
2.  **增强 (Augment)**：然后，系统将这些检索到的相关信息片段与用户的原始问题**拼接**在一起，形成一个内容更丰富、信息更具体的**新提示（Prompt）**。
3.  **生成 (Generate)**：最后，将这个增强后的新提示发送给 LLM。LLM 会基于这个提供了明确上下文的新提示来生成最终的回答。

通过这种方式，LLM 的角色从一个“封闭的知识库”转变为一个“基于所提供材料的阅读理解和总结工具”。

## RAG 系统的典型架构

一个完整的 RAG 系统通常包含两个主要阶段：

### 1. 数据准备 / 索引阶段 (离线)

这个阶段的目标是建立一个可供快速检索的知识库。

-   **加载数据 (Load)**：从各种来源（如 PDF, HTML, Word 文档）加载原始数据。
-   **切分 (Chunk/Split)**：将长文档切分成更小的、有意义的文本块（chunks）。切分的大小和策略对 RAG 的性能至关重要。
-   **向量化 (Embed)**：使用一个**嵌入模型（Embedding Model）**（类似于 Word2Vec 或 BERT，但通常是为句子/段落优化的）将每个文本块转换成一个数值向量，即**向量嵌入（Vector Embedding）**。
-   **存储/索引 (Store/Index)**：将所有的文本块及其对应的向量嵌入存储到一个**向量数据库（Vector Database）**中。向量数据库经过特殊优化，可以根据向量之间的相似性进行极快速的检索。

### 2. 查询 / 生成阶段 (在线)

这是用户与系统交互时发生的实时过程。

-   **用户提问**：用户输入一个问题（查询）。
-   **查询向量化**：使用**同一个**嵌入模型，将用户的查询也转换成一个向量。
-   **相似性搜索**：在向量数据库中，使用这个查询向量去搜索与之最相似的文本块向量。最常用的搜索算法是**余弦相似度（Cosine Similarity）**。
-   **构建提示**：从搜索结果中选取最相关的 Top-K 个文本块（例如，前 3 个），并将它们与用户的原始问题组合成一个增强的提示。
-   **生成回答**：将增强后的提示喂给 LLM，生成最终的、基于检索到的知识的回答。

## RAG 的优势

-   **提高准确性，减少幻觉**：模型被要求基于提供的材料作答，而不是凭空捏造，从而大大减少了幻觉现象。
-   **知识实时更新**：当外部知识库更新时，只需重新对数据进行索引，而无需重新训练昂贵的 LLM。这使得知识的更新变得非常高效和廉价。
-   **可追溯性与可解释性**：由于答案是基于检索到的特定文本块生成的，我们可以很容易地将答案追溯到其信息来源，方便用户验证答案的准确性。
-   **实现私有知识问答**：企业可以将内部的、私密的文档构建成 RAG 知识库，从而创建一个能回答内部问题的安全 AI 助手，而无需将私有数据用于模型训练。

RAG 是目前将大语言模型落地到实际应用中最重要、最实用的技术之一，它有效地将 LLM 强大的语言能力与外部知识库的准确性和实时性结合了起来。