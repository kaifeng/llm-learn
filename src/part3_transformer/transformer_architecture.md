# Transformer 架构

Transformer 模型由 Google 在其里程碑式的论文《Attention Is All You Need》(2017) 中提出，它彻底改变了自然语言处理的范式。其核心贡献在于：
完全摒弃了以往模型中常见的循环（RNN）和卷积（CNN）结构，而仅仅依赖于注意力机制来捕捉输入和输出之间的全局依赖关系。

这种设计的最大优势是巨大的并行计算能力，使得模型可以在前所未有的大规模数据集上进行训练，为后来超大规模语言模型的出现奠定了基础。

## 整体架构：编码器-解码器

Transformer 的顶层架构仍然沿用了经典的 **编码器-解码器（Encoder-Decoder）** 结构。编码器负责将输入序列处理成一系列高级的、富含上下文信息的表示，解码器则利用这些表示来生成输出序列。

在原始论文中，编码器和解码器都是由 6 个完全相同的层堆叠而成。

![Transformer Architecture](https://miro.medium.com/v2/resize:fit:720/format:webp/0*Qb1YkJkJsh-OjoZg.png)

解码器的大部分结构与编码器相似，但有两点关键不同：

1.  **带掩码的多头自注意力 (Masked Multi-Head Self-Attention)**：解码器用于生成任务，必须是自回归的（auto-regressive），即在预测第 $t$ 个词时，只能依赖于已经生成的 $t-1$ 个词，不能“看到”未来的信息。因此，在解码器的自注意力层中，会使用一个“掩码（mask）”，将当前位置之后的所有位置的注意力权重设置为负无穷，这样在 Softmax 之后它们的值就变成了 0。

2.  **编码器-解码器注意力 (Encoder-Decoder Attention)**：这是解码器模块的第二个注意力层。在这一层，**Query** 来自于解码器自身的（带掩码的）自注意力层的输出，而 **Key** 和 **Value** 则来自于**编码器最终的输出**。这一步完全模拟了原始的注意力机制，即解码器在决定生成哪个词时，会回顾并关注整个输入序列。

## 其他关键组件

### 3. 残差连接与层归一化 (Add & Norm)

在每个主要组件（如多头注意力和前馈网络）之后，都跟着一个“Add & Norm”操作。

-   **残差连接（Add）**：将该组件的输入直接加到其输出上。这是一种来自计算机视觉领域的技巧（如 ResNet），它能有效防止梯度消失，使得训练非常深的网络成为可能。
-   **层归一化（Layer Normalization）**：对上一层的输出进行归一化，使数据分布更加稳定，从而加速训练过程。

### 4. 位置前馈网络 (Position-wise Feed-Forward Network)

在每个注意力层之后，都有一个简单但重要的全连接前馈网络。它对序列中的每个“位置”的向量进行相同的、独立的非线性变换，增加了模型的表达能力。

## 总结

Transformer 的设计是革命性的。它证明了仅凭注意力机制，就足以构建出性能顶尖的序列处理模型。其高度并行的特性释放了硬件的潜力，直接推动了 NLP 进入以 BERT、GPT 等大规模预训练语言模型为代表的新时代。