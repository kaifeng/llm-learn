# Transformer 架构

Transformer 模型由 Google 在其里程碑式的论文《Attention Is All You Need》(2017) 中提出，它彻底改变了自然语言处理的范式。其核心贡献在于：

> 完全摒弃了以往模型中常见的循环（Recurrence, RNN）和卷积（Convolution, CNN）结构，而**仅仅依赖于注意力机制**来捕捉输入和输出之间的全局依赖关系。

这种设计的最大优势是**巨大的并行计算能力**，使得模型可以在前所未有的大规模数据集上进行训练，为后来超大规模语言模型的出现奠定了基础。

## 整体架构：编码器-解码器

Transformer 的顶层架构仍然沿用了经典的**编码器-解码器（Encoder-Decoder）**结构。编码器负责将输入序列处理成一系列高级的、富含上下文信息的表示，解码器则利用这些表示来生成输出序列。

在原始论文中，编码器和解码器都是由 6 个完全相同的层堆叠而成。

![Transformer Architecture](https://miro.medium.com/v2/resize:fit:1400/1*hrJ_j2V2u1oPOa2rR2Q2gQ.png)
*图片来源: Google AI Blog*

## 关键组件

### 1. 输入处理：词嵌入与位置编码

-   **词嵌入（Input Embedding）**：与之前的模型一样，输入文本首先通过词嵌入层，将每个词或词元（token）转换为一个固定维度的向量。
-   **位置编码（Positional Encoding）**：自注意力机制本身无法感知序列中词语的顺序，它像一个“装着词的袋子”。为了解决这个问题，模型必须引入一种能表达单词位置信息的方式，这就是位置编码。

    它的核心思想是：为输入序列中的每个位置创建一个唯一的、固定维度的向量（位置向量），然后将这个位置向量与对应位置的单词嵌入向量**相加**，得到一个既包含语义又包含位置信息的新向量，作为模型真正的输入。

    **正弦/余弦编码公式**

    原始论文提出了一种使用不同频率的正弦（sine）和余弦（cosine）函数来生成位置向量的方案，其公式如下：
    $$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$
    $$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$
    - `pos` 是单词在序列中的位置。
    - `i` 是位置向量中的维度索引。
    - `d_model` 是嵌入维度。

    这个公式最巧妙的一点在于，它使得模型能够轻易地学习到**相对位置信息**。因为对于任何固定的偏移量 `k`，`PE(pos+k)` 都可以表示为 `PE(pos)` 的一个线性变换。这意味着模型可以学会识别“后面第 k 个词”这样的相对关系，而无论当前词的位置 `pos` 在哪里。

### 2. 多头自注意力机制 (Multi-Head Self-Attention)

这是 Transformer 的核心。它允许模型在处理一个词时，同时关注到序列中的所有其他词，从而捕捉丰富的上下文信息。

-   **自注意力（Self-Attention）**：这是注意力机制的一种特殊情况，其中查询（Query）、键（Key）和值（Value）均来自同一个地方——即上一层的输出。简单来说，序列中的每个词都会生成自己的 Q、K、V 向量，然后计算自己与其他所有词的注意力权重。

-   **多头（Multi-Head）**：与其只进行一次注意力计算，不如将 Q、K、V 向量在维度上拆分成多个“头（head）”，并让每个头并行地进行注意力计算。这允许模型在不同的表示子空间中学习到不同方面的信息（例如，一个头可能关注句法关系，另一个头可能关注语义关联）。最后，所有头的输出会被拼接起来，并通过一次线性变换得到最终结果。

### 3. 残差连接与层归一化 (Add & Norm)

在每个主要组件（如多头注意力和前馈网络）之后，都跟着一个“Add & Norm”操作。

-   **残差连接（Add）**：将该组件的输入直接加到其输出上。这是一种来自计算机视觉领域的技巧（如 ResNet），它能有效防止梯度消失，使得训练非常深的网络成为可能。
-   **层归一化（Layer Normalization）**：对上一层的输出进行归一化，使数据分布更加稳定，从而加速训练过程。

### 4. 位置前馈网络 (Position-wise Feed-Forward Network)

在每个注意力层之后，都有一个简单但重要的全连接前馈网络。它对序列中的每个“位置”的向量进行相同的、独立的非线性变换，增加了模型的表达能力。

## 解码器（Decoder）的特殊之处

解码器的大部分结构与编码器相似，但有两点关键不同：

1.  **带掩码的多头自注意力 (Masked Multi-Head Self-Attention)**：解码器用于生成任务，必须是自回归的（auto-regressive），即在预测第 $t$ 个词时，只能依赖于已经生成的 $t-1$ 个词，不能“看到”未来的信息。因此，在解码器的自注意力层中，会使用一个“掩码（mask）”，将当前位置之后的所有位置的注意力权重设置为负无穷，这样在 Softmax 之后它们的值就变成了 0。

2.  **编码器-解码器注意力 (Encoder-Decoder Attention)**：这是解码器模块的第二个注意力层。在这一层，**Query** 来自于解码器自身的（带掩码的）自注意力层的输出，而 **Key** 和 **Value** 则来自于**编码器最终的输出**。这一步完全模拟了原始的注意力机制，即解码器在决定生成哪个词时，会回顾并关注整个输入序列。

## 总结

Transformer 的设计是革命性的。它证明了仅凭注意力机制，就足以构建出性能顶尖的序列处理模型。其高度并行的特性释放了硬件的潜力，直接推动了 NLP 进入以 BERT、GPT 等大规模预训练语言模型为代表的新时代。