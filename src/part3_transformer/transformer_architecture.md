# Transformer 架构

Transformer 模型由 Google 在其里程碑式的论文《Attention Is All You Need》(2017) 中提出，它彻底改变了自然语言处理的范式。其核心贡献在于：

> 完全摒弃了以往模型中常见的循环（Recurrence, RNN）和卷积（Convolution, CNN）结构，而**仅仅依赖于注意力机制**来捕捉输入和输出之间的全局依赖关系。

这种设计的最大优势是**巨大的并行计算能力**，使得模型可以在前所未有的大规模数据集上进行训练，为后来超大规模语言模型的出现奠定了基础。

## 整体架构：编码器-解码器

Transformer 的顶层架构仍然沿用了经典的**编码器-解码器（Encoder-Decoder）**结构。编码器负责将输入序列处理成一系列高级的、富含上下文信息的表示，解码器则利用这些表示来生成输出序列。

在原始论文中，编码器和解码器都是由 6 个完全相同的层堆叠而成。

![Transformer Architecture](https://miro.medium.com/v2/resize:fit:1400/1*hrJ_j2V2u1oPOa2rR2Q2gQ.png)
*图片来源: Google AI Blog*

## 关键组件

### 1. 输入处理：词嵌入与位置编码

-   **词嵌入（Input Embedding）**：与之前的模型一样，输入文本首先通过词嵌入层，将每个词或词元（token）转换为一个固定维度的向量。
-   **位置编码（Positional Encoding）**：由于 Transformer 完全没有循环结构，它本身无法感知序列中词语的顺序。为了解决这个问题，模型引入了“位置编码”。这是一种特殊设计的向量，它被加到词嵌入向量上，为模型提供了关于词语在序列中绝对或相对位置的信息。原始论文使用了不同频率的正弦（sine）和余弦（cosine）函数来生成这种位置向量。

### 2. 多头自注意力机制 (Multi-Head Self-Attention)

这是 Transformer 的核心。它允许模型在处理一个词时，同时关注到序列中的所有其他词，从而捕捉丰富的上下文信息。

-   **自注意力（Self-Attention）**：这是注意力机制的一种特殊情况，其中查询（Query）、键（Key）和值（Value）均来自同一个地方——即上一层的输出。简单来说，序列中的每个词都会生成自己的 Q、K、V 向量，然后计算自己与其他所有词的注意力权重。

-   **多头（Multi-Head）**：与其只进行一次注意力计算，不如将 Q、K、V 向量在维度上拆分成多个“头（head）”，并让每个头并行地进行注意力计算。这允许模型在不同的表示子空间中学习到不同方面的信息（例如，一个头可能关注句法关系，另一个头可能关注语义关联）。最后，所有头的输出会被拼接起来，并通过一次线性变换得到最终结果。

### 3. 残差连接与层归一化 (Add & Norm)

在每个主要组件（如多头注意力和前馈网络）之后，都跟着一个“Add & Norm”操作。

-   **残差连接（Add）**：将该组件的输入直接加到其输出上。这是一种来自计算机视觉领域的技巧（如 ResNet），它能有效防止梯度消失，使得训练非常深的网络成为可能。
-   **层归一化（Layer Normalization）**：对上一层的输出进行归一化，使数据分布更加稳定，从而加速训练过程。

### 4. 位置前馈网络 (Position-wise Feed-Forward Network)

在每个注意力层之后，都有一个简单但重要的全连接前馈网络。它对序列中的每个“位置”的向量进行相同的、独立的非线性变换，增加了模型的表达能力。

## 解码器（Decoder）的特殊之处

解码器的大部分结构与编码器相似，但有两点关键不同：

1.  **带掩码的多头自注意力 (Masked Multi-Head Self-Attention)**：解码器用于生成任务，必须是自回归的（auto-regressive），即在预测第 $t$ 个词时，只能依赖于已经生成的 $t-1$ 个词，不能“看到”未来的信息。因此，在解码器的自注意力层中，会使用一个“掩码（mask）”，将当前位置之后的所有位置的注意力权重设置为负无穷，这样在 Softmax 之后它们的值就变成了 0。

2.  **编码器-解码器注意力 (Encoder-Decoder Attention)**：这是解码器模块的第二个注意力层。在这一层，**Query** 来自于解码器自身的（带掩码的）自注意力层的输出，而 **Key** 和 **Value** 则来自于**编码器最终的输出**。这一步完全模拟了原始的注意力机制，即解码器在决定生成哪个词时，会回顾并关注整个输入序列。

## 总结

Transformer 的设计是革命性的。它证明了仅凭注意力机制，就足以构建出性能顶尖的序列处理模型。其高度并行的特性释放了硬件的潜力，直接推动了 NLP 进入以 BERT、GPT 等大规模预训练语言模型为代表的新时代。