# 状态空间模型 (State Space Models)

尽管 Transformer 架构在 LLM 领域取得了巨大成功，但其核心的自注意力机制存在一个显著的缺点：计算和内存的复杂度会随着序列长度的增加而呈二次方增长。这使得它在处理超长序列时效率低下。

状态空间模型（State Space Models, SSMs）作为一种新兴的序列模型架构，旨在解决这一问题，它在保持强大性能的同时，实现了更高的计算效率。

## Mamba

Mamba 是 SSM 架构的杰出代表，它通过引入选择性机制和硬件感知的算法设计，实现了对长序列数据的高效处理。

-   **核心思想**：
    1.  **选择性 (Selectivity)**：Mamba 能够根据输入内容动态地决定哪些信息是重要的，应该被保留在状态中；哪些信息是次要的，可以被忽略。这使得模型能更灵活地压缩和记忆序列信息。
    2.  **线性复杂度**：与 Transformer 不同，Mamba 的计算复杂度随序列长度呈线性增长。这意味着在处理长文本、高分辨率图像或音频等长序列数据时，Mamba 相比 Transformer 具有显著的速度和内存优势。

由于其独特的架构优势，Mamba 及其后续变体被认为是构建下一代高效大语言模型的有力竞争者。