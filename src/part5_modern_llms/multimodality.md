# 多模态大模型 (Multimodal LLMs)

当前的大语言模型正在超越纯文本的限制，发展出理解和处理多种信息类型（模态）的能力，这就是多模态。这使得模型能像人类一样，通过看图、听声等方式来感知和理解世界。

## 视觉语言模型 (Vision-Language Models)

视觉语言模型是多模态领域最主要的发展方向，它旨在让 LLM 具备强大的图像理解能力。

-   **核心思想**：其核心架构通常是将一个强大的 **图像编码器 (Image Encoder)** 与一个预训练好的大语言模型连接起来。
    1.  图像编码器（如 ViT, Vision Transformer）负责将输入的图像转换成一系列的特征向量，这些向量可以被看作是描述图像内容的“图像词元 (Image Tokens)”。
    2.  这些“图像词元”与用户的文本提示词元一起被送入 LLM。
    3.  LLM 在其统一的表示空间中同时处理这两种模态的信息，从而实现对图像内容的理解，并生成相关的文本描述、回答或分析。

-   **代表模型**：
    -   **GPT-4V(ision)**：展现了强大的零样本图像理解能力，能够完成复杂的视觉推理任务。
    -   **LLaVA (Large Language and Vision Assistant)**：一个著名的开源项目，通过在图像-文本对数据上进行指令微调，将 ViT 和 Vicuna (一个 LLM) 连接起来。