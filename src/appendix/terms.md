# 术语

**生成式AI（Genrative AI）**
机器产生复杂有结构的物件，尽乎无法穷举。如文章、图像、语音等。不是生成式AI：分类（如垃圾邮件分类、猫狗分类）。

**多模态（multi modality）**
模态指信息的类型或格式，如文本、图像、音频、视频等。多模态指 AI 模型能够同时理解、处理和关联多种不同模态信息的能力。GPT-4V 就是一个典型的多模态模型。

**AR(Autoregressive Generation)**
自回归生成。这是一种生成序列数据（如文本）的方式，即逐个生成序列中的元素（token），并且每生成一个新的元素，都要依赖于所有在它之前已经生成的元素。这就像我们写句子一样，下一个词总是基于前面已经写好的内容。GPT 系列模型就是典型的自回归模型。这种方式生成的文本质量高、连贯性好，但缺点是速度较慢，因为必须串行生成。

**NAR(Non-Autoregressive Generation)**
非自回归生成。与自回归相反，这种方式试图一次性或并行地生成整个序列的所有元素，而不是逐个生成。例如，模型可能先预测目标句子的长度，然后同时填充所有位置的词语。这种方式生成速度极快，但通常会牺牲一定的文本质量和连贯性，因为它在预测某个位置的词时，并不知道其他位置的词是什么。

**ChatGPT(Generative Pre-trained Transformer)**
由 OpenAI 开发的著名对话式 AI 模型。它本身属于“生成式”、“预训练”、“Transformer”模型，其核心架构是“仅解码器（Decoder-only）”，因此它的生成方式是“自回归（AR）”的。ChatGPT 的革命性之处不仅在于其巨大的模型规模，更在于它通过“指令微调”和“基于人类反馈的强化学习（RLHF）”等对齐技术，使其能更好地理解人类意图并进行流畅、有帮助的对话。同类：Google Bard, Anthropic Claude

**Semi-Supervised Learning (SSL)**
半监督学习，一种机器学习范式，它结合了少量有标签数据和大量无标签数据进行训练。当获取大量标注数据成本高昂时，半监督学习能够利用易于获取的无标签数据来提升模型的性能和泛化能力。常见的技术包括自训练、协同训练和一致性正则化等。

**Transformer**
一种著名的深度学习模型，它最初是作为机器翻译的序列到序列模型提出的。后来，基于Transformer的预训练模型在各种自然语言处理任务上实现了最优性能，因此Transformer已经成为NLP中的主流架构。
Transformer模型主要包括两部分：Encoder和Decoder。其中，Encoder负责编码输入序列，而Decoder负责生成输出序列。在编码过程中，每个词首先被转换为向量表示，然后通过多层的Encoder逐步传递信息，形成编码后的表示向量。在解码过程中，Decoder通过将编码后的表示向量与目标序列逐词匹配，生成输出序列。Transformer的核心技术是self-attention，它通过计算输入序列中不同位置之间的相关性，得到每个单词的权重，从而更好地捕捉输入序列中的重要信息。

**全连接层（Fully Connected Layer）**
简称 FC 层，也常被称为密集层（Dense Layer），是神经网络中最基本的一种层。在该层中，每一个输入神经元都与该层的所有输出神经元相连接。每个连接都有一个独立的权重。全连接层执行的操作本质上是一个线性变换（矩阵乘法），通常后面会跟着一个激活函数引入非线性。

**前馈神经网络（Feedforward Neural Network, FNN）**
是一种最基础的神经网络架构类型。它的特点是信息在网络中单向流动，从输入层经过一个或多个隐藏层，最终到达输出层，没有循环或反馈连接。一个典型的前馈神经网络通常由多个全连接层堆叠而成，层与层之间通过激活函数连接。

**多层感知机(Multi-Layer Perceptron, MLP)**
是前馈神经网络的一种具体实现，也是最基础和最经典的深度神经网络形态。当我们将输入层、至少一个隐藏层和输出层堆叠在一起，并且层与层之间的神经元是全连接的，这种网络结构就被称为 MLP。全连接神经网络（FNN）通常指的就是 MLP。它是后续更复杂的网络结构（如 CNN, RNN）的基础。
