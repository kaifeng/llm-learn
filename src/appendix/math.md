# 数学基础

理解机器学习和深度学习模型的工作原理，离不开扎实的数学基础。以下是几个最重要的数学领域及其在机器学习中的应用。

## 1. 线性代数 (Linear Algebra)

线性代数是机器学习的语言，几乎所有的数据和模型操作都可以用线性代数的概念来表达。

-   **核心概念**：
    -   **标量 (Scalar)**：单个数值。
    -   **向量 (Vector)**：一维数组，表示空间中的一个点或方向。在机器学习中，一个样本的特征通常表示为一个向量。
    -   **矩阵 (Matrix)**：二维数组，由行和列组成。数据集通常表示为一个矩阵，神经网络的权重也表示为矩阵。
    -   **张量 (Tensor)**：多维数组，是向量和矩阵的推广。深度学习中，数据和模型参数通常以张量的形式存在。
-   **基本运算**：
    -   **加法与减法**：向量和矩阵的对应元素相加减。
    -   **乘法**：
        -   **标量乘法**：标量与向量/矩阵的每个元素相乘。
        -   **点积 (Dot Product)**：两个向量的对应元素相乘再求和，结果是标量。用于计算相似度、投影等。
        -   **矩阵乘法**：矩阵之间按照特定规则相乘，结果是矩阵。神经网络中层与层之间的计算核心。
    -   **转置 (Transpose)**：矩阵的行和列互换。
    -   **逆 (Inverse)**：矩阵的逆用于求解线性方程组。
-   **应用**：数据表示、特征工程、主成分分析（PCA）、神经网络的层运算、损失函数计算。

## 2. 微积分 (Calculus)

微积分是优化机器学习模型（尤其是深度学习模型）的关键工具，它告诉我们如何调整模型参数以最小化误差。

-   **核心概念**：
    -   **导数 (Derivative)**：衡量函数在某一点的变化率。在机器学习中，用于计算损失函数对单个参数的影响。
    -   **偏导数 (Partial Derivative)**：衡量多变量函数对其中一个变量的变化率，而保持其他变量不变。
    -   **梯度 (Gradient)**：由一个函数对所有自变量的偏导数组成的向量。梯度指向函数值增长最快的方向。在机器学习中，我们通常沿着梯度的反方向调整参数以最小化损失函数。
    -   **链式法则 (Chain Rule)**：用于计算复合函数的导数。在神经网络中，这是**反向传播（Backpropagation）**算法的核心，用于高效计算每一层参数的梯度。
-   **应用**：优化算法（如梯度下降）、反向传播、损失函数最小化。

## 3. 概率论与统计 (Probability and Statistics)

概率论和统计学提供了处理不确定性、理解数据分布和评估模型性能的工具。

-   **核心概念**：
    -   **概率 (Probability)**：事件发生的可能性。
    -   **随机变量 (Random Variable)**：其取值是随机事件结果的变量。
    -   **概率分布 (Probability Distribution)**：描述随机变量所有可能取值及其对应概率的函数。常见的有正态分布（高斯分布）、伯努利分布等。
    -   **期望 (Expectation)**：随机变量的平均值。
    -   **方差 (Variance)**：衡量随机变量与其期望值之间的离散程度。
    -   **协方差 (Covariance)**：衡量两个随机变量的联合变化程度。
-   **重要定理**：
    -   **大数定律 (Law of Large Numbers)**：随着样本数量的增加，样本均值会趋近于总体均值。
    -   **中心极限定理 (Central Limit Theorem)**：在适当条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。
    -   **贝叶斯定理 (Bayes' Theorem)**：用于更新事件的概率，是贝叶斯机器学习方法的基础。
-   **应用**：数据分析、模型评估（如准确率、召回率）、损失函数设计（如交叉熵）、贝叶斯推断、生成模型。

## 4. 优化理论 (Optimization Theory)

优化理论提供了寻找函数最小值（或最大值）的方法，这在机器学习中对应于寻找使模型损失最小化的参数。

-   **核心概念**：
    -   **目标函数 (Objective Function)**：在机器学习中通常指**损失函数（Loss Function）**，我们希望最小化它。
    -   **梯度下降 (Gradient Descent)**：最基本的优化算法，通过沿着损失函数梯度的反方向迭代调整参数来寻找最小值。
    -   **学习率 (Learning Rate)**：控制梯度下降每一步的步长。
    -   **收敛 (Convergence)**：优化算法达到或接近最优解的状态。
-   **应用**：训练神经网络、支持向量机等各种机器学习模型，寻找模型参数的最优解。