# 神经网络

神经网络（Neural Networks）是深度学习的核心构建块。神经网络的基本计算单元是神经元（或称为节点）。神经元接收来自其他神经元或外部数据源的多个输入信号（$x_1, x_2, ..., x_n$）。每个输入信号都有一个关联的**权重（weight, $w_i$）**，代表该输入的重要性。

神经元将所有输入的加权值相加，并加上一个**偏置（bias, $b$）**。偏置允许我们微调输出，可以看作是激活函数的触发阈值。

$$ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b = \sum_{i=1}^{n} w_ix_i + b $$

加权求和的结果会被送入一个激活函数得到该神经元的最终输出 $ a = f(z) $。激活函数的作用是向网络中引入非线性，否则神经网络最终输出都只是输入的线性组合，无法学习复杂的数据模式。

![神经网络结构](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png)

一个典型的神经网络由输入层、隐藏层和输出层组成。

**输入层（Input Layer）** 接收原始数据。该层的神经元数量通常等于数据特征的数量。

**隐藏层（Hidden Layers）** 位于输入层和输出层之间，负责提取数据中的各种特征。一个神经网络可以没有隐藏层（如逻辑回归），也可以有一个或多个隐藏层。拥有多个隐藏层的网络通常被称为“深度神经网络”（Deep Neural Networks）。

**输出层（Output Layer）** 产生最终的预测结果。该层的神经元数量和激活函数取决于具体的任务。回归任务通常只有一个输出神经元，且不使用或使用线性激活函数。
分类任务的输出神经元的数量等于类别的数量，通常使用 Softmax 将输出转换为概率分布。

神经网络的学习过程，即 **训练 (Training)**，本质上是一个寻找最优参数（权重和偏置）的迭代优化过程。其目标是让模型对训练数据的预测结果与真实标签之间的损失最小化。

这个过程的核心是 **梯度下降 (Gradient Descent)** 算法，它包含前向传播、计算损失、反向传播和参数更新等一系列步骤。训练完成后，模型即可用于 **推理 (Inference)**，对新数据进行预测。

## 激活函数

激活函数被应用于神经元的输出，为其引入非线性。如果没有非线性激活函数，无论神经网络有多少层，其本质上都只是一个线性模型，无法学习和表示复杂的数据模式。

- **Sigmoid** 函数将任意实数输入压缩到 (0, 1) 的范围内，可以被解释为概率，常用于二分类问题的输出层或作为门控机制（如 LSTM 中的门）。

  $$ f(x) = \frac{1}{1 + e^{-x}} $$

  但是当输入值非常大或非常小时，函数的梯度会趋近于0，在反向传播过程中，会导致深层网络的梯度信号变得非常微弱，使得网络难以训练。

- **Tanh** (双曲正切) 函数可以看作是 Sigmoid 函数的一个缩放和移位版本，它将输入压缩到 (-1, 1) 的范围内。通常比 Sigmoid 函数有更好的性能，收敛速度更快，但仍然存在梯度消失的问题。

  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

- **ReLU** 是目前在深度学习的隐藏层中最常用的激活函数，只涉及一个简单的阈值操作。

  $$ f(x) = \max(0, x) $$

  当输入为正数时，其梯度恒为 1，这使得梯度能够很好地在网络中传播。但是，如果一个神经元的输入在训练过程中持续为负，那么它的梯度将永远为0，导致该神经元的权重无法再被更新。

- **Leaky ReLU** 是对 ReLU 的改进，它允许在输入为负时也有一个小的、非零的梯度。$\alpha$ 是一个很小的常数（如 0.01）。

  $$ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$

- **Softmax** 的作用是将一个包含任意实数值的向量（称为 logits）转换为一个概率分布。每个元素的值都在 (0, 1) 之间，并且所有元素的总和为 1。
  这使得模型的输出可以直接解释为输入样本属于每一个类别的概率，专门用于多分类问题的输出层。

  $$ \sigma_i = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}, \quad i=1,2,...,n $$





