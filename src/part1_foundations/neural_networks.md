# 神经网络基础

神经网络（Neural Networks, NN），又称人工神经网络（Artificial Neural Networks, ANN），是深度学习的核心构建块。其设计灵感来源于人脑的神经元结构，旨在模仿大脑处理信息和识别模式的方式。

## 基本单元：神经元（Neuron）

神经网络的基本计算单元是**神经元**（或称为“节点”）。一个简化的神经元模型如下：

1.  **接收输入**：神经元接收来自其他神经元或外部数据源的多个输入信号（$x_1, x_2, ..., x_n$）。

2.  **加权求和**：每个输入信号都有一个关联的**权重（weight, $w_i$）**，代表该输入的重要性。神经元将所有输入的加权值相加，并加上一个**偏置（bias, $b$）**。偏置允许我们微调输出，可以看作是激活函数的触发阈值。

    $$ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b = \sum_{i=1}^{n} w_ix_i + b $$

3.  **激活函数**：加权求和的结果 $z$ 会被送入一个非线性的**激活函数（activation function, $f$）**中，得到该神经元的最终输出 $a$。

    $$ a = f(z) $$

    激活函数的关键作用是向网络中引入**非线性**。如果没有非线性激活函数，无论神经网络有多少层，其最终输出都只是输入的线性组合，无法学习复杂的数据模式。常见的激活函数有 Sigmoid、ReLU、Tanh 等。

## 神经网络的层次结构

单个神经元的能力有限，但将它们组织成层次结构，就能构建出强大的神经网络。一个典型的神经网络由以下几层组成：

-   **输入层（Input Layer）**：接收原始数据。该层的神经元数量通常等于数据特征的数量（例如，如果输入一张 28x28 像素的图片，输入层可能有 784 个神经元）。

-   **隐藏层（Hidden Layers）**：位于输入层和输出层之间。这些层负责提取数据中的各种特征。一个神经网络可以没有隐藏层（如逻辑回归），也可以有一个或多个隐藏层。拥有多个隐藏层的网络通常被称为“深度神经网络”（Deep Neural Networks）。

-   **输出层（Output Layer）**：产生最终的预测结果。该层的神经元数量和激活函数取决于具体的任务。
    -   **回归任务**：通常只有一个输出神经元，且不使用或使用线性激活函数。
    -   **分类任务**：输出神经元的数量等于类别的数量。例如，在进行手写数字识别（0-9）时，输出层会有 10 个神经元，并通常使用 Softmax 函数将输出转换为概率分布。

## 多层感知机 (Multi-Layer Perceptron, MLP)

当我们将上述的输入层、至少一个隐藏层和输出层堆叠在一起，并且层与层之间的神经元是**全连接（Fully-Connected）**的（即前一层中的每个神经元都与后一层中的每个神经元相连），这种网络结构就被称为**多层感知机（MLP）**。

MLP 是最基础也是最经典的深度神经网络形态。我们通常所说的“全连接神经网络（FNN）”指的就是 MLP。它是后续更复杂的网络结构（如 CNN, RNN）的基础。

## 神经网络如何学习？

神经网络的学习过程本质上是一个**参数优化**的过程，目标是找到一组最佳的权重和偏置，使得网络对给定输入的预测结果与真实标签之间的**损失（loss）**最小。

这个过程通常通过一个称为**梯度下降（Gradient Descent）**的算法来迭代完成：

1.  **前向传播（Forward Propagation）**：将一批训练数据输入网络，数据从输入层流向输出层，最终得到预测结果。

2.  **计算损失（Loss Calculation）**：使用一个**损失函数**（如均方误差或交叉熵）来量化预测结果与真实标签之间的差距。

3.  **反向传播（Backpropagation）**：这是学习过程的核心。算法会从输出层开始，反向计算损失函数相对于网络中每一个权重和偏置的梯度（偏导数）。这个梯度指明了参数调整的方向，以最快地减小损失。

4.  **参数更新（Weight Update）**：使用优化器（如 Adam、SGD）根据计算出的梯度来更新网络中的所有权重和偏置。

这个“前向传播 -> 计算损失 -> 反向传播 -> 参数更新”的循环会重复进行，直到网络的性能达到满意的水平。
