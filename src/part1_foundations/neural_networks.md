# 神经网络基础

神经网络（Neural Networks, NN），又称人工神经网络（Artificial Neural Networks, ANN），是深度学习的核心构建块。

## 基本单元：神经元（Neuron）

神经网络的基本计算单元是**神经元**（或称为“节点”）。一个简化的神经元模型如下：

1.  **接收输入**：神经元接收来自其他神经元或外部数据源的多个输入信号（$x_1, x_2, ..., x_n$）。

2.  **加权求和**：每个输入信号都有一个关联的**权重（weight, $w_i$）**，代表该输入的重要性。神经元将所有输入的加权值相加，并加上一个**偏置（bias, $b$）**。偏置允许我们微调输出，可以看作是激活函数的触发阈值。

    $$ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b = \sum_{i=1}^{n} w_ix_i + b $$

3.  **激活函数**：加权求和的结果 $z$ 会被送入一个非线性的 **激活函数** 得到该神经元的最终输出 $$ a = f(z) $$。

    激活函数的关键作用是向网络中引入**非线性**。如果没有非线性激活函数，无论神经网络有多少层，最终输出都只是输入的线性组合，无法学习复杂的数据模式。

## 神经网络的层次结构

单个神经元的能力有限，但将它们组织成层次结构就能构建出强大的神经网络。一个典型的神经网络由以下几层组成：

-   **输入层（Input Layer）**：接收原始数据。该层的神经元数量通常等于数据特征的数量。

-   **隐藏层（Hidden Layers）**：位于输入层和输出层之间。这些层负责提取数据中的各种特征。一个神经网络可以没有隐藏层（如逻辑回归），也可以有一个或多个隐藏层。拥有多个隐藏层的网络通常被称为“深度神经网络”（Deep Neural Networks）。

-   **输出层（Output Layer）**：产生最终的预测结果。该层的神经元数量和激活函数取决于具体的任务。
    -   **回归任务**：通常只有一个输出神经元，且不使用或使用线性激活函数。
    -   **分类任务**：输出神经元的数量等于类别的数量。例如，在进行手写数字识别（0-9）时，输出层会有 10 个神经元，通常使用 Softmax 将输出转换为概率分布。

## 全连接层

**全连接层**（Fully Connected Layer，简称 FC 层），也常被称为密集层（Dense Layer），是神经网络中最基本的一种层。在该层中，每一个输入神经元都与该层的所有输出神经元相连接。每个连接都有一个独立的权重。全连接层执行的操作本质上是一个线性变换（矩阵乘法），通常后面会跟着一个激活函数引入非线性。

## 前馈神经网络

**前馈神经网络**（Feedforward Neural Network，简称 FFN），是一种最基础的神经网络架构类型。它的特点是信息在网络中单向流动，从输入层经过一个或多个隐藏层，最终到达输出层，没有循环或反馈连接。一个典型的前馈神经网络通常由多个全连接层堆叠而成，层与层之间通过激活函数连接。

## 多层感知机

**多层感知机** (Multi-Layer Perceptron, MLP) 是前馈神经网络的一种具体实现，也是最基础和最经典的深度神经网络形态。当我们将输入层、至少一个隐藏层和输出层堆叠在一起，并且层与层之间的神经元是全连接的，这种网络结构就被称为 MLP。全连接神经网络（FNN）通常指的就是 MLP。它是后续更复杂的网络结构（如 CNN, RNN）的基础。

## 神经网络如何学习

神经网络的学习过程本质上是一个**参数优化**的过程，目标是找到一组最佳的权重和偏置，使得网络对给定输入的预测结果与真实标签之间的 **损失（loss）** 最小。

这个过程通常通过一个称为 **梯度下降（Gradient Descent）** 的算法来迭代完成：

1.  **前向传播（Forward Propagation）**：将一批训练数据输入网络，数据从输入层流向输出层，最终得到预测结果。

2.  **计算损失（Loss Calculation）**：使用一个**损失函数**（如均方误差或交叉熵）来量化预测结果与真实标签之间的差距。

3.  **反向传播（Backpropagation）**：这是学习过程的核心。算法会从输出层开始，反向计算损失函数相对于网络中每一个权重和偏置的梯度（偏导数）。这个梯度指明了参数调整的方向，以最快地减小损失。

4.  **参数更新（Weight Update）**：使用优化器（如 Adam、SGD）根据计算出的梯度来更新网络中的所有权重和偏置。

这个“前向传播 -> 计算损失 -> 反向传播 -> 参数更新”的循环会重复进行，直到网络的性能达到满意的水平。

### 训练的关键概念：Epoch, Batch, Iteration

- **Batch (批次)**：由于整个数据集可能非常大，一次性将其全部加载到内存中并计算梯度是不现实的。因此通常将数据集分成若干个小的子集，这些子集被称为批次。
- **Iteration (迭代)**：指完成一次批次数据的前向传播、反向传播和参数更新的过程。这是参数更新的最小单位。
- **Epoch (轮次)**：指整个训练数据集中的所有样本都已经被模型“过”了一遍。例如，如果一个数据集有 1000 个样本，批次大小（Batch Size）为 100，那么一个 Epoch 就包含 10 次迭代（1000 / 100 = 10）。

完整的训练过程是：在多个 Epoch 中，每个 Epoch 都包含多次迭代，每次迭代处理一个 Batch 的数据来更新模型的权重。

## 训练与推理

神经网络的生命周期主要有两个阶段：训练和推理。
训练过程是从数据中学习，找到最佳的模型参数（权重和偏置）。
推理过程则是使用训练好的模型对新数据进行预测。

| 特征 | 训练 | 推理 |
| :--- | :--- | :--- |
| **过程** | 包括前向传播、计算损失、反向传播和参数更新。 | 只有前向传播。 |
| **数据** | 需要大量的**有标签**训练数据。 | 处理无标签的新数据。 |
| **计算成本**| 非常高，需要大量的计算资源和时间。 | 相对较低，速度快。 |
| **模型状态**| 模型参数在每次迭代中都会被修改。 | 模型参数是固定不变的。 |
| **特定技术**| Dropout、Batch Normalization 等技术处于“训练模式”，以帮助模型泛化。 | Dropout 被关闭，Batch Normalization 使用固定的统计值。 |
