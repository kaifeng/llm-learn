# 过拟合、欠拟合与正则化

在训练机器学习模型时，我们的目标是让模型不仅在训练数据上表现良好，更重要的是在未见过的、新的测试数据上同样表现出色。这种在新数据上的表现能力被称为模型的**泛化能力（Generalization）**。而过拟合与欠拟合是影响模型泛化能力的两种最常见的问题。

## 1. 欠拟合 (Underfitting)

-   **现象**：模型在训练集和测试集上的表现都非常差。
-   **原因**：通常是因为模型过于简单，无法捕捉到数据中复杂的模式和规律。
-   **解决方法**：增加模型复杂度（例如增加神经网络的层数或神经元数量）、增加模型的训练轮次等。

## 2. 过拟合 (Overfitting)

-   **现象**：模型在训练集上表现极好，甚至接近 100% 准确，但在测试集上表现却很差。
-   **原因**：模型过于复杂，以至于它把训练数据中的**噪声和随机波动**也当作规律学习了进去，而不是学习数据背后真正的、普适的模式。
-   **解决方法**：降低模型复杂度，或采用 **正则化（Regularization）** 等技术。

## 3. 正则化 (Regularization)

**正则化**是一系列旨在**防止模型过拟合、提升其泛化能力**的技术的总称。它的核心思想是在优化模型的损失函数时，增加一个对模型复杂度进行惩罚的“惩罚项”。

### 为什么需要正则化？

-   **控制模型复杂度**：在优化目标函数时，不仅要考虑模型在训练数据上的误差（损失），还要额外加入一个“惩罚项”，这个惩罚项用来限制模型的复杂度。
-   **提升泛化能力**：通过限制模型复杂度，正则化迫使模型去学习更普遍、更具泛化性的特征，从而提高其在测试数据上的表现。

### 常见的正则化方法

-   **L1 和 L2 正则化**：这两种方法通过在损失函数中增加一个与模型参数（权重）相关的惩罚项来限制权重的大小。
    -   **L2 正则化 (Ridge)**: 惩罚项是所有权重平方和的一半。它会使所有权重趋向于接近 0，但不会完全变为 0。在梯度下降更新时，L2 正则化项会导致权重在每次更新时都减去一个与自身大小成正比的量，这个过程被称为**权重衰退（Weight Decay）**。因此，L2 正则化和权重衰退在实践中常常被认为是等价的。
    -   **L1 正则化 (Lasso)**: 惩罚项是所有权重绝对值的和。它能将一些不重要的特征的权重直接变为 0，从而实现**特征选择**。
-   **Dropout**：在训练过程中，**随机**地“丢弃”一部分神经元（即将其输出暂时设为 0）。这迫使网络不能依赖于任何一个特定的神经元，从而防止了神经元之间的复杂协同适应，增强了模型的鲁棒性。
-   **数据增强 (Data Augmentation)**：通过对训练数据进行一些变换（如旋转、裁剪、翻转等）来扩充数据集，使得模型能接触到更多样的样本，从而提升泛化能力。

## 4. 归一化 (Normalization)

**归一化**是一种数据预处理技术，其主要目的是将输入数据调整到一个统一的尺度（scale），这通常意味着将数据转换到特定的范围（如 $[0, 1]$ 或 $[-1, 1]$），或者使其均值为 0、方差为 1。

### 为什么需要归一化？

-   **加速训练**：当不同特征的数值范围相差很大时，梯度下降算法的收敛速度会非常慢。归一化可以使数据空间变得更“圆”，使得梯度下降能更直接地找到最优解。
-   **防止梯度爆炸/消失**：在某些激活函数中，输入值过大或过小会导致梯度变得非常小。归一化可以确保输入在激活函数的敏感区域内。
-   **提高模型精度**：有些算法对特征的尺度非常敏感。归一化可以确保所有特征在训练过程中都得到平等的对待。

### 常见的归一化方法

-   **最小-最大归一化 (Min-Max Normalization)**：将数据线性地缩放到一个固定的区间，通常是 $[0, 1]$。
    $$ x_{new} = \frac{x - x_{min}}{x_{max} - x_{min}} $$
-   **Z-Score 归一化 (Standardization)**：将数据转换为均值为 0、标准差为 1 的分布。这是最常用的方法之一。
    $$ x_{new} = \frac{x - \mu}{\sigma} $$

## 5. 归一化与正则化的区别总结

| 特性 | 归一化 (Normalization) | 正则化 (Regularization) |
| :--- | :--- | :--- |
| **目的** | 改变数据尺度，加速模型收敛，提高训练稳定性。 | 防止模型过拟合，提升模型的泛化能力。 |
| **作用对象** | 输入数据（特征）。 | 模型参数（权重）和/或网络结构。 |
| **阶段** | 模型训练前的数据预处理阶段。 | 模型训练过程中的优化阶段。 |
| **例子** | Min-Max 归一化、Z-Score 归一化。 | L1/L2 正则化、Dropout、数据增强。 |

简而言之，**归一化是关于“数据”的预处理，它让数据变得更利于模型学习；而正则化是关于“模型”的优化，它让模型变得更不容易过拟合。**