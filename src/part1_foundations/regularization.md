# 正则化

正则化是一系列旨在防止模型过拟合、提升其泛化能力的技术的总称。它的核心思想是在优化模型的损失函数时，增加一个对模型复杂度进行惩罚的惩罚项。
这个惩罚项用来限制模型的复杂度，通过限制模型复杂度迫使模型去学习更普遍、更具泛化性的特征，从而提高在测试数据上的表现。

常见的正则化方法有：

- **L1 和 L2 正则化**：这两种方法通过在损失函数中增加一个与模型参数（权重）相关的惩罚项来限制权重的大小。
    - **L2 正则化 (Ridge)**: 惩罚项是所有权重平方和的一半。它会使所有权重趋向于接近 0，但不会完全变为 0。在梯度下降更新时，L2 正则化项会导致权重在每次更新时都减去一个与自身大小成正比的量，这个过程被称为 **权重衰退（Weight Decay）**。因此，L2 正则化和权重衰退在实践中常常被认为是等价的。
    - **L1 正则化 (Lasso)**: 惩罚项是所有权重绝对值的和。它能将一些不重要的特征的权重直接变为 0，从而实现 **特征选择**。
- **Dropout**：在训练过程中，**随机**地“丢弃”一部分神经元（即将其输出暂时设为 0）。这迫使网络不能依赖于任何一个特定的神经元，从而防止了神经元之间的复杂协同适应，增强了模型的鲁棒性。
- **数据增强 (Data Augmentation)**：通过对训练数据进行一些变换（如旋转、裁剪、翻转等）来扩充数据集，使得模型能接触到更多样的样本，从而提升泛化能力。

## 权重衰退（weight decay）

是常见的一种避免过拟合的方法，使用均方范数作为硬性限制

$min \; l(w,b) \quad subject \: to \quad ||w||^2 \le \theta$

优化比较麻烦，一般不使用这种方法。常用的是使用均方范数作为柔性限制，对等。

$min \; l(w,b)+\frac{\lambda}{2}||w||^2$

参数更新法则

计算梯度：

$\frac{\partial}{\partial \mathbf{w}} \left( \ell(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2 \right) = \frac{\partial \ell(\mathbf{w}, b)}{\partial \mathbf{w}} + \lambda \mathbf{w}$

时间t更新参数：

$\mathbf{w}_{t+1} = (1 - \eta \lambda) \mathbf{w}_t - \eta \frac{\partial \ell(\mathbf{w}_t, b_t)}{\partial \mathbf{w}_t}$

通常 $\eta\lambda < 1$，在深度学习中通常叫叫做权重衰退。

## 丢弃法

在层之间加入噪音。通常将丢弃法作用在隐藏全连接层的输出上。

对x加入噪音得到$x'$，希望 $E[x']=x$

丢弃法对每个元素进行如下扰动

$x'_i=
\begin{cases}
0 & with \, probability \, p \\
\frac{x_i}{1-p} & otherwise
\end{cases}$

$E[x'_i] = p*0+(1-p)\frac{x_i}{1-p} = x_i$

丢弃只用在训练过程，推理过程中dropout直接返回输入，保证确定性的输出。

dropout主要用在全连接层。权重衰退使用的范围更多。drop比较好调参，常用0.1, 0.9, 0.5。
