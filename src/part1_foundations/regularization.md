# 过拟合、欠拟合与正则化

在训练机器学习模型时，我们的目标是让模型不仅在训练数据上表现良好，更重要的是在未见过的、新的测试数据上同样表现出色。这种在新数据上的表现能力被称为模型的 **泛化能力（Generalization）**。而过拟合与欠拟合是影响模型泛化能力的两种最常见的问题。

欠拟合 (Underfitting)是指模型在训练集和测试集上的表现都非常差。通常是因为模型过于简单，无法捕捉到数据中复杂的模式和规律。
解决方法为增加模型复杂度（例如增加神经网络的层数或神经元数量）、增加模型的训练轮次等。

过拟合 (Overfitting)是指模型在训练集上表现极好，但在测试集上表现很差。通常是模型过于复杂，以至于把训练数据中的噪声和随机波动也当作规律学习进去，而不是学习数据背后真正的、普适的模式。通常需要降低模型复杂度，或采用正则化（Regularization）等技术。

## 正则化

正则化是一系列旨在防止模型过拟合、提升其泛化能力的技术的总称。它的核心思想是在优化模型的损失函数时，增加一个对模型复杂度进行惩罚的惩罚项。
这个惩罚项用来限制模型的复杂度，通过限制模型复杂度迫使模型去学习更普遍、更具泛化性的特征，从而提高在测试数据上的表现。

### 常见的正则化方法

-   **L1 和 L2 正则化**：这两种方法通过在损失函数中增加一个与模型参数（权重）相关的惩罚项来限制权重的大小。
    -   **L2 正则化 (Ridge)**: 惩罚项是所有权重平方和的一半。它会使所有权重趋向于接近 0，但不会完全变为 0。在梯度下降更新时，L2 正则化项会导致权重在每次更新时都减去一个与自身大小成正比的量，这个过程被称为 **权重衰退（Weight Decay）**。因此，L2 正则化和权重衰退在实践中常常被认为是等价的。
    -   **L1 正则化 (Lasso)**: 惩罚项是所有权重绝对值的和。它能将一些不重要的特征的权重直接变为 0，从而实现 **特征选择**。
-   **Dropout**：在训练过程中，**随机**地“丢弃”一部分神经元（即将其输出暂时设为 0）。这迫使网络不能依赖于任何一个特定的神经元，从而防止了神经元之间的复杂协同适应，增强了模型的鲁棒性。
-   **数据增强 (Data Augmentation)**：通过对训练数据进行一些变换（如旋转、裁剪、翻转等）来扩充数据集，使得模型能接触到更多样的样本，从而提升泛化能力。
