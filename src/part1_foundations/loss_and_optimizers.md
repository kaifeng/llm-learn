# 损失函数与优化器

在神经网络的训练过程中，损失函数（Loss Function）和优化器（Optimizer）是两个至关重要的组件。它们共同指导着模型如何从数据中学习并逐步改进自身。

-   **损失函数**：定义了模型要优化的目标，即衡量模型预测值与真实值之间的差距。
-   **优化器**：实现了如何优化的过程，即根据损失函数计算出的差距，来调整模型的参数（权重和偏置）。

## 损失函数 (Loss Function)

损失函数输出一个数值，该数值代表了单次预测的好坏程度。损失值越大，说明模型的预测越不准确。训练的目标就是最小化损失值。

不同类型的任务需要使用不同的损失函数。

### 用于回归任务的损失函数

回归任务的目标是预测一个连续值（如房价、温度）。

-   **均方误差 (Mean Squared Error, MSE)**：也称为 L2 损失。它计算的是预测值与真实值之差的平方的平均值。由于平方的存在，MSE 对较大的误差给予更重的惩罚。
    $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

-   **平均绝对误差 (Mean Absolute Error, MAE)**：也称为 L1 损失。它计算的是预测值与真实值之差的绝对值的平均值。相比 MSE，MAE 对异常值不那么敏感，鲁棒性更强。
    $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

### 用于分类任务的损失函数

分类任务的目标是预测一个离散的类别（如“猫”或“狗”）。

-   **交叉熵损失 (Cross-Entropy Loss)**：是分类任务中最常用的损失函数。它衡量的是模型预测的概率分布与真实的概率分布之间的差异。

    -   **二元交叉熵 (Binary Cross-Entropy)**：用于二分类问题（例如，是/不是垃圾邮件）。此时，输出层只有一个神经元，并使用 Sigmoid 激活函数输出一个概率值 $p$。
        $$ L = -[y \log(p) + (1-y) \log(1-p)] $$
        其中 $y$ 是真实标签（0 或 1），$p$ 是模型预测为类别 1 的概率。

    -   **分类交叉熵 (Categorical Cross-Entropy)**：用于多分类问题（例如，手写数字 0-9 识别）。此时，输出层有 N 个神经元（N 为类别数），并使用 Softmax 激活函数输出一个概率分布。
        $$ L = -\sum_{i=1}^{N} y_i \log(p_i) $$
        其中 $y_i$ 是一个独热编码的真实标签，$p_i$ 是模型预测为类别 $i$ 的概率。

## 优化器 (Optimizer)

优化器是根据反向传播计算出的梯度来更新网络参数（权重和偏置）的算法。它的目标是找到一组能使损失函数最小化的参数。

### 梯度下降 (Gradient Descent)

梯度下降是所有优化算法的基础。其核心思想是：**沿着梯度下降最快的方向（梯度的反方向）调整参数，从而逐步减小损失值。**

更新规则如下：
`新参数 = 旧参数 - 学习率 × 梯度`

-   **学习率 (Learning Rate)**：是一个超参数，它控制着每次参数更新的步长。学习率过大可能导致模型在最优点附近震荡甚至发散；学习率过小则会导致模型收敛速度过慢。

根据每次更新使用的数据量，梯度下降分为三种变体：

1.  **批量梯度下降 (Batch Gradient Descent)**：每次更新都使用整个训练集的数据。计算精确但速度慢，且需要大量内存。
2.  **随机梯度下降 (Stochastic Gradient Descent, SGD)**：每次更新仅使用训练集中的一个样本。速度快，但更新方向不稳定，损失函数会剧烈波动。
3.  **小批量梯度下降 (Mini-batch Gradient Descent)**：是上述两者的折中，也是最常用的方法。每次更新使用一小批（mini-batch）数据（如 32, 64, 128 个样本）。它兼顾了计算效率和更新的稳定性。

### 高级优化器

为了解决 SGD 的一些问题（如容易陷入局部最优、收敛速度慢等），研究者们提出了许多更先进的优化器。

-   **Momentum**：引入了“动量”的概念。它在更新参数时，不仅考虑当前的梯度，还考虑了历史的更新方向。这有助于加速收敛并冲出局部最优。就像一个从山上滚下来的球，它会保持之前的速度。

-   **Adam (Adaptive Moment Estimation)**：是目前最流行、最常用的优化器之一。它结合了 Momentum 和另一种名为 RMSprop 的优化器的思想，能够为网络中的每一个参数计算自适应的学习率。Adam 通常能快速收敛，并且对超参数的选择不那么敏感，是许多任务的默认首选。