# 卷积神经网络 (Convolutional Neural Network, CNN)

卷积神经网络（CNN）是一种专门为处理具有类似网格结构的数据（如图像）而设计的深度学习模型。它在计算机视觉领域取得了巨大的成功，并深刻影响了现代深度学习的发展。

对于图像数据，如果使用标准的全连接神经网络，会面临两个主要问题：
1.  **参数爆炸**：一张 256x256 的彩色图片，其输入维度为 256x256x3 = 196,608。如果第一个隐藏层有 1000 个神经元，那么仅这一层就会有超过 1.9 亿个参数，在计算上是难以承受的。
2.  **空间结构丢失**：全连接网络将输入图像展平成一个向量，破坏了像素之间的空间邻近关系。然而，图像中的信息（如边缘、形状）正是由像素的空间排列决定的。

CNN 通过引入**局部连接**和**参数共享**等思想，巧妙地解决了这些问题。

## CNN 的核心组件

### 1. 卷积层 (Convolutional Layer)

卷积层是 CNN 的核心。它通过一个称为 **卷积核（Kernel）** 或 **滤波器（Filter）** 的小窗口在输入数据上滑动，来提取局部特征。

-   **卷积核 (Kernel)**：一个尺寸较小（如 3x3 或 5x5）的权重矩阵。这个卷积核就是模型要学习的参数。
-   **卷积操作**：卷积核在输入图像上从左到右、从上到下滑动。在每个位置，卷积核与其覆盖的图像区域进行逐元素相乘然后求和，得到一个输出值。
-   **特征图 (Feature Map)**：卷积核滑动完整个图像后，形成的所有输出值构成一个新的二维矩阵，称为特征图。特征图代表了输入图像在经过特定卷积核处理后提取出的特征。例如，一个卷积核可能学会了检测垂直边缘，另一个则可能学会检测绿色斑块。

通过**参数共享**（一个卷积核在整个图像上共享同一套权重），CNN 极大地减少了模型的参数量。一个卷积层通常会学习多个不同的卷积核，从而可以同时提取多种局部特征。

### 2. 池化层 (Pooling Layer)

在卷积层之后，通常会接一个池化层。

-   **目的**：对特征图进行**下采样（Down-sampling）**，以减小其空间尺寸。这有两个主要好处：
    1.  减少后续层次的计算量和参数数量。
    2.  使模型对特征的位置变化具有一定的不变性（例如，无论猫的眼睛在图像的左上角还是右上角，都能被识别）。
-   **最大池化 (Max Pooling)**：是最常见的池化操作。它将特征图划分为若干个小区域（如 2x2），并从每个区域中取出最大值作为输出。

### 3. 全连接层 (Fully-Connected Layer)

在一系列卷积和池化层之后，最终得到的特征图会被“展平”成一个一维向量。这个向量随后被送入一个或多个标准的全连接层（就像普通神经网络一样），进行最终的分类或回归任务。

一个典型的 CNN 架构看起来像这样：
`输入 -> [卷积层 -> 激活层 -> 池化层] * N -> 全连接层 -> 输出`

## 著名的 CNN 模型

### LeNet-5 (1998)

LeNet-5 是现代卷积神经网络的鼻祖，它被成功应用于银行的支票手写数字识别系统中，是 CNN 早期商业化应用的典范。

LeNet-5 的架构非常经典，为后续的 CNN 模型奠定了基础。它通常由 7 层构成（不含输入），其顺序为：

    输入 -> 卷积层 -> 池化层 -> 卷积层 -> 池化层 -> 全连接层 -> 全连接层 -> 输出层

LeNet 确立了 CNN 的基本模式：通过交替堆叠卷积层和池化层来提取特征，最后通过全连接层进行分类，这一模式成为了后来几十年 CNN 设计的蓝图。验证了卷积核通过共享参数来提取全图特征的有效性，极大地减少了模型参数。

### AlexNet (2012)

AlexNet赢了2012年ImageNet竞赛，本质上是更大更深的LeNet。

它比 LeNet-5 更深，并首次成功应用了 ReLU 激活函数和 Dropout 等技术，主要改进有：丢弃法、ReLU（减缓梯度消失）、MaxPooling，和数据增强（对输入数据进行变换生成更多的输入）。

### VGGNet (2014)

VGG使用可重复使用的卷积块来构建深度卷积神经网络。不同的卷积块个数和超参数可以得到不同的复杂度的变种。

探索了网络深度的影响。其特点是结构非常简洁，完全由 3x3 的小卷积核和 2x2 的池化层堆叠而成。证明了增加网络深度可以显著提升性能。

### GoogLeNet (Inception v1) (2014)

GoogLeNet 是 2014 年 ImageNet 挑战赛的冠军，它在追求更深网络的同时，也极其注重计算效率。

-   **所属类别**：注重计算效率的深度卷积神经网络。
-   **核心创新：Inception 模块**：面对“该用 3x3、5x5 还是 1x1 的卷积核？”这一问题，GoogLeNet 的回答是“我全都要！”。Inception 模块的核心思想是：
    1.  **并行多尺度处理**：在一个模块内，并行地使用不同尺寸的卷积核（1x1, 3x3, 5x5）以及池化操作，来捕捉不同尺度的特征。
    2.  **结果拼接**：将所有并行分支提取到的特征图在深度维度上拼接（concatenate）起来，作为模块的最终输出。
    3.  **1x1 卷积降维**：为了避免因并联多个大卷积核而导致的计算量爆炸，Inception 模块巧妙地在 3x3 和 5x5 卷积之前，以及在池化之后，都使用了一个 1x1 的卷积层。这个 1x1 卷积的作用像一个“瓶颈”，它能以很小的计算代价对特征图的深度进行降维，从而显著减少整个模块的参数量和计算量。
-   **意义**：GoogLeNet 证明了，通过精心设计的、非线性的网络拓扑结构（“网络中的网络”），可以在提升性能的同时，比 VGGNet 等模型更节省计算资源。

### ResNet (Residual Network) (2015)

是计算机视觉领域最具影响力的架构之一。它通过引入残差连接，巧妙地解决了超深网络（超过 100 层甚至 1000 层）中的梯度消失问题，使得训练前所未有的深度模型成为可能。