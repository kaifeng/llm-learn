# 激活函数 (Activation Functions)

激活函数是神经网络中的一个关键组件。它被应用于神经元的输出，为其引入非线性。如果没有非线性激活函数，无论神经网络有多少层，其本质上都只是一个线性模型，无法学习和表示复杂的数据模式。
激活函数决定了神经元在接收到特定输入后是否应该被“激活”或“点燃”，以及它应该向下一层传递什么强度的信号。

- **Sigmoid** 函数将任意实数输入压缩到 (0, 1) 的范围内，可以被解释为概率，常用于二分类问题的输出层或作为门控机制（如 LSTM 中的门）。

  $$ f(x) = \frac{1}{1 + e^{-x}} $$

  但是当输入值非常大或非常小时，函数的梯度会趋近于0，在反向传播过程中，会导致深层网络的梯度信号变得非常微弱，使得网络难以训练。

- **Tanh** (双曲正切) 函数可以看作是 Sigmoid 函数的一个缩放和移位版本，它将输入压缩到 (-1, 1) 的范围内。通常比 Sigmoid 函数有更好的性能，收敛速度更快，但仍然存在梯度消失的问题。

  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

- **ReLU** 是目前在深度学习的隐藏层中最常用的激活函数，只涉及一个简单的阈值操作。

  $$ f(x) = \max(0, x) $$

  当输入为正数时，其梯度恒为 1，这使得梯度能够很好地在网络中传播。但是，如果一个神经元的输入在训练过程中持续为负，那么它的梯度将永远为0，导致该神经元的权重无法再被更新。

- **Leaky ReLU** 是对 ReLU 的改进，它允许在输入为负时也有一个小的、非零的梯度。$\alpha$ 是一个很小的常数（如 0.01）。

  $$ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$

- **Softmax** 的作用是将一个包含任意实数值的向量（称为 logits）转换为一个概率分布。每个元素的值都在 (0, 1) 之间，并且所有元素的总和为 1。
  这使得模型的输出可以直接解释为输入样本属于每一个类别的概率，专门用于多分类问题的输出层。

  $$ \sigma_i = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}, \quad i=1,2,...,n $$

