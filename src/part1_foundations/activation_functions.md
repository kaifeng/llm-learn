# 激活函数 (Activation Functions)

激活函数是神经网络中的一个关键组件。它被应用于神经元的输出，为其引入**非线性（non-linearity）**。如果没有非线性激活函数，无论神经网络有多少层，其本质上都只是一个线性模型，无法学习和表示复杂的数据模式。

激活函数决定了神经元在接收到特定输入后是否应该被“激活”或“点燃”，以及它应该向下一层传递什么强度的信号。

## Sigmoid 函数

Sigmoid 函数是最早被使用的激活函数之一。它将任意实数输入压缩到 (0, 1) 的范围内，使其看起来像一个S型曲线。

$$ f(x) = \frac{1}{1 + e^{-x}} $$

-   **优点**：输出范围在 0 和 1 之间，可以被解释为概率，常用于二分类问题的输出层或作为门控机制（如 LSTM 中的门）。
-   **缺点**：
    1.  **梯度消失**：当输入值非常大或非常小时，函数的梯度（导数）会趋近于 0。在反向传播过程中，这会导致深层网络的梯度信号变得非常微弱，使得网络难以训练。
    2.  **输出非零中心**：函数的输出总是正数，这可能导致后续网络层的参数更新朝同一个方向偏移，降低收敛速度。

## Tanh (双曲正切) 函数

Tanh 函数可以看作是 Sigmoid 函数的一个缩放和移位版本，它将输入压缩到 (-1, 1) 的范围内。

$$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

-   **优点**：输出是零中心的，这通常比 Sigmoid 函数有更好的性能，收敛速度更快。
-   **缺点**：仍然存在梯度消失的问题。

## ReLU (修正线性单元)

ReLU 是目前在深度学习的隐藏层中**最常用**的激活函数。

$$ f(x) = \max(0, x) $$

它的工作方式非常简单：如果输入大于 0，则直接输出该值；如果输入小于或等于 0，则输出 0。

-   **优点**：
    1.  **计算高效**：只涉及一个简单的阈值操作。
    2.  **解决梯度消失问题（部分）**：当输入为正数时，其梯度恒为 1，这使得梯度能够很好地在网络中传播。
-   **缺点**：
    1.  **Dying ReLU 问题**：如果一个神经元的输入在训练过程中持续为负，那么它的梯度将永远为 0，导致该神经元的权重无法再被更新。这个神经元就“死亡”了。

### Leaky ReLU

Leaky ReLU 是对 ReLU 的一个改进，旨在解决Dying ReLU问题。它允许在输入为负时，也有一个小的、非零的梯度。

$$ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$

其中 $\alpha$ 是一个很小的常数，例如 0.01。

## Softmax 函数

Softmax 函数**也是一种激活函数**，但它与其他用于隐藏层的激活函数有所不同，它通常不用于隐藏层，而是专门用于**多分类问题的输出层**。

它的作用是将一个包含任意实数值的向量（称为 logits）转换为一个**概率分布**。每个元素的值都在 (0, 1) 之间，并且所有元素的总和为 1。

对于一个长度为 n 的实数向量 $z=(z_1, z_2, \dots, z_n)$，Softmax 函数的计算公式为：

$$ \sigma_i = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}, \quad i=1,2,...,n $$

这使得模型的输出可以直接解释为“输入样本属于每一个类别的概率”。
