# 文本表示与词嵌入

对于人类来说，语言是直观的。但对于计算机来说，原始文本只是一串字符，无法直接用于数学计算。因此，在进行自然语言处理（NLP）任务时，首要步骤就是将文本转换成机器可以理解的数值形式。这个过程称为**文本表示（Text Representation）**。

## 离散表示：独热编码 (One-Hot Encoding)

最简单的文本表示方法之一是独热编码。

1.  **构建词典**：首先，统计语料库中所有出现过的独立词语，构建一个词典。
2.  **向量化**：对于词典中的每一个词，创建一个长度等于词典大小的向量。这个向量在代表该词的位置上为 1，在所有其他位置上都为 0。

**示例**：
假设我们的词典只包含三个词：`{猫, 狗, 鱼}`。

-   `猫` 的独热编码为 `[1, 0, 0]`
-   `狗` 的独热编码为 `[0, 1, 0]`
-   `鱼` 的独热编码为 `[0, 0, 1]`

**独热编码的缺点**：

-   **高维稀疏**：如果词典中有成千上万个词，那么每个词的向量维度就会非常高，并且其中绝大多数元素都是 0，这在计算上是低效的。
-   **无法表达语义相似性**：这是最致命的缺点。在独热编码中，任意两个不同词的向量都是正交的（它们的点积为 0）。这意味着，模型无法从向量本身得知 `猫` 和 `狗` 的关系比 `猫` 和 `鱼` 更近。词与词之间的语义关系完全丢失了。

## 分布式表示：词嵌入 (Word Embedding)

为了克服独热编码的缺点，研究者们提出了**分布式表示**，其中最著名的就是**词嵌入**。

### 核心思想

词嵌入不再使用稀疏的高维向量，而是将每个词表示为一个**低维、稠密的浮点数向量**（例如，一个长度为 100 或 300 的向量）。

其核心思想基于**分布式假设（Distributional Hypothesis）**：**上下文相似的词，其含义也相似**。例如，在大量文本中，“猫”和“狗”经常出现在相似的语境中，而“鱼”的上下文则可能很不一样。

词嵌入模型通过在大量文本上进行训练，将这种上下文关系编码到向量中。最终得到的词向量具有以下优良特性：

-   **语义相似性**：意义相近的词在向量空间中的距离也相近。
-   **语义关系**：向量之间甚至可以进行有意义的运算。最经典的例子是：`vector('King') - vector('Man') + vector('Woman') ≈ vector('Queen')`。

### 主流的词嵌入模型

-   **Word2Vec (Google, 2013)**：是词嵌入领域最经典的模型之一。它本质上是一个浅层神经网络，通过预测词与其上下文的关系来学习词向量。它包含两种主要的训练架构：
    -   **CBOW (Continuous Bag-of-Words)**：根据上下文词来预测中心词。
    -   **Skip-gram**：根据中心词来预测其上下文的词。通常在大型语料库上表现更好。

-   **GloVe (Stanford, 2014)**：全称为“Global Vectors for Word Representation”。它结合了两种方法的优点：既利用了全局的统计信息（词的共现矩阵），又利用了局部的上下文信息。GloVe 通过对一个巨大的“词-词共现矩阵”进行分解，来学习词向量。

词嵌入是现代 NLP 任务的基石。它们为神经网络模型提供了一种捕捉词语语义的有效方式，是后续理解句子、段落乃至文档语义的基础。