# 前言

什么是大语言模型？简单来说，大语言模型是一种在海量文本数据上进行训练的、规模极其庞大的深度学习模型。它们的核心任务是**预测文本序列中的下一个词**。通过在数万亿词汇的语料上进行这种看似简单的训练，模型被迫学习到了关于语言、世界和逻辑的深层模式。

本文档记录大语言模型的相关知识点，包括：

1.  **机器学习与深度学习基础**：回顾神经网络、梯度计算、损失函数等基本构建块。
2.  **自然语言处理（NLP）基础**：了解计算机如何理解文字，从词嵌入到处理序列的经典模型 RNN/LSTM。
3.  **Transformer 与注意力机制**：深入剖析引爆了这场技术革命的核心架构——Transformer 模型及其灵魂“注意力机制”。
4.  **预训练语言模型**：探索 BERT 和 GPT 这两条不同技术路线的代表，以及“预训练-微调”这一关键范式。
5.  **现代大语言模型**：揭秘让 LLM 变得实用、可控的核心技术，如指令微调、RLHF、提示工程，以及解决“幻觉”问题的 RAG 技术。
