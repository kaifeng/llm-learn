# 前言

**机器学习** 是人工智能的一个分支，其核心思想是让计算机系统通过数据而非明确编程来学习。它通过算法分析数据，从中学习模式，并利用这些模式对新数据进行预测或决策。机器学习涵盖了多种算法和技术，包括线性回归、决策树、支持向量机等。

**深度学习** 是机器学习的一个子领域，它专注于使用深度神经网络（即包含多个隐藏层的神经网络）来学习数据中的复杂模式。深度学习的深度指的是网络中层数的增加，这使得模型能够自动从原始数据中提取多层次、抽象的特征，而无需人工进行特征工程。大语言模型（LLM）是深度学习在自然语言处理领域最前沿的应用之一。

机器学习分为监督学习、非监督学习、自监督学习和强化学习等。

**监督学习 (Supervised Learning)** 是最常见、最成熟的机器学习范式。

-   **核心思想**：模型从 **有标签（labeled）** 的数据中学习。数据集中的每个样本都包含一个输入特征（input）和一个期望的输出标签（output/label）。
-   **学习目标**：学习一个从输入到输出的映射函数。模型通过比较自己的预测输出和真实标签之间的差异（即“损失”），来不断调整自身参数，直到能对未见过的新输入做出准确的预测。
-   **主要任务**：监督学习主要解决两大类问题：分类和回归。
    -   **分类 (Classification)**：预测一个离散的类别。例如：判断一封邮件是否是垃圾邮件（二分类），或识别一张图片中的动物是猫、狗还是鸟（多分类）。 **逻辑回归（Logistic Regression）** 是解决这类问题的经典算法之一。
    -   **回归 (Regression)**：预测一个连续的数值。例如：根据房屋的面积、位置等特征预测其价格。**线性回归（Linear Regression）** 就是解决这类问题的经典算法之一。

-   **典型算法**：监督学习的算法非常丰富，包括：
    -   **用于分类的算法**：**逻辑回归（Logistic Regression）**、**决策树（Decision Tree）**、支持向量机（SVM）、K近邻（KNN）等。
    -   **用于回归的算法**：**线性回归（Linear Regression）**、决策树回归、支持向量回归等。
    决策树通过构建一个树状的决策模型来进行预测，具有很强的可解释性。

**非监督学习 (Unsupervised Learning)**

-   **核心思想**：模型从**没有标签**的数据中学习。它只能接触到输入数据，必须自己从中发现隐藏的结构、模式或关系。
-   **学习目标**：探索数据内在的结构。
-   **主要任务**：
    -   **聚类 (Clustering)**：将相似的数据点分到同一个簇（cluster）中。例如：根据用户的购买行为将其划分为不同的客户群体。
    -   **降维 (Dimensionality Reduction)**：在保留数据主要信息的前提下，减少数据的特征数量。例如：主成分分析（PCA）。

**自监督学习 (Self-Supervised Learning, SSL)** 是近年来推动大语言模型发展的关键技术，可以看作是无监督学习的一种特殊形式。

-   **核心思想**：同样使用**没有标签**的海量数据，但它巧妙地从数据本身中自动创建伪标签，从而将问题转化为一个监督学习问题来进行训练。
-   **学习目标**：通过解决一个精心设计的“借口任务（pretext task）”，来让模型学习到关于数据内在结构的、有价值的表示。好比一个学生拿到一张被撕碎的报纸，他通过学习如何将碎片拼接回完整的报纸（借口任务），从而学会了语法、词汇和常识（学到了表示）。
-   **在 LLM 中的应用**：
    -   **掩码语言模型 (Masked Language Modeling)**：随机遮盖句子中的一些词，然后让模型预测被遮盖的词是什么（BERT 使用的方式）。
    -   **下一词预测 (Next Token Prediction)**：根据一段文本的前半部分，让模型预测下一个词是什么（GPT 使用的方式）。

自监督学习是“预训练-微调”范式中**预训练**阶段的理论基础，它使得从未经标注的、海量的互联网文本中学习通用语言知识成为可能。

**强化学习 (Reinforcement Learning, RL)**

-   **核心思想**：模型（称为**智能体 Agent**）通过与一个**环境（Environment）**进行交互来学习。智能体在环境中做出**动作（Action）**，环境会相应地改变 **状态（State）** 并反馈给智能体一个 **奖励（Reward）** 或惩罚。
-   **学习目标**：学习一个最优**策略（Policy）**，即在什么状态下应该采取什么动作，以最大化长期累积的总奖励。好比训练一只宠物。当它做出正确的动作时，给它零食（正奖励）；当它做出错误动作时，不给奖励或进行轻微的惩罚。
-   **在 LLM 中的应用**：
    -   **基于人类反馈的强化学习 (RLHF)**：这是对齐（align）大语言模型、使其回答更有用、更无害的关键技术。人类对模型生成的多个回答进行偏好排序，这些排序被用来训练一个“奖励模型”，然后用这个奖励模型作为环境，通过强化学习来微调语言模型，使其更倾向于生成人类偏好的内容。