# LLM领域概览

LLM领域是一个快速发展且极其庞大的生态系统，可以从多个维度进行细分，以更好地理解其构成、技术栈和应用场景。

## 1. 按模型架构

这是 LLM 最基础的分类方式之一，主要基于 Transformer 模型的不同组件。

原始的 Transformer 模型是一个包含编码器（Encoder）和解码器（Decoder）的完整架构，它在机器翻译等序列到序列（Seq2Seq）任务上取得了巨大成功。然而，研究者们很快发现，根据任务性质的不同，只使用其中的一部分往往能达到更优的效果。这导致了 Transformer 架构的三个主要分支。

### Encoder-only 架构（编码器模型）

这种架构只使用 Transformer 的编码器部分。

-   **工作方式**：输入一个完整的句子，模型中的自注意力机制可以同时关注到句子中的所有词元（token），无视其前后位置。因此，它能生成每个词元在 **深度双向（deeply bidirectional）** 语境下的表示。它对整个句子的理解是全局性的。

-   **适用场景**：非常适合需要对已有文本进行深入理解的任务，而不是生成新文本。
    -   句子分类（如情感分析）
    -   命名实体识别（NER）
    -   完形填空（如 BERT 的 MLM 任务）
    -   从段落中提取答案的问答任务

-   **代表模型**：**BERT** 及其所有变体（如 RoBERTa, ALBERT, DistilBERT）。

### Decoder-only 架构（解码器模型）

这种架构只使用 Transformer 的解码器部分。

-   **工作方式**：其核心是**带掩码的自注意力（Masked Self-Attention）**。在处理或生成任何一个词元时，模型只能关注到它自己以及它之前的所有词元，无法看到未来的信息。这种从左到右的单向信息流使其本质上是一个 **自回归（Autoregressive）** 模型。

-   **适用场景**：天然适合于根据上文生成新文本的任务。
    -   文本续写、故事创作
    -   开放式对话、聊天机器人
    -   代码生成

-   **代表模型**：**GPT 系列**（GPT-1, 2, 3, 4）、**Llama 系列**、Mistral 等。这是当今绝大多数最强大的大语言模型所采用的架构。

### Encoder-Decoder 架构（序列到序列模型）

这种架构保留了 Transformer 的完整结构。

-   **工作方式**：编码器首先将整个输入序列编码成一个富含信息的中间表示。然后，解码器在自回归地生成输出序列的每一步，都会通过“编码器-解码器注意力”来关注编码器产出的完整输入信息。

-   **适用场景**：最适合需要将一个输入序列转换为另一个输出序列的任务，尤其是当输入和输出序列的格式、长度或语言不同时。
    -   机器翻译
    -   文本摘要
    -   代码注释生成

-   **代表模型**：原始的 Transformer 模型、**T5**、**BART**。

### 对比

| 架构分支 | 工作方式 | 核心优势 | 适用任务 | 代表模型 |
| :--- | :--- | :--- | :--- | :--- |
| **Encoder-only** | 双向上下文 | 深度理解 | NLU (理解) | BERT, RoBERTa |
| **Decoder-only** | 单向自回归 | 文本生成 | NLG (生成) | GPT, Llama, Mistral |
| **Encoder-Decoder** | 映射输入与输出 | 序列转换 | Seq2Seq (转换) | T5, BART, M2M-100 |

## 2. 按模型生命周期

LLM 的开发和应用涉及多个连续的阶段：

-   **预训练**：在海量无标签数据上进行自监督学习，使模型学习通用语言知识和世界模型。这是 LLM 智能的基石。相关技术有大规模分布式训练（如 Megatron-LM）、数据清洗与过滤。
-   **微调与对齐 (Fine-tuning & Alignment)**：
    -   **指令微调 (Instruction Fine-tuning)**：使模型学会遵循人类指令。
    -   **基于人类反馈的强化学习 (RLHF)**：进一步将模型行为与人类偏好和价值观对齐，提升有用性、无害性。
-   **部署与推理**：将训练好的模型投入实际使用，并优化其运行效率和成本。相关技术有高效推理引擎（如 vLLM）、模型量化、模型蒸馏。

## 3. 按模型类型与能力

-   **基础模型**：仅经过大规模预训练，尚未进行指令微调，主要用于文本补全。
-   **指令遵循模型**：经过指令微调，能更好地理解和执行人类指令。
-   **对话模型**：专门为多轮对话优化，具有更强的对话连贯性和交互性。
-   **多模态模型**：能够理解和生成多种模态的数据（如文本、图像、音频、视频）。
-   **领域专用模型**：针对特定行业或知识领域（如医疗、法律、金融）进行训练或微调，具有更专业的知识和表现。

## 4. 按工具与框架

支撑 LLM 开发和应用的软件生态系统：

-   **深度学习框架**：用于构建和训练神经网络。如 PyTorch, TensorFlow。
-   **高效推理框架**：优化 LLM 在生产环境中的推理速度和资源消耗。如 vLLM, TensorRT-LLM。
-   **统一算子框架**：用于编写高性能的自定义 GPU 算子。如 Triton。
-   **Agent 框架**：用于构建能够自主规划、工具使用和多步推理的 LLM 智能体。如 LangChain, LlamaIndex, AutoGen。

## 5. 按开放性

-   **闭源模型**：模型权重和架构不公开，通过 API 提供服务。如 OpenAI GPT 系列、Anthropic Claude 系列。
-   **开放权重模型**：模型权重公开，但可能附带使用许可限制。如 Llama 系列、Mistral 系列、Gemma。
-   **开源模型**：模型权重、代码和数据均公开，通常遵循宽松的开源许可。一般是一些较小的研究模型或社区驱动项目。
