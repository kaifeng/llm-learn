# 架构分野：Encoder-only, Decoder-only 与 Encoder-Decoder

原始的 Transformer 模型是一个包含编码器（Encoder）和解码器（Decoder）的完整架构，它在机器翻译等序列到序列（Seq2Seq）任务上取得了巨大成功。然而，研究者们很快发现，根据任务性质的不同，只使用其中的一部分往往能达到更优的效果。这导致了 Transformer 架构的三个主要分支。

## 1. Encoder-only 架构（编码器模型）

这种架构只使用 Transformer 的编码器部分。

-   **工作方式**：输入一个完整的句子，模型中的自注意力机制可以同时关注到句子中的所有词元（token），无视其前后位置。因此，它能生成每个词元在**深度双向（deeply bidirectional）**语境下的表示。它对整个句子的理解是全局性的。

-   **适用场景**：非常适合需要对已有文本进行深入**理解（NLU）**的任务，而不是生成新文本。
    -   句子分类（如情感分析）
    -   命名实体识别（NER）
    -   完形填空（如 BERT 的 MLM 任务）
    -   从段落中提取答案的问答任务

-   **代表模型**：**BERT** 及其所有变体（如 RoBERTa, ALBERT, DistilBERT）。

## 2. Decoder-only 架构（解码器模型）

这种架构只使用 Transformer 的解码器部分。

-   **工作方式**：其核心是**带掩码的自注意力（Masked Self-Attention）**。在处理或生成任何一个词元时，模型只能关注到它自己以及它之前的所有词元，无法“看到”未来的信息。这种从左到右的单向信息流使其本质上是一个**自回归（Autoregressive）**模型。

-   **适用场景**：天然适合于根据上文**生成（NLG）**新文本的任务。
    -   文本续写、故事创作
    -   开放式对话、聊天机器人
    -   代码生成

-   **代表模型**：**GPT 系列**（GPT-1, 2, 3, 4）、**Llama 系列**、Mistral 等。这是当今绝大多数最强大的大语言模型所采用的架构。

## 3. Encoder-Decoder 架构（序列到序列模型）

这种架构保留了 Transformer 的完整结构。

-   **工作方式**：编码器首先将整个输入序列编码成一个富含信息的中间表示。然后，解码器在自回归地生成输出序列的每一步，都会通过“编码器-解码器注意力”来关注编码器产出的完整输入信息。

-   **适用场景**：最适合需要将一个输入序列**转换**为另一个输出序列的**序列到序列（Seq2Seq）**任务，尤其是当输入和输出序列的格式、长度或语言不同时。
    -   机器翻译
    -   文本摘要
    -   代码注释生成

-   **代表模型**：原始的 Transformer 模型、**T5**、**BART**。

## 总结对比

| 架构分支 | 工作方式 | 核心优势 | 适用任务 | 代表模型 | 
| :--- | :--- | :--- | :--- | :--- |
| **Encoder-only** | 双向上下文 | 深度理解 | NLU (理解) | BERT, RoBERTa |
| **Decoder-only** | 单向自回归 | 文本生成 | NLG (生成) | GPT, Llama, Mistral |
| **Encoder-Decoder** | 映射输入与输出 | 序列转换 | Seq2Seq (转换) | T5, BART, M2M-100 |