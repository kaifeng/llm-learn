# GPT 模型：生成式预训练的先驱

GPT（Generative Pre-trained Transformer）系列模型由 OpenAI 提出，是现代大语言模型（LLM）的另一条重要发展路线。与 BERT 专注于语言理解不同，GPT 的核心在于**生成（Generation）**。它开创了通过大规模自回归预训练来构建强大通用语言模型的范式，是“解码器路径”模型的典范。

## 核心架构：Transformer 解码器

GPT 的架构与 BERT 恰好相反：它完全由**一堆堆叠起来的 Transformer 解码器（Decoder）** 组成。这意味着它的核心是**带掩码的自注意力机制（Masked Self-Attention）**。

这种“解码器-only”的架构决定了 GPT 的本质特性：

-   **自回归（Autoregressive）**：在处理或生成文本时，模型在任何一个位置都只能关注到它之前的内容，无法看到“未来”的词。这使得 GPT 天然地适合执行“根据已有内容、续写下一个词”的任务。
-   **单向性（Unidirectional）**：信息流是严格从左到右的。这与 BERT 的双向性形成了鲜明对比。

## 预训练任务：标准语言建模

GPT 的预训练任务非常直接和经典：就是**标准语言建模（Standard Language Modeling）**。

> 目标是预测序列中的下一个词，给定所有前面的词。

$$ P(w_t | w_1, w_2, ..., w_{t-1}) $$

模型在一个极其庞大的、未标记的文本语料库（例如，整个互联网的文本）上进行这个任务的训练。通过不断地预测下一个词，模型被迫学习到语法、语义、事实知识以及各种复杂的语言模式。

## 微调 (Fine-Tuning)

与 BERT 类似，预训练完成后，GPT 也可以针对特定的下游任务进行微调。例如，在模型顶部添加一个线性层，就可以将其用于情感分类、文章分类等任务。由于其生成性质，它也能够直接用于文本摘要、翻译和对话等任务。

## GPT 的演进：规模与能力的涌现

GPT 系列的发展清晰地展示了“规模定律（Scaling Laws）”的力量：即随着模型参数、数据量和计算量的指数级增长，模型的能力会“涌现”出意想不到的飞跃。

-   **GPT-1 (2018)**：首次验证了“生成式预训练 + 任务微调”这一范式的有效性。

-   **GPT-2 (2019)**：显著扩大了模型规模（最多 15 亿参数）。它最引人注目的发现是，一个足够大的语言模型无需任何微调，就能在**零样本（Zero-shot）**设置下执行多种任务。例如，你只要给它一个翻译任务的开头 `translate to french: ...`，它就能理解并继续完成任务。GPT-2 因其强大的文本生成能力而名声大噪。

-   **GPT-3 (2020)**：再一次实现了巨大的规模跨越（最多 1750 亿参数）。GPT-3 的一个核心贡献是展示了**上下文学习（In-context Learning）** 的惊人能力。这意味着你甚至不需要微调模型，只需在输入提示（Prompt）中提供几个任务的示例（**少样本学习, Few-shot Learning**），模型就能理解你的意图并执行相应的任务。例如：

    ```
    Q: What is the capital of France?
    A: Paris.
    
    Q: What is the capital of Japan?
    A: Tokyo.
    
    Q: What is the capital of Canada?
    A: 
    ```
    模型会接着输出 `Ottawa`。

## 总结

GPT 系列模型证明了自回归语言建模和大规模训练的巨大潜力。它不仅是强大的文本生成器，更通过上下文学习展示了一种与模型交互的全新方式，为后来的指令微调和 ChatGPT 的诞生铺平了道路。它被认为是“解码器路径”模型的代表，尤其擅长**自然语言生成（NLG）**相关的任务。