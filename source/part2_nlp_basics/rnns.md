# 循环神经网络 (RNN & LSTM)

在理解了如何将词语转换为向量（词嵌入）之后，下一步是处理由这些词向量组成的序列（即句子）。常规的前馈神经网络（Feed-forward Neural Network）在每个时间步独立处理输入，无法捕捉序列中的时间依赖关系。例如，在句子“我来自中国”中，“来自”这个词的含义依赖于前面的“我”。
**循环神经网络（Recurrent Neural Network, RNN）** 就是用来解决这个问题。

## 循环神经网络 (RNN)

RNN 的核心思想是“记忆”。它在网络中引入了一个“循环”，允许信息在序列的不同时间步之间传递。

在每个时间步 $t$，RNN 的计算单元会接收两个输入：

1.  当前时间步的输入 $x_t$（例如，句子中第 $t$ 个词的词嵌入）。
2.  上一个时间步的隐藏状态 $h_{t-1}$（这部分就是“记忆”）。

然后，它会计算出当前时间步的输出 $y_t$ 和新的隐藏状态 $h_t$。新的隐藏状态 $h_t$ 会被传递到下一个时间步 $t+1$。

$$ h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h) $$
$$ y_t = W_{hy}h_t + b_y $$

通过这种方式，隐藏状态 $h_t$ 充当了网络的记忆，它在理论上编码了从序列开始到当前时间步的所有信息。

## RNN 的局限：长程依赖问题

尽管 RNN 在理论上很强大，但在实践中，它很难学习到序列中的**长程依赖（Long-Term Dependencies）**。例如，在句子 “我在法国长大，......，所以我能说一口流利的法语” 中，模型需要将“法语”和很久之前的“法国”联系起来。

这个问题源于**梯度消失（Vanishing Gradients）** 和 **梯度爆炸（Exploding Gradients）**。

-   **梯度消失**：在通过反向传播算法训练 RNN 时，梯度需要从序列的末尾传递到开头。在每一步传递中，梯度都会乘以一个权重矩阵。如果这个矩阵的范数小于 1，经过多次相乘后，梯度会迅速衰减到接近于零。这导致网络无法学习到早期输入对当前输出的影响，即“长期记忆”失效。
-   **梯度爆炸**：相反，如果权重矩阵的范数大于 1，梯度会在反向传播过程中指数级增长，导致训练过程不稳定。

梯度爆炸相对容易解决（例如，通过梯度裁剪），但梯度消失问题则更为根本和棘手。

## 长短期记忆网络 (LSTM)

为了解决 RNN 的长程依赖问题，**长短期记忆网络（Long Short-Term Memory, LSTM）** 被提了出来。LSTM 是一种特殊的 RNN，它通过更精巧的内部结构来让梯度更容易地在长序列中流动。

LSTM 的核心创新是引入了**单元状态（Cell State）** 和三个**门（Gates）**。

-   **单元状态 ($c_t$)**：可以看作是一条“传送带”，信息可以在上面直接流过，只做一些微小的线性修改。这使得信息和梯度能够更容易地在长序列中保持不变。

-   **门（Gates）**：门是一种让信息选择性通过的结构。它由一个 Sigmoid 激活函数和一个按元素乘法操作组成。Sigmoid 函数输出一个 0 到 1 之间的值，这个值决定了有多少信息可以通过。LSTM 有三个门来保护和控制单元状态：

    1.  **遗忘门 (Forget Gate)**：决定应该从上一个单元状态 $c_{t-1}$ 中丢弃什么信息。
    2.  **输入门 (Input Gate)**：决定什么样的新信息可以被存放到当前的单元状态 $c_t$ 中。
    3.  **输出门 (Output Gate)**：决定单元状态 $c_t$ 中的哪些信息需要被输出到当前的隐藏状态 $h_t$。

通过这套精密的门控机制，LSTM 能够有效地学习哪些信息需要被长期记忆，哪些信息是暂时的，从而成功捕捉长程依赖。

## GRU (Gated Recurrent Unit)

GRU 是 LSTM 的一个流行变体，它对 LSTM 的结构进行了简化。

-   它将遗忘门和输入门合并为了一个单一的**更新门（Update Gate）**。
-   它还合并了单元状态和隐藏状态。

GRU 的参数比标准 LSTM 少，因此训练起来通常更快。在许多任务上，它的表现与 LSTM 相当，但在某些情况下，拥有更多参数的 LSTM 可能会更强大。在实践中，可以根据具体任务和计算资源来选择使用 LSTM 还是 GRU。