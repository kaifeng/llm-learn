# 长短期记忆网络 (LSTM)

长短期记忆网络（Long Short-Term Memory, LSTM）是循环神经网络（RNN）的一种特殊类型，由 Hochreiter 和 Schmidhuber 在 1997 年提出。它被设计用来解决标准 RNN 在处理长序列时遇到的**长程依赖（Long-Term Dependencies）**问题，即模型难以学习到序列中相隔较远的信息之间的关联。

## 核心思想

LSTM 的核心创新是引入了**单元状态（Cell State）**和三个**门（Gates）**。这使得 LSTM 能够选择性地记住或遗忘信息，从而有效地捕捉长程依赖。

-   **单元状态 ($c_t$)**：可以看作是一条“传送带”，信息可以在上面直接流过，只做一些微小的线性修改。这使得信息和梯度能够更容易地在长序列中保持不变。

-   **门（Gates）**：门是一种让信息选择性通过的结构。它由一个 Sigmoid 激活函数和一个按元素乘法操作组成。Sigmoid 函数输出一个 0 到 1 之间的值，这个值决定了有多少信息可以通过。LSTM 有三个门来保护和控制单元状态：

    1.  **遗忘门 (Forget Gate)**：决定应该从上一个单元状态 $c_{t-1}$ 中丢弃什么信息。它通过读取 $h_{t-1}$ 和 $x_t$ 来输出一个 0 到 1 之间的数值，1 表示完全保留，0 表示完全丢弃。
    2.  **输入门 (Input Gate)**：决定什么样的新信息可以被存放到当前的单元状态 $c_t$ 中。它包含两部分：一个 Sigmoid 层决定哪些值需要更新，一个 Tanh 层创建新的候选值 $\tilde{C}_t$。
    3.  **输出门 (Output Gate)**：决定单元状态 $c_t$ 中的哪些信息需要被输出到当前的隐藏状态 $h_t$。它通过一个 Sigmoid 层来决定输出哪些部分，然后将单元状态通过 Tanh 激活函数，再与 Sigmoid 层的输出相乘。

## LSTM 的优势

-   **解决长程依赖**：通过门控机制，LSTM 能够有效地学习哪些信息需要被长期记忆，哪些信息是暂时的，从而成功捕捉长程依赖。
-   **缓解梯度消失**：单元状态的线性流使得梯度更容易在时间步之间传播，有效缓解了梯度消失问题。

LSTM 在语音识别、机器翻译、文本生成等多种序列任务中取得了巨大成功，是深度学习领域的重要里程碑之一。
