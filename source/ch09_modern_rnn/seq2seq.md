# 序列到序列学习 (Sequence to Sequence, Seq2Seq)

序列到序列学习（Sequence to Sequence, Seq2Seq）是一种通用的深度学习框架，用于将一个序列映射到另一个序列。它在机器翻译、文本摘要、对话系统等任务中取得了巨大成功。

## 核心思想

Seq2Seq 模型的核心是**编码器-解码器（Encoder-Decoder）架构**。它由两个独立的循环神经网络（RNN，通常是 LSTM 或 GRU）组成：

1.  **编码器 (Encoder)**：读取输入序列（如源语言句子），并将其压缩成一个固定长度的**上下文向量（Context Vector）**。这个向量被认为是输入序列的语义表示。
2.  **解码器 (Decoder)**：接收编码器生成的上下文向量，并逐个生成输出序列（如目标语言句子）。

## 工作流程

- **编码阶段**：编码器逐个处理输入序列的元素，并更新其内部隐藏状态。最终的隐藏状态（或所有隐藏状态的某种组合）作为上下文向量传递给解码器。
- **解码阶段**：解码器以上下文向量作为初始状态，并以一个特殊的起始符（如 `<SOS>`）作为第一个输入。然后，它在每个时间步生成一个输出词，并将上一个时间步生成的词作为下一个时间步的输入，直到生成一个结束符（如 `<EOS>`）。

## 挑战与改进

传统的 Seq2Seq 模型在处理长序列时存在**信息瓶颈**问题，因为所有的输入信息都被压缩到一个固定长度的上下文向量中。这使得模型难以记住长序列中的所有细节。

为了解决这个问题，**注意力机制（Attention Mechanism）**被引入到 Seq2Seq 模型中。注意力机制允许解码器在生成每个输出词时，动态地关注输入序列的不同部分，从而克服了固定长度上下文向量的限制，显著提升了模型在长序列任务上的性能。

Seq2Seq 框架是神经机器翻译的基石，也是许多其他序列生成任务的基础。
