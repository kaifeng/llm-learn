# 编码器-解码器 (Encoder-Decoder) 架构

编码器-解码器（Encoder-Decoder）架构是处理序列到序列（Sequence-to-Sequence, Seq2Seq）任务的核心框架，广泛应用于机器翻译、文本摘要、对话系统等领域。

## 核心思想

该架构由两个主要部分组成：

1.  **编码器 (Encoder)**：负责将输入序列（如源语言句子）编码成一个固定长度的**上下文向量（Context Vector）**。这个上下文向量被认为是输入序列的语义表示，它浓缩了输入序列的所有相关信息。
    -   编码器通常是一个循环神经网络（RNN），如 LSTM 或 GRU。它逐个读取输入序列的元素，并更新其内部隐藏状态，最终的隐藏状态（或所有隐藏状态的某种组合）就是上下文向量。

2.  **解码器 (Decoder)**：负责将编码器生成的上下文向量解码成输出序列（如目标语言句子）。
    -   解码器也是一个循环神经网络。它以上下文向量作为初始隐藏状态，并逐个生成输出序列的元素。在生成每个输出元素时，解码器通常会接收上一个时间步生成的元素作为输入，并结合上下文向量来预测下一个元素。

## 工作流程

1.  **编码阶段**：编码器接收输入序列 $x_1, x_2, ..., x_T$，并生成上下文向量 $c$。
2.  **解码阶段**：解码器接收上下文向量 $c$ 和一个特殊的起始符（如 `<SOS>`），然后逐个生成输出序列 $y_1, y_2, ..., y_{T'}$，直到生成一个结束符（如 `<EOS>`）。

## 局限性

传统的编码器-解码器架构的一个主要局限是，它将整个输入序列压缩成一个固定长度的上下文向量。这使得模型在处理长序列时，难以有效地保留所有相关信息，容易出现信息瓶颈。

为了解决这个问题，**注意力机制（Attention Mechanism）**被引入到编码器-解码器架构中，允许解码器在生成每个输出时，动态地关注输入序列的不同部分。
