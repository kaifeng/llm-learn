# 损失函数

**损失函数**定义了模型要优化的目标，即衡量模型预测值与真实值之间的差距。损失函数输出一个数值，该数值代表了单次预测的好坏程度。损失值越大，说明模型的预测越不准确。训练的目标就是最小化损失值。

回归任务的目标是预测一个连续值。

- **均方误差 (Mean Squared Error, MSE)**：也称为 L2 损失。它计算的是预测值与真实值之差的平方的平均值。由于平方的存在，MSE 对较大的误差给予更重的惩罚。

    $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

    > 在一些教材或理论推导中，您可能会看到损失函数带有一个 $\frac{1}{2}$ 的系数，例如 $L = \frac{1}{2} \sum (y_i - \hat{y}_i)^2$。这个 $\frac{1}{2}$ 是为了在求导时，可以方便地与平方项求导后产生的系数 2 相抵消，使梯度表达式更简洁。对于优化过程而言，这个常数系数不影响梯度的方向，因此不会改变模型参数优化的最终结果。现代深度学习框架（如 PyTorch）的默认实现通常采用求均值（除以n）的方式。

- **平均绝对误差 (Mean Absolute Error, MAE)**：也称为 L1 损失。它计算的是预测值与真实值之差的绝对值的平均值。相比 MSE，MAE 对异常值不那么敏感，鲁棒性更强。

    $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

- **Huber 损失 (Huber Loss)**：这是一种“鲁棒”的损失函数，可以看作是 MSE 和 MAE 的结合。它通过一个超参数 $\delta$ 来控制：当误差小于 $\delta$ 时，它等同于 MSE（平方损失）；当误差大于 $\delta$ 时，它等同于 MAE（线性损失）。这使得它在误差较小时表现平滑，同时对大的异常值不那么敏感。

    $$
    L_\delta(y, f(x)) =
    \begin{cases}
    \frac{1}{2}(y - f(x))^2 & \text{for } |y - f(x)| \le \delta \\
    \delta \cdot (|y - f(x)| - \frac{1}{2}\delta) & \text{otherwise}
    \end{cases}
    $$

分类任务的目标是预测一个离散的类别。

- **交叉熵损失 (Cross-Entropy Loss)**：是分类任务中最常用的损失函数。其梯度是真实概率与预测概率的区别。它衡量的是模型预测的概率分布与真实的概率分布之间的差异。

    - **二元交叉熵 (Binary Cross-Entropy)**：用于二分类问题。输出层只有一个神经元，并使用 Sigmoid 激活函数输出一个概率值 $p$。

        $$ L = -[y \log(p) + (1-y) \log(1-p)] $$

        其中 $y$ 是真实标签（0 或 1），$p$ 是模型预测为类别 1 的概率。

    - **分类交叉熵 (Categorical Cross-Entropy)**：用于多分类问题。输出层有 N 个神经元（N 为类别数），并使用 Softmax 激活函数输出一个概率分布。

        $$ L = -\sum_{i=1}^{N} y_i \log(p_i) $$

        其中 $y_i$ 是一个独热编码的真实标签，$p_i$ 是模型预测为类别 $i$ 的概率。
