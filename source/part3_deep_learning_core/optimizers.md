# 优化器

优化器是根据反向传播计算出的梯度来更新网络参数（权重和偏置）的算法，它的目标是找到一组能使损失函数最小化的参数。

## 梯度下降

梯度下降(Gradient Descent)是所有优化算法的基础。其核心思想是：**沿着梯度下降最快的方向（梯度的反方向）调整参数，从而逐步减小损失值。**

更新规则如下：
`新参数 = 旧参数 - 学习率 × 梯度`

**学习率 (Learning Rate)** 是一个超参数，它控制着每次参数更新的步长。学习率过大可能导致模型在最优点附近震荡甚至发散；学习率过小则会导致模型收敛速度过慢。

根据每次更新使用的数据量，梯度下降分为三种变体：

1.  **批量梯度下降 (Batch Gradient Descent)**：每次更新都使用整个训练集的数据。计算精确但速度慢，且需要大量内存。
2.  **随机梯度下降 (Stochastic Gradient Descent, SGD)**：每次更新仅使用训练集中的一个样本。速度快，但更新方向不稳定，损失函数会剧烈波动。
3.  **小批量梯度下降 (Mini-batch Gradient Descent)**：是上述两者的折中，也是最常用的方法。每次更新使用一小批（mini-batch）数据（如 32, 64, 128 个样本）。它兼顾了计算效率和更新的稳定性。

## 高级优化器

为了解决 SGD 的一些问题（如容易陷入局部最优、收敛速度慢等），研究者们提出了许多更先进的优化器。

- **Momentum**：引入了“动量”的概念。它在更新参数时，不仅考虑当前的梯度，还考虑了历史的更新方向。这有助于加速收敛并冲出局部最优。就像一个从山上滚下来的球，它会保持之前的速度。

- **Adam (Adaptive Moment Estimation)**：是目前最流行、最常用的优化器之一。它结合了 Momentum 和另一种名为 RMSprop 的优化器的思想，能够为网络中的每一个参数计算自适应的学习率。Adam 通常能快速收敛，并且对超参数的选择不那么敏感，是许多任务的默认首选。