# LLM领域概览

LLM领域是一个快速发展且极其庞大的生态系统，可以从多个维度进行细分，以更好地理解其构成、技术栈和应用场景。

## 1. 按模型架构

这是 LLM 最基础的分类方式之一，主要基于 Transformer 模型的不同组件。

原始的 Transformer 模型是一个包含编码器（Encoder）和解码器（Decoder）的完整架构，它在机器翻译等序列到序列（Seq2Seq）任务上取得了巨大成功。然而，研究者们很快发现，根据任务性质的不同，只使用其中的一部分往往能达到更优的效果。这导致了 Transformer 架构的三个主要分支。

### Encoder-only 架构（编码器模型）

这种架构只使用 Transformer 的编码器部分。

-   **工作方式**：输入一个完整的句子，模型中的自注意力机制可以同时关注到句子中的所有词元（token），无视其前后位置。因此，它能生成每个词元在 **深度双向（deeply bidirectional）** 语境下的表示。它对整个句子的理解是全局性的。

-   **适用场景**：非常适合需要对已有文本进行深入理解的任务，而不是生成新文本。
    -   句子分类（如情感分析）
    -   命名实体识别（NER）
    -   完形填空（如 BERT 的 MLM 任务）
    -   从段落中提取答案的问答任务

-   **代表模型**：**BERT** 及其所有变体（如 RoBERTa, ALBERT, DistilBERT）。

### Decoder-only 架构（解码器模型）

这种架构只使用 Transformer 的解码器部分。

-   **工作方式**：其核心是**带掩码的自注意力（Masked Self-Attention）**。在处理或生成任何一个词元时，模型只能关注到它自己以及它之前的所有词元，无法看到未来的信息。这种从左到右的单向信息流使其本质上是一个 **自回归（Autoregressive）** 模型。

-   **适用场景**：天然适合于根据上文生成新文本的任务。
    -   文本续写、故事创作
    -   开放式对话、聊天机器人
    -   代码生成

-   **代表模型**：**GPT 系列**（GPT-1, 2, 3, 4）、**Llama 系列**、Mistral 等。这是当今绝大多数最强大的大语言模型所采用的架构。

### Encoder-Decoder 架构（序列到序列模型）

这种架构保留了 Transformer 的完整结构。

-   **工作方式**：编码器首先将整个输入序列编码成一个富含信息的中间表示。然后，解码器在自回归地生成输出序列的每一步，都会通过“编码器-解码器注意力”来关注编码器产出的完整输入信息。

-   **适用场景**：最适合需要将一个输入序列转换为另一个输出序列的任务，尤其是当输入和输出序列的格式、长度或语言不同时。
    -   机器翻译
    -   文本摘要
    -   代码注释生成

-   **代表模型**：原始的 Transformer 模型、**T5**、**BART**。

### 对比

| 架构分支 | 工作方式 | 核心优势 | 适用任务 | 代表模型 |
| :--- | :--- | :--- | :--- | :--- |
| **Encoder-only** | 双向上下文 | 深度理解 | NLU (理解) | BERT, RoBERTa |
| **Decoder-only** | 单向自回归 | 文本生成 | NLG (生成) | GPT, Llama, Mistral |
| **Encoder-Decoder** | 映射输入与输出 | 序列转换 | Seq2Seq (转换) | T5, BART, M2M-100 |

## 2. 按模型生命周期

LLM 的开发和应用涉及多个连续的阶段：

-   **预训练**：在海量无标签数据上进行自监督学习，使模型学习通用语言知识和世界模型。这是 LLM 智能的基石。相关技术有大规模分布式训练（如 Megatron-LM）、数据清洗与过滤。
-   **微调与对齐 (Fine-tuning & Alignment)**：
    -   **指令微调 (Instruction Fine-tuning)**：使模型学会遵循人类指令。
    -   **基于人类反馈的强化学习 (RLHF)**：进一步将模型行为与人类偏好和价值观对齐，提升有用性、无害性。
-   **部署与推理**：将训练好的模型投入实际使用，并优化其运行效率和成本。相关技术有高效推理引擎（如 vLLM）、模型量化、模型蒸馏。

## 3. 按模型类型与能力

-   **基础模型**：仅经过大规模预训练，尚未进行指令微调，主要用于文本补全。
-   **指令遵循模型**：经过指令微调，能更好地理解和执行人类指令。
-   **对话模型**：专门为多轮对话优化，具有更强的对话连贯性和交互性。
-   **多模态模型**：能够理解和生成多种模态的数据（如文本、图像、音频、视频）。
-   **领域专用模型**：针对特定行业或知识领域（如医疗、法律、金融）进行训练或微调，具有更专业的知识和表现。

## 4. 按工具与框架

支撑 LLM 开发和应用的软件生态系统：

-   **深度学习框架**：用于构建和训练神经网络。如 PyTorch, TensorFlow。
-   **高效推理框架**：优化 LLM 在生产环境中的推理速度和资源消耗。如 vLLM, TensorRT-LLM。
-   **统一算子框架**：用于编写高性能的自定义 GPU 算子。如 Triton。
-   **Agent 框架**：用于构建能够自主规划、工具使用和多步推理的 LLM 智能体。如 LangChain, LlamaIndex, AutoGen。

## 5. 按开放性

-   **闭源模型**：模型权重和架构不公开，通过 API 提供服务。如 OpenAI GPT 系列、Anthropic Claude 系列。
-   **开放权重模型**：模型权重公开，但可能附带使用许可限制。如 Llama 系列、Mistral 系列、Gemma。
-   **开源模型**：模型权重、代码和数据均公开，通常遵循宽松的开源许可。一般是一些较小的研究模型或社区驱动项目。

## Hugging Face上的常见模型类型

Hugging Face模型库中有数万个模型，它们可以根据其设计和应用场景分为几个主要类别。以下是一些常见的模型类型及其主要应用：

### 1. 自然语言处理 (NLP)

这是Hugging Face上最庞大、最成熟的模型类别。

-   **文本分类 (Text Classification)**:
    -   **应用**: 情感分析、主题识别、垃圾邮件检测。
    -   **常见模型**: `BERT`, `RoBERTa`, `DistilBERT`。
-   **命名实体识别 (Token Classification / NER)**:
    -   **应用**: 从文本中识别人名、地名、组织机构等特定实体。
    -   **常见模型**: `BERT`, `ELECTRA`。
-   **问答 (Question Answering)**:
    -   **应用**: 根据给定的上下文回答问题（抽取式问答），或直接生成答案（生成式问答）。
    -   **常见模型**: `BERT`, `DistilBERT` (抽取式), `T5`, `BART` (生成式)。
-   **文本生成 (Text Generation)**:
    -   **应用**: 续写文本、创作故事、生成代码。
    -   **常见模型**: `GPT-2`, `BLOOM`, `Llama`。
-   **摘要 (Summarization)**:
    -   **应用**: 将长篇文章缩减为简短的摘要。
    -   **常见模型**: `BART`, `T5`, `Pegasus`。
-   **翻译 (Translation)**:
    -   **应用**: 将文本从一种语言翻译成另一种语言。
    -   **常见模型**: `T5`, `MarianMT`, `M2M100`。
-   **特征提取 (Feature Extraction)**:
    -   **应用**: 将文本转换为固定大小的向量（嵌入），用于语义搜索、聚类等下游任务。
    -   **常见模型**: `BERT`, `RoBERTa`, `Sentence-Transformers`。

### 2. 计算机视觉 (Computer Vision)

-   **图像分类 (Image Classification)**:
    -   **应用**: 识别图像中的主要物体（例如，猫、狗、汽车）。
    -   **常见模型**: `ViT` (Vision Transformer), `ResNet`, `Swin Transformer`。
-   **目标检测 (Object Detection)**:
    -   **应用**: 在图像中定位多个物体并识别其类别。
    -   **常见模型**: `YOLOS`, `DETR`。
-   **图像分割 (Image Segmentation)**:
    -   **应用**: 对图像中的每个像素进行分类，以区分不同的物体和背景。
    -   **常见模型**: `SegFormer`, `Mask2Former`。

### 3. 音频处理 (Audio)

-   **音频分类 (Audio Classification)**:
    -   **应用**: 识别音频中的声音事件（如掌声、警报声）或关键词。
    -   **常见模型**: `Wav2Vec2`, `HuBERT`。
-   **自动语音识别 (Automatic Speech Recognition, ASR)**:
    -   **应用**: 将语音转换为文字。
    -   **常见模型**: `Whisper`, `Wav2Vec2`。

### 4. 多模态 (Multimodal)

这类模型能够同时处理多种信息模态（如文本和图像）。

-   **图文问答 (Visual Question Answering, VQA)**:
    -   **应用**: 根据图像内容回答相关问题。
    -   **常见模型**: `ViLT`, `BLIP`。
-   **图像描述生成 (Image Captioning)**:
    -   **应用**: 为图像生成描述性文字。
    -   **常见模型**: `BLIP`, `GIT`。
