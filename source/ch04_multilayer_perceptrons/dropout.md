# 暂退法 (Dropout)

暂退法（Dropout）是一种强大且广泛使用的正则化技术，专门用于减少神经网络中的过拟合。

## 核心思想

在模型训练的每一次迭代中，Dropout 会**随机**地“丢弃”隐藏层中的一部分神经元（即将其输出暂时设为 0）。“丢弃”的概率 $p$ 是一个可以设置的超参数。

这意味着，在每次前向传播和反向传播中，网络都是一个略微不同的“稀疏”版本。

## 工作原理

为了在丢弃一部分神经元后，不改变该层输出的期望值，需要对未被丢弃的神经元的输出进行缩放。假设丢弃概率为 $p$，那么一个神经元的输出 $x_i$ 会按如下方式进行扰动：

$$
 x&#39;_i=
\begin{cases}
0 & \text{with probability } p \\
\frac{x_i}{1-p} & \text{otherwise}
\end{cases}
$$

通过除以 $(1-p)$，可以保证输出的期望值 $E[x&#39;_i]$ 仍然等于 $x_i$。

## 为什么有效？

- **强制网络学习冗余表示**：由于任何一个神经元都可能在下一次迭代中被丢弃，网络不能过度依赖于任何一个或少数几个神经元的特定组合。它被迫学习到更具鲁棒性和冗余性的特征表示。
- **模型集成的近似**：Dropout 可以被看作是一种高效的模型集成（Ensemble）方法。在每次训练迭代中，我们都在训练一个不同的、从完整网络中采样得到的子网络。在测试时，我们使用完整的网络，这近似于对所有这些子网络进行平均预测。

**注意**：Dropout 只在模型**训练**时使用，在**推理（测试）**时会关闭，以确保得到一个确定的、稳定的输出。
