# 数值稳定性和模型初始化

在深度神经网络的训练过程中，尤其是在网络层数较深或使用某些特定的激活函数时，反向传播算法可能会遇到两个主要的数值稳定性问题：**梯度消失（Vanishing Gradient）**和**梯度爆炸（Exploding Gradient）**。合理的模型初始化是缓解这些问题的关键手段之一。

## 梯度消失

- **现象**：在反向传播过程中，梯度值随着网络层数的增加而指数级减小，导致靠近输入层的网络层（即浅层）的参数更新变得非常缓慢甚至停滞。这意味着浅层网络几乎无法学习到有用的特征。
- **原因**：
    1.  **链式法则的乘法效应**：在反向传播中，梯度是通过链式法则逐层相乘得到的。如果每层的梯度（例如，激活函数的导数）都小于 1，那么经过多层相乘后，梯度会迅速趋近于 0。
    2.  **激活函数**：传统的激活函数如 Sigmoid 和 Tanh，在输入值过大或过小时，其导数会非常接近 0。这使得这些区域的梯度几乎为零，导致梯度无法有效传播。
- **影响**：深层网络难以训练，模型无法捕捉长程依赖（在 RNN 中尤为明显）。
- **解决方法**：
    - **改进激活函数**：使用 ReLU 及其变体（如 Leaky ReLU）代替 Sigmoid 或 Tanh 函数，因为它们在正区间的导数恒为 1，能有效维持梯度流。
    - **改变网络结构**：
        - **残差连接 (Residual Connections)**：在 ResNet 等结构中，通过“快捷连接”将输入直接加到输出上，使得梯度可以通过“加法”路径直接回传，有效缓解了深层网络中因连乘导致的梯度衰减问题。
        - **门控机制 (Gating Mechanisms)**：在 LSTM 和 GRU 等循环网络中，通过精心设计的门（如遗忘门、输入门）来控制信息的流动和梯度的传播，使其能更好地捕捉长程依赖。
    - **归一化层**：使用批归一化（Batch Normalization），通过对每层的输入进行归一化，使其分布保持稳定，从而让梯度处于一个更健康的范围内。

## 梯度爆炸

- **现象**：与梯度消失相反，梯度值在反向传播过程中指数级增大，导致参数更新过大，模型权重剧烈震荡，甚至溢出（NaN），使得训练过程不稳定，模型无法收敛。
- **原因**：
    1.  **链式法则的乘法效应**：如果每层的梯度（例如，激活函数的导数或权重矩阵的范数）都大于 1，那么经过多层相乘后，梯度会迅速变得非常大。
    2.  **不合适的权重初始化**：如果初始权重过大，也容易导致梯度爆炸。
- **影响**：训练不稳定，损失函数值变为 NaN，模型无法学习。
- **解决方法**：
    - **梯度裁剪 (Gradient Clipping)**：这是最直接有效的控制方法。当梯度的范数超过某个预设的阈值时，就对其进行缩放，强制将其限制在一个合理的范围内。
    - **权重正则化与初始化**：
        - 使用 L1 或 L2 正则化来约束权重的大小。
        - 采用合理的权重初始化策略（如 Xavier, He 初始化），从一开始就让权重处于一个合适的范围内。
    - **归一化层**：同样，批归一化（Batch Normalization）也有助于缓解梯度爆炸问题。
