# 状态空间模型 (State Space Models)

Transformer 架构在 LLM 领域取得了巨大成功，但其核心的自注意力机制存在一个显著的缺点：计算和内存的复杂度会随着序列长度的增加而呈二次方增长，使得它在处理超长序列时效率低下。

状态空间模型（State Space Models, SSMs）作为一种新兴的序列模型架构，旨在解决这一问题，它在保持强大性能的同时，实现了更高的计算效率。

Mamba 是 SSM 架构的杰出代表，它通过引入选择性机制和硬件感知的算法设计，实现了对长序列数据的高效处理。Mamba 能够根据输入内容动态地决定哪些信息是重要的，应该被保留在状态中；哪些信息是次要的，可以被忽略。这使得模型能更灵活地压缩和记忆序列信息。Mamba 的计算复杂度随序列长度呈线性增长。这意味着在处理长文本、高分辨率图像或音频等长序列数据时，Mamba 相比 Transformer 具有显著的速度和内存优势。
