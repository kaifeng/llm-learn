# 模型量化与推理优化

随着大语言模型的参数规模越来越大，在消费级硬件上进行部署和推理变得极具挑战。模型量化和推理优化是解决这一问题的关键技术，旨在降低模型的计算和存储开销。

## 模型量化 (Quantization)

量化的核心思想是降低模型参数（权重）的数值精度，用更少的比特数来表示它们，从而达到压缩模型、减少显存占用的目的。

-   **基本原理**：标准的模型训练通常使用 32 位浮点数（FP32）或 16 位浮点数（BF16/FP16）。量化则将这些高精度数值转换为 8 位整数（INT8）甚至 4 位整数（INT4）。
-   **主流方案**：
    -   **GPTQ (Generative Pre-trained Transformer Quantization)**：一种训练后量化方法，它逐层对权重进行量化，并通过校准数据来最小化量化带来的精度损失。
    -   **GGUF (GPT-Generated Unified Format)**：由 `llama.cpp` 项目推出的文件格式，专门为在 CPU 上高效运行量化模型而设计，支持多种量化策略。
    -   **AWQ (Activation-aware Weight Quantization)**：一种激活感知量化方法，它认为并非所有权重都同等重要，通过跳过对模型性能影响大的权重来保护模型的性能。

## 推理优化 (Inference Optimization)

推理优化旨在通过算法和系统层面的改进来提升模型的计算速度。

-   **PagedAttention**：由 vLLM 项目提出，它借鉴了操作系统中虚拟内存和分页的思想，有效管理了注意力机制中的 Key 和 Value 缓存，极大地提升了吞吐量和内存利用率。
-   **Speculative Decoding (推测解码)**：用一个小的、计算速度快的草稿模型来生成一个 token 序列草稿，然后用大的、能力强的原始模型一次性地验证和修正这个草稿。在很多情况下，这比逐个生成 token 的传统方式要快得多。