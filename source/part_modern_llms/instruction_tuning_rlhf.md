# 指令微调与RLHF

经过大规模预训练的模型虽然知识渊博，但它们的核心任务仍然是文本补全。这意味着它们只会根据提示进行续写，不一定能理解用户的真实意图。
对齐（Alignment）则是为了解决这个问题，让模型变得更有用、更听话、更安全，最核心的就是 **指令微调（Instruction Fine-Tuning）** 和 **基于人类反馈的强化学习**。

指令微调的思想非常直观，希望模型学会遵循指令，最直接的方法就是用大量的指令-回答样本对来对它进行微调。

首先，需要一个高质量的、多样化的指令数据集。这个数据集由成千上万个样本组成，每个样本都包含一个描述了用户想要模型执行的任务的指令（例如，“总结以下段落”、“写一个 Python 函数来实现快速排序”），和一个期望的回答，由人类或更高质量的模型针对该指令给出的优质答案。然后以监督学习的方式对预训练好的大语言模型进行微调。经过指令微调后，模型就从一个基础模型变成了指令模型（Instruct Model），能够更好地泛化到没有见过的、新的指令上。

指令微调教会了模型做什么，但没有告诉它做得怎么样以及如何做得更好，RLHF就是为了让模型的回答更符合人类的偏好。

首先训练一个奖励模型 (Reward Model, RM)：

  1.  **收集对比数据**：选取一批指令，用指令微调过的模型为每个指令生成多个不同的回答。
  2.  **人类进行排序**：让人类标注者对这些回答进行排序，从最好到最差。这些排序数据体现了人类的偏好。
  3.  **训练奖励模型**：基于这些排序数据，训练一个奖励模型。这个模型接收一个“指令+回答”对作为输入，输出一个标量分数（奖励），这个分数代表了该回答有多符合人类的偏好。奖励模型的目标是给人类偏爱度高的回答打高分，给差的回答打低分。

通过强化学习微调语言模型。这一步不再改动奖励模型，而是用它来进一步优化语言模型。

  1.  **环境与策略**：我们将语言模型看作强化学习中的策略，它要学会在给定的环境（即用户输入的指令）中，做出一个动作（即生成一个回答）。
  2.  **奖励**：当语言模型生成一个回答后，奖励模型会为这个回答打分，这个分数就是强化学习中的奖励。
  3.  **优化**：使用强化学习算法（如 PPO，Proximal Policy Optimization），根据奖励信号来更新语言模型的参数。目标是让语言模型学会生成能够从奖励模型那里获得更高分数的回答。

  为了防止模型在追求高奖励时产生一些虽然得分高但内容混乱的文本，通常还会在优化目标中加入一个惩罚项，确保模型的输出与原始的、经过指令微调的模型输出不要偏离太远。
  这个过程可以重复进行：用优化过的新语言模型去收集新的对比数据，训练出更好的奖励模型，再用这个奖励模型去进一步优化语言模型。
