# 检索增强生成

大语言模型（LLM）虽然在其训练数据中编码了海量的知识，但它们存在两个固有的、难以解决的问题：

1.  **知识陈旧**：LLM 的知识被冻结在其训练数据截止的那个时间点，无法获知此后发生的新事件或出现的新信息。
2.  **幻觉 (Hallucination)**：在被问到其知识范围之外或不确定的问题时，LLM 倾向于编造听起来合理但实际上是错误的答案。

为了解决这些问题，同时让 LLM 能够基于私有的、特定的知识（如企业内部文档）来回答问题，**检索增强生成（Retrieval-Augmented Generation, RAG）** 应运而生。

## 核心思想

RAG 的核心思想非常直观：与其完全依赖模型内部的记忆，不如在回答问题之前，先让模型去一个外部知识库中“开卷查书”。

具体来说，当用户提出一个问题时，RAG 系统并不直接将问题发送给 LLM。相反，它会执行以下步骤：

1.  **检索 (Retrieve)**：系统首先将用户的问题作为一个查询，在一个外部的知识库（例如，公司的 PDF 文档、网站内容、数据库）中检索最相关的信息片段。
2.  **增强 (Augment)**：然后，系统将这些检索到的相关信息片段与用户的原始问题拼接在一起，形成一个内容更丰富、信息更具体的**新提示（Prompt）**。
3.  **生成 (Generate)**：最后，将这个增强后的新提示发送给 LLM。LLM 会基于这个提供了明确上下文的新提示来生成最终的回答。

通过这种方式，LLM 的角色从一个封闭的知识库转变为一个基于所提供材料的阅读理解和总结工具。

## 向量数据库

传统的数据库是为存储和查询结构化数据设计的，它们不擅长处理“哪个向量和我的查询向量最像”这类问题。向量数据库的核心能力是**相似性搜索（Similarity Search）**，它能在数百万甚至数十亿的向量中，快速找到与给定查询向量在语义空间中最接近的邻居。它是一种专门为高效存储、管理和搜索海量高维向量数据而设计的数据库。

没有向量数据库，要在海量文本中找到与问题相关的片段，就需要将问题与知识库中的每个文本块逐一比较，计算量巨大，速度极慢。向量数据库利用专门的索引算法（如 HNSW）将这个过程加速成百上千倍。
搜索不再是基于关键词匹配，而是基于语义相似度。

常见的向量数据库有：

- **Chroma**：一个开源、对开发者友好的向量数据库，非常适合快速上手和在本地运行。
- **Pinecone**：一个高性能的、完全托管的商业向量数据库服务。
- **Weaviate**：另一个功能丰富的开源向量数据库，支持复杂的查询和数据类型。
- **Milvus**：一个为大规模生产环境设计的、云原生的开源向量数据库。

## RAG 系统的典型架构

一个完整的 RAG 系统通常包含两个主要阶段：

- 数据准备 / 索引阶段 (离线)：这个阶段的目标是建立一个可供快速检索的知识库。

    -   **加载数据 (Load)**：从各种来源（如 PDF, HTML, Word 文档）加载原始数据。
    -   **切分 (Chunk/Split)**：将长文档切分成更小的、有意义的文本块（chunks）。切分的大小和策略对 RAG 的性能至关重要。
    -   **向量化 (Embed)**：使用一个**嵌入模型（Embedding Model）**（类似于 Word2Vec 或 BERT，但通常是为句子/段落优化的）将每个文本块转换成一个数值向量，即**向量嵌入（Vector Embedding）**。
    -   **存储/索引 (Store/Index)**：将所有的文本块及其对应的向量嵌入存储到一个 **向量数据库** 中。向量数据库经过特殊优化，可以根据向量之间的相似性进行极快速的检索。


- 查询 / 生成阶段 (在线)：这是用户与系统交互时发生的实时过程。

    -   **用户提问**：用户输入一个问题（查询）。
    -   **查询向量化**：使用同一个嵌入模型，将用户的查询也转换成一个向量。
    -   **相似性搜索**：在向量数据库中，使用这个查询向量去搜索与之最相似的文本块向量。最常用的搜索算法是**余弦相似度（Cosine Similarity）**。
    -   **构建提示**：从搜索结果中选取最相关的 Top-K 个文本块（例如，前 3 个），并将它们与用户的原始问题组合成一个增强的提示。
    -   **生成回答**：将增强后的提示喂给 LLM，生成最终的、基于检索到的知识的回答。

## RAG 的优势

-   **提高准确性，减少幻觉**：模型被要求基于提供的材料作答，而不是凭空捏造，从而大大减少了幻觉现象。
-   **知识实时更新**：当外部知识库更新时，只需重新对数据进行索引，而无需重新训练昂贵的 LLM。
-   **可追溯性与可解释性**：由于答案是基于检索到的特定文本块生成的，很容易将答案追溯到其信息来源，方便用户验证答案的准确性。
-   **实现私有知识问答**：企业可以将内部的、私密的文档构建成 RAG 知识库，从而创建一个能回答内部问题的助手，无需将私有数据用于模型训练。

RAG 是目前将大语言模型落地到实际应用中最重要、最实用的技术之一，它有效地将 LLM 强大的语言能力与外部知识库的准确性和实时性结合了起来。