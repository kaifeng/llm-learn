# 批量归一化 (Batch Normalization)

批量归一化（Batch Normalization，简称 BN）是深度学习中一个非常重要的技术，由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出。它旨在解决深度神经网络训练过程中“内部协变量偏移”（Internal Covariate Shift）的问题，从而加速训练、提高模型的稳定性和泛化能力。

## 内部协变量偏移

在深度神经网络中，每一层的输入都受到前一层参数变化的影响。这意味着在训练过程中，即使我们固定了当前层的参数，其输入分布也会不断变化。这种现象被称为“内部协变量偏移”。

内部协变量偏移会导致：
- **训练速度慢**：每一层都需要不断适应新的输入分布，使得学习率必须设置得很小，训练过程变得缓慢。
- **梯度消失/爆炸**：输入分布的变化可能导致激活函数进入饱和区，从而加剧梯度消失或爆炸问题。
- **对初始化敏感**：模型对参数的初始化更加敏感。

## 批量归一化的原理

批量归一化通过在网络的每一层（通常在激活函数之前）插入一个归一化操作来解决这个问题。对于每个小批量（mini-batch）数据，BN 层会：

1.  **计算均值和方差**：对当前小批量数据的每个特征维度计算其均值 $\mu_B$ 和方差 $\sigma_B^2$。
2.  **归一化**：将每个特征维度的数据 $x_i$ 归一化为均值为 0、方差为 1 的分布：
    $$ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $$
    其中 $\epsilon$ 是一个很小的常数，用于防止除以零。
3.  **缩放和平移**：为了保留模型的表达能力，BN 层还会引入两个可学习的参数 $\gamma$（缩放因子）和 $\beta$（平移因子），对归一化后的数据进行线性变换：
    $$ y_i = \gamma \hat{x}_i + \beta $$
    这两个参数允许网络学习恢复原始分布的任何变换，从而确保归一化不会损害模型的表达能力。

## 批量归一化的优点

- **加速训练**：允许使用更大的学习率，显著加快收敛速度。
- **提高稳定性**：减少了对参数初始化的依赖，使得训练过程更加稳定。
- **缓解过拟合**：具有轻微的正则化效果，减少了对 Dropout 等其他正则化技术的依赖。
- **缓解梯度消失/爆炸**：通过将激活值保持在非饱和区域，有助于梯度的有效传播。
