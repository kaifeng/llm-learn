# ResNet (Residual Network)

ResNet（Residual Network，残差网络）由 Kaiming He 等人在 2015 年提出，是计算机视觉领域最具影响力的架构之一。它通过引入**残差连接（Residual Connection）**，巧妙地解决了超深网络（超过 100 层甚至 1000 层）中的梯度消失问题，使得训练前所未有的深度模型成为可能。

## 深度网络的挑战

在 ResNet 出现之前，人们普遍认为增加神经网络的深度可以提升模型性能。然而，实践中发现，当网络层数增加到一定程度后，模型的性能反而会下降，这并非由于过拟合，而是因为**梯度消失**和**梯度爆炸**导致网络难以训练。

## 核心创新：残差连接

ResNet 的核心思想是引入了**残差块（Residual Block）**。在一个残差块中，输入 $x$ 不仅通过一系列卷积层得到输出 $F(x)$，还会被直接加到这个输出上，形成 $H(x) = F(x) + x$。这个直接相加的路径被称为**快捷连接（Shortcut Connection）**或**跳跃连接（Skip Connection）**。

![残差块](https://zh-v2.d2l.ai/_images/residual-block.svg)

## 为什么残差连接有效？

- **缓解梯度消失**：在反向传播时，梯度可以通过快捷连接直接回传到前面的层，避免了梯度在深层网络中逐层衰减的问题。这使得深层网络更容易训练。
- **学习残差函数**：残差连接使得网络不再直接学习一个复杂的映射 $H(x)$，而是学习一个残差函数 $F(x) = H(x) - x$。学习残差函数通常比学习原始函数更容易。如果最优映射是恒等映射（即 $H(x) = x$），那么网络只需要将 $F(x)$ 学习为 0 即可，这比直接学习恒等映射要容易得多。

## ResNet 架构

ResNet 通常由多个残差块堆叠而成，并结合了批量归一化（Batch Normalization）和 ReLU 激活函数。其深度可以达到 50 层、101 层、152 层甚至更深。

## 影响

ResNet 的提出是深度学习领域的一个里程碑，它极大地推动了深度学习模型在图像识别、目标检测等计算机视觉任务上的发展，并启发了后续许多更深、更复杂的网络架构设计。
