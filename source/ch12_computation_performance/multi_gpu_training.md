# 多 GPU 训练

当深度学习模型变得越来越大，或者训练数据集非常庞大时，单个 GPU 的计算能力和内存容量可能不足以满足需求。多 GPU 训练（Multi-GPU Training）技术允许我们将训练任务分布到多个 GPU 上，从而显著加速训练过程。

## 核心思想

多 GPU 训练的目标是利用多个 GPU 的并行计算能力来缩短训练时间。主要有两种常见的策略：

1.  **数据并行 (Data Parallelism)**：
    -   这是最常见的多 GPU 训练方式。
    -   每个 GPU 都拥有模型的完整副本。
    -   每个 GPU 处理不同批次的数据。
    -   在每个训练步骤中，每个 GPU 独立计算其分配到的数据批次的梯度。
    -   所有 GPU 计算出的梯度会被聚合（通常是求平均），然后用于更新所有 GPU 上的模型参数。
    -   这种方式的优点是实现相对简单，且扩展性好。

2.  **模型并行 (Model Parallelism)**：
    -   当模型本身太大，无法完全放入单个 GPU 的内存时，需要使用模型并行。
    -   模型的不同层或不同部分被放置在不同的 GPU 上。
    -   数据在 GPU 之间流动，每个 GPU 只负责模型的一部分计算。
    -   这种方式实现起来更复杂，需要仔细设计模型的划分策略。

## 深度学习框架中的实现

现代深度学习框架（如 PyTorch 的 `DistributedDataParallel`，TensorFlow 的 `tf.distribute.Strategy`）提供了高级 API 来简化多 GPU 训练的实现。这些 API 通常会自动处理数据分发、梯度聚合和参数同步等复杂细节。

## 挑战

- **通信开销**：多 GPU 之间的数据传输和梯度同步会引入通信开销，这可能成为性能瓶颈。
- **负载均衡**：确保每个 GPU 都有均衡的计算负载是提高效率的关键。
- **内存管理**：在模型并行中，如何有效地划分模型以适应不同 GPU 的内存限制是一个挑战。
