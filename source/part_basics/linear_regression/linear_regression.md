# 线性回归

线性回归通过学习一个线性函数来对输入特征和输出结果之间的关系进行建模。

一个预测值 $\hat{y}$ 是通过以下方式计算的：

$$
\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \mathbf{W}^T \mathbf{X} + b
$$

其中：
- $\mathbf{X} = (x_1, x_2, ..., x_n)$ 是输入特征向量。
- $\mathbf{W} = (w_1, w_2, ..., w_n)$ 是模型的权重，表示每个特征的重要性。
- $b$ 是偏置项。

学习过程的目标就是找到最优的权重 $\mathbf{W}$ 和偏置 $b$。

# Softmax回归

Softmax的作用是将一个包含任意实数值的向量（称为 logits）转换为一个概率分布。每个元素的值都在 (0, 1) 之间，并且所有元素的总和为 1。
这使得模型的输出可以直接解释为输入样本属于每一个类别的概率，专门用于多分类问题的输出层。Softmax回归实际是一个分类问题，分类预测一个离散类别（输出的个数是类别的个数）。

  $$ \sigma_i = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}, \quad i=1,2,...,n $$

