# 多层感知机

多层感知机（Multilayer Perceptron, MLP）是人工神经网络的一种，它在结构上比简单的感知机更为复杂，能够处理和学习非线性模式。MLP是深度学习的基础模型之一，广泛应用于分类、回归等任务。

## 结构

一个典型的多层感知机包含以下几层：

*   **输入层 (Input Layer)**：接收原始数据输入。每个节点代表输入数据的一个特征。
*   **隐藏层 (Hidden Layers)**：MLP可以有一个或多个隐藏层。这些层位于输入层和输出层之间，负责对输入数据进行复杂的非线性变换和特征提取。隐藏层的数量和每层的神经元数量是MLP模型复杂度的关键决定因素。
*   **输出层 (Output Layer)**：产生模型的最终输出。输出层的神经元数量取决于任务类型（例如，二分类任务通常有一个输出神经元，多分类任务则有与类别数量相等的输出神经元）。

## 工作原理

MLP的工作原理可以概括为以下几点：

1.  **加权求和**：每个神经元接收来自前一层神经元的输入，并将这些输入与对应的权重相乘，然后将所有加权输入求和，并加上一个偏置项（bias）。

    $$ z = \sum_{i=1}^{n} (w_i x_i) + b $$

    其中，$x_i$ 是输入，$w_i$ 是权重，$b$ 是偏置。

2.  **激活函数 (Activation Function)**：加权求和的结果会通过一个非线性激活函数。激活函数引入了非线性，使得MLP能够学习和逼近复杂的非线性函数。常用的激活函数包括：
    *   **Sigmoid**：将输入压缩到0到1之间。
    *   **ReLU (Rectified Linear Unit)**：$f(x) = \max(0, x)$，在深度学习中非常流行。
    *   **Tanh (Hyperbolic Tangent)**：将输入压缩到-1到1之间。

3.  **层间传递**：一个隐藏层的输出会作为下一个隐藏层（或输出层）的输入，这个过程逐层进行，直到输出层产生最终结果。

## 训练

MLP的训练通常采用**反向传播 (Backpropagation)** 算法。反向传播是一种高效的梯度下降算法，用于调整网络中的权重和偏置，以最小化模型预测与真实标签之间的误差（由损失函数衡量）。

训练过程包括：

1.  **前向传播 (Forward Pass)**：输入数据通过网络，计算出预测输出。
2.  **计算损失 (Loss Calculation)**：根据预测输出和真实标签，计算损失函数的值。
3.  **反向传播 (Backward Pass)**：计算损失函数对每个权重和偏置的梯度。
4.  **权重更新 (Weight Update)**：使用梯度下降（或其变种，如Adam、SGD等优化器）来更新网络的权重和偏置，以减小损失。

## 优点与局限性

**优点**：
*   能够学习和表示复杂的非线性关系。
*   适用于多种任务，如分类、回归、模式识别等。
*   是许多更复杂深度学习模型的基础。

**局限性**：
*   容易陷入局部最优。
*   对超参数（如学习率、隐藏层数量、神经元数量）的选择敏感。
*   在处理高维数据（如图像）时，参数数量巨大，容易过拟合，且计算成本高。
*   梯度消失/爆炸问题在深层网络中较为常见。

## 总结

多层感知机是理解神经网络和深度学习的关键一步。通过引入隐藏层和非线性激活函数，MLP克服了简单感知机的局限性，为更强大的深度学习模型奠定了基础。
