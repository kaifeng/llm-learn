# 概述

深度学习是机器学习的一个子领域，专注于使用深度神经网络（包含多个隐藏层的神经网络）来学习数据中的复杂模式。深度学习的深度指的是网络中层数的增加，这使得模型能够自动从原始数据中提取多层次、抽象的特征。大语言模型（LLM）是深度学习在自然语言处理领域最前沿的应用之一。

从早期的统计方法到如今的生成式 AI，机器学习与大语言模型的发展经历了几个关键的变革阶段。

**经典机器学习时代 (~1980s - ~2010s)**：这一时期主要由统计学习理论驱动，通过数学和统计模型（如几何、概率）来寻找数据中的规律。模型通常需要人工进行大量的特征工程。
这一时期产生了决策树（Decision Trees）、支持向量机（SVM）、{term}`逻辑回归`、Boosting 算法等。

**深度学习革命 (~2012 - ~2017)**：随着计算能力（尤其是 GPU）的提升和大规模数据集的出现，深度神经网络开始展现出超越传统方法的巨大潜力。它使用包含多个隐藏层的深层神经网络，自动从原始数据中学习层次化的特征表示。**AlexNet (2012)** 在 ImageNet 图像识别竞赛中取得压倒性胜利，引爆了深度学习的浪潮。
**RNN/LSTM** 在处理序列数据（如语音识别、机器翻译）方面取得了显著进展。

**Transformer 与预训练模型时代 (~2017 - ~2022)**：{term}`Transformer` 是一种全新的网络架构，它完全抛弃循环和卷积结构，仅依赖 **注意力机制（Attention Mechanism）** 来捕捉长距离依赖，并利用其高度并行的特性在海量无标签数据上进行预训练。
**BERT (2018)** 由 Google 推出，其“掩码语言模型”的预训练方式使其成为理解（NLU）任务的王者，推广了“预训练-微调”范式。**GPT 系列 (2018-2021)** 由 OpenAI 推出，其自回归的预训练方式在文本生成（NLG）任务上表现出色，展示了模型规模（Scaling Law）带来的惊人能力。

**ChatGPT 时刻与大模型生态爆发 (2022 - 至今)**：通过 {term}`指令微调` 和 {term}`基于人类反馈的强化学习`，将强大的基础模型“对齐”到人类的意图和偏好上，使其变得有用且易于交互。**ChatGPT (2022)** 不是一个全新的模型，而是将 GPT-3.5 模型通过对齐技术精心调教后的产物，它的成功引爆了全球对生成式 AI 的关注。
以 Meta 的 **Llama** 系列为代表的开源模型，极大地促进了社区的研究和应用创新。**GPT-4V**, **Gemini** 等多模态大模型，使模型不再局限于文本，开始能够同时理解和处理图像、音频等多种信息。
