# 神经网络

## 感知机 (Perceptron)

感知机可以被看作是神经网络最基础、最简单的原型，是 20 世纪 50 年代末期被提出的最早的 AI 模型之一。它是一个二分类模型，其结构与单个神经元非常相似，但使用了一个简单的阶跃函数（Step Function）作为激活函数：

$
\sigma(x)=
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$

然而，感知机只能学习线性可分的数据模式，这一局限性在当时导致了人工智能领域的第一次寒冬。

## 线性模型

线性模型通过学习一个线性函数来对输入特征和输出结果之间的关系进行建模，是机器学习中最基本且应用最广泛的一类模型。

线性模型的核心假设是，输出变量可以表示为输入特征的线性组合。一个预测值 $\hat{y}$ 是通过以下方式计算的：

$$
\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \mathbf{W}^T \mathbf{X} + b
$$

其中：
- $\mathbf{X} = (x_1, x_2, ..., x_n)$ 是输入特征向量。
- $\mathbf{W} = (w_1, w_2, ..., w_n)$ 是模型的权重，表示每个特征的重要性。
- $b$ 是偏置项（bias）。

学习过程的目标就是找到最优的权重 $\mathbf{W}$ 和偏置 $b$。

根据任务的不同，线性模型主要分为：

- 线性回归 (Linear Regression)

  用于解决回归问题，即预测一个连续的数值输出。线性回归的目标是找到一条直线（或超平面），使其尽可能地拟合训练数据。

- 逻辑回归 (Logistic Regression)

  用于解决分类问题。它通过一个 Sigmoid (Logistic) 函数将线性模型的输出映射到 (0, 1) 区间，表示属于某个类别的概率。


## 神经网络

神经网络的基本计算单元是神经元（或称为节点）。神经元接收来自其他神经元或外部数据源的多个输入信号（$x_1, x_2, ..., x_n$）。每个输入信号都有一个关联的**权重（weight, $w_i$）**，代表该输入的重要性。神经元将所有输入的加权值相加，并加上一个**偏置（bias, $b$）**。偏置允许微调输出。

$$ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b = \sum_{i=1}^{n} w_ix_i + b $$

加权求和的结果会被送入一个激活函数得到该神经元的最终输出 $ a = f(z) $。激活函数的作用是向网络中引入非线性，否则神经网络最终输出都只是输入的线性组合，无法学习复杂的数据模式。

![神经网络结构](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png)

一个典型的神经网络由输入层、隐藏层和输出层组成。

**输入层（Input Layer）** 接收原始数据。该层的神经元数量通常等于数据特征的数量。

**隐藏层（Hidden Layers）** 位于输入层和输出层之间，负责提取数据中的各种特征。一个神经网络可以没有隐藏层（如逻辑回归），也可以有一个或多个隐藏层。拥有多个隐藏层的网络通常被称为“深度神经网络”（Deep Neural Networks）。

**输出层（Output Layer）** 产生最终的预测结果。该层的神经元数量和激活函数取决于具体的任务。回归任务通常只有一个输出神经元，且不使用或使用线性激活函数。
分类任务的输出神经元的数量等于类别的数量，通常使用 Softmax 将输出转换为概率分布。

神经网络的学习过程（训练），本质上是一个寻找最优参数（权重和偏置）的迭代优化过程。其目标是让模型对训练数据的预测结果与真实标签之间的损失最小化。

这个过程的核心是 **梯度下降 (Gradient Descent)** 算法，它包含前向传播、计算损失、反向传播和参数更新等一系列步骤。训练完成后，模型即可用于 **推理 (Inference)**，对新数据进行预测。

## 多层感知机 (Multi-Layer Perceptron, MLP)

多层感知机通过在输入层和输出层之间加入一个或多个隐藏层，并使用非线性的激活函数（如 Sigmoid, Tanh, ReLU），能够处理和学习非线性模式，广泛应用于分类、回归等任务。

一个典型的多层感知机包含以下几层：

*   **输入层 (Input Layer)**：接收原始数据输入。每个节点代表输入数据的一个特征。
*   **隐藏层 (Hidden Layers)**：MLP可以有一个或多个隐藏层。这些层位于输入层和输出层之间，负责对输入数据进行复杂的非线性变换和特征提取。隐藏层的数量和每层的神经元数量是MLP模型复杂度的关键决定因素。
*   **输出层 (Output Layer)**：产生模型的最终输出。输出层的神经元数量取决于任务类型（例如，二分类任务通常有一个输出神经元，多分类任务则有与类别数量相等的输出神经元）。

MLP的工作原理可以概括为以下几点：

1.  **加权求和**：每个神经元接收来自前一层神经元的输入，并将这些输入与对应的权重相乘，然后将所有加权输入求和，并加上一个偏置项（bias）。

    $$ z = \sum_{i=1}^{n} (w_i x_i) + b $$

    其中，$x_i$ 是输入，$w_i$ 是权重，$b$ 是偏置。

2.  **激活函数 (Activation Function)**：加权求和的结果会通过一个非线性激活函数。激活函数引入了非线性，使得MLP能够学习和逼近复杂的非线性函数。常用的激活函数包括：
    *   **Sigmoid**：将输入压缩到0到1之间。
    *   **ReLU (Rectified Linear Unit)**：$f(x) = \max(0, x)$，在深度学习中非常流行。
    *   **Tanh (Hyperbolic Tangent)**：将输入压缩到-1到1之间。

3.  **层间传递**：一个隐藏层的输出会作为下一个隐藏层（或输出层）的输入，这个过程逐层进行，直到输出层产生最终结果。

MLP的训练通常采用**反向传播 (Backpropagation)** 算法。反向传播是一种高效的梯度下降算法，用于调整网络中的权重和偏置，以最小化模型预测与真实标签之间的误差（由损失函数衡量）。

训练过程包括：

1.  **前向传播 (Forward Pass)**：输入数据通过网络，计算出预测输出。
2.  **计算损失 (Loss Calculation)**：根据预测输出和真实标签，计算损失函数的值。
3.  **反向传播 (Backward Pass)**：计算损失函数对每个权重和偏置的梯度。
4.  **权重更新 (Weight Update)**：使用梯度下降（或其变种，如Adam、SGD等优化器）来更新网络的权重和偏置，以减小损失。

**优点**：
*   能够学习和表示复杂的非线性关系。
*   适用于多种任务，如分类、回归、模式识别等。
*   是许多更复杂深度学习模型的基础。

**局限性**：
*   容易陷入局部最优。
*   对超参数（如学习率、隐藏层数量、神经元数量）的选择敏感。
*   在处理高维数据（如图像）时，参数数量巨大，容易过拟合，且计算成本高。
*   梯度消失/爆炸问题在深层网络中较为常见。

## 激活函数

激活函数被应用于神经元的输出，为其引入非线性。如果没有非线性激活函数，无论神经网络有多少层，其本质上都只是一个线性模型，无法学习和表示复杂的数据模式。

- **Sigmoid** 函数将任意实数输入压缩到 (0, 1) 的范围内，可以被解释为概率，常用于二分类问题的输出层或作为门控机制（如 LSTM 中的门）。

  $$ f(x) = \frac{1}{1 + e^{-x}} $$

  但是当输入值非常大或非常小时，函数的梯度会趋近于0，在反向传播过程中，会导致深层网络的梯度信号变得非常微弱，使得网络难以训练。

- **Tanh** (双曲正切) 函数可以看作是 Sigmoid 函数的一个缩放和移位版本，它将输入压缩到 (-1, 1) 的范围内。通常比 Sigmoid 函数有更好的性能，收敛速度更快，但仍然存在梯度消失的问题。

  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

- **ReLU** 是目前在深度学习的隐藏层中最常用的激活函数，只涉及一个简单的阈值操作。

  $$ f(x) = \max(0, x) $$

  当输入为正数时，其梯度恒为 1，这使得梯度能够很好地在网络中传播。但是，如果一个神经元的输入在训练过程中持续为负，那么它的梯度将永远为0，导致该神经元的权重无法再被更新。

- **Leaky ReLU** 是对 ReLU 的改进，它允许在输入为负时也有一个小的、非零的梯度。$\alpha$ 是一个很小的常数（如 0.01）。

  $$ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$

- **Softmax** 的作用是将一个包含任意实数值的向量（称为 logits）转换为一个概率分布。每个元素的值都在 (0, 1) 之间，并且所有元素的总和为 1。
  这使得模型的输出可以直接解释为输入样本属于每一个类别的概率，专门用于多分类问题的输出层。

  $$ \sigma_i = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}, \quad i=1,2,...,n $$

Softmax虽然叫回归，实际是一个分类问题。分类和回归的区别：回归估计一个连续值，分类预测一个离散类别（输出的个数是类别的个数）。
