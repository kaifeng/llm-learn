# 训练和推理

## 数据预处理

在将原始数据用于训练机器学习模型之前，通常需要对其进行一系列的准备和转换。

数据预处理涵盖了多种技术，主要包括：

- **数据清洗**：处理数据中的错误、缺失值和异常值。例如，填充缺失的数值，或删除无法修复的样本。
- **数据转换**：将数据转换成更适合模型学习的形式。**归一化** 和 **标准化** 是其中最常见的技术。
- **特征工程**：创建新的特征或选择最有用的特征。这可能包括对现有特征进行组合、分解或编码（如将类别标签转换为数字）。

归一化(Normalization)的主要目的是将输入数据调整到一个统一的尺度，通常意味着将数据转换到特定的范围（如 $[0, 1]$ 或 $[-1, 1]$），或者使其均值为 0、方差为 1。

- **加速训练**：当不同特征的数值范围相差很大时，梯度下降算法的收敛速度会非常慢。归一化可以使数据空间变得更圆，使得梯度下降能更直接地找到最优解。
- **防止梯度爆炸/消失**：在某些激活函数中，输入值过大或过小会导致梯度变得非常小。归一化可以确保输入在激活函数的敏感区域内。
- **提高模型精度**：有些算法（如 K-近邻、支持向量机等）对特征的尺度非常敏感。归一化可以确保所有特征在训练过程中都得到平等的对待。

常见的归一化方法有：

- **最小-最大归一化**：将数据线性地缩放到一个固定的区间，通常是 $[0, 1]$。
    $$ x_{new} = \frac{x - x_{min}}{x_{max} - x_{min}} $$
- **Z-Score 归一化**：将数据转换为均值为 0、标准差为 1 的分布。这是最常用的方法之一。
    $$ x_{new} = \frac{x - \mu}{\sigma} $$

## 模型训练

神经网络的学习过程，即 **训练**，本质上是一个参数优化的过程，目标是找到一组最佳的权重和偏置，使得网络对给定输入的预测结果与真实标签之间的损失最小。

这个过程通常通过 **梯度下降（Gradient Descent）** 的算法来迭代完成，其核心循环包括四个关键步骤：

1.  **前向传播（Forward Propagation）**：将一批训练数据输入网络，数据从输入层流向输出层，最终得到预测结果。
2.  **计算损失（Loss Calculation）**：使用一个**损失函数**（如均方误差或交叉熵）来量化预测结果与真实标签之间的差距。
3.  **反向传播（Backpropagation）**：这是学习过程的核心。算法会从输出层开始，反向计算损失函数相对于网络中每一个权重和偏置的梯度（偏导数）。
4.  **参数更新（Weight Update）**：使用优化器（如 Adam、SGD）根据计算出的梯度来更新网络中的所有权重和偏置，以期在下一步减小损失。

### 欠拟合、过拟合

一些概念：
- 训练误差：模型在训练数据上的误差
- 泛化误差：模型在新数据上的误差
- 验证数据集：用于评估模型好坏的数据集，不要跟训练数据混在一起。
- 测试数据集：只用一次的数据集，不能用于调超参数。非大数据集上通常使用K-折交叉验证：将训练数据分割成K块。使用第i块作为验证数据集，其余的作为训练数据集。报告K个验证集误差的平均。常用K=5或10。
- 模型容量：拟合各种函数的能力。VC维（深度学习中很少使用）。模型容量需要匹配数据复杂度。

在训练机器学习模型时，我们的目标是让模型不仅在训练数据上表现良好，更重要的是在未见过的、新的测试数据上同样表现出色。这种在新数据上的表现能力被称为模型的 **泛化能力（Generalization）**。而过拟合与欠拟合是影响模型泛化能力的两种最常见的问题。

欠拟合 (Underfitting)是指模型在训练集和测试集上的表现都非常差。通常是因为模型过于简单，无法捕捉到数据中复杂的模式和规律。
解决方法为增加模型复杂度（例如增加神经网络的层数或神经元数量）、增加模型的训练轮次等。

过拟合 (Overfitting)是指模型在训练集上表现极好，但在测试集上表现很差。通常是模型过于复杂，以至于把训练数据中的噪声和随机波动也当作规律学习进去，而不是学习数据背后真正的、普适的模式。通常需要降低模型复杂度，或采用正则化（Regularization）等技术。

### 梯度消失与梯度爆炸

在深度神经网络的训练过程中，尤其是在网络层数较深或使用某些特定的激活函数时，反向传播算法可能会遇到两个主要问题：**梯度消失（Vanishing Gradient）**和**梯度爆炸（Exploding Gradient）**。

梯度消失是指在反向传播过程中，梯度值随着网络层数的增加而指数级减小，导致靠近输入层的网络层（即浅层）的参数更新变得非常缓慢甚至停滞。这意味着浅层网络几乎无法学习到有用的特征。深层网络难以训练，模型无法捕捉长程依赖（在 RNN 中尤为明显）。
- **原因**：
    1.  **链式法则的乘法效应**：在反向传播中，梯度是通过链式法则逐层相乘得到的。如果每层的梯度（例如，激活函数的导数）都小于 1，那么经过多层相乘后，梯度会迅速趋近于 0。
    2.  **激活函数**：传统的激活函数如 Sigmoid 和 Tanh，在输入值过大或过小时，其导数会非常接近 0。这使得这些区域的梯度几乎为零，导致梯度无法有效传播。
- **解决方法**：
    - **改进激活函数**：使用 ReLU 及其变体（如 Leaky ReLU）代替 Sigmoid 或 Tanh 函数，因为它们在正区间的导数恒为 1，能有效维持梯度流。
    - **改变网络结构**：
        - **残差连接 (Residual Connections)**：在 ResNet 等结构中，通过“快捷连接”将输入直接加到输出上，使得梯度可以通过“加法”路径直接回传，有效缓解了深层网络中因连乘导致的梯度衰减问题。
        - **门控机制 (Gating Mechanisms)**：在 LSTM 和 GRU 等循环网络中，通过精心设计的门（如遗忘门、输入门）来控制信息的流动和梯度的传播，使其能更好地捕捉长程依赖。
    - **归一化层**：使用批归一化（Batch Normalization），通过对每层的输入进行归一化，使其分布保持稳定，从而让梯度处于一个更健康的范围内。

与梯度消失相反，梯度值在反向传播过程中指数级增大，导致参数更新过大，模型权重剧烈震荡，甚至溢出（NaN），使得训练过程不稳定，模型无法收敛。
- **原因**：
    1.  **链式法则的乘法效应**：如果每层的梯度（例如，激活函数的导数或权重矩阵的范数）都大于 1，那么经过多层相乘后，梯度会迅速变得非常大。
    2.  **不合适的权重初始化**：如果初始权重过大，也容易导致梯度爆炸。
- **解决方法**：
    - **梯度裁剪 (Gradient Clipping)**：这是最直接有效的控制方法。当梯度的范数超过某个预设的阈值时，就对其进行缩放，强制将其限制在一个合理的范围内。
    - **权重正则化与初始化**：
        - 使用 L1 或 L2 正则化来约束权重的大小。
        - 采用合理的权重初始化策略（如 Xavier, He 初始化），从一开始就让权重处于一个合适的范围内。
    - **归一化层**：同样，批归一化（Batch Normalization）也有助于缓解梯度爆炸问题。


### Batch, Iteration和Epoch

- **Batch (批次)**：由于整个数据集可能非常大，一次性将其全部加载到内存中并计算梯度是不现实的。因此通常将数据集分成若干个小的子集，这些子集被称为批次。深度学习框架主要采用的是无放回采样 (Sampling without Replacement) ，在一个 Epoch 开始时，将整个数据集（例如全部 1000 个样本）的索引进行一次完全的随机打乱，然后，按照打乱后的新顺序，从头到尾依次将数据切分成一个个批次。保证了每个样本在一个 Epoch 中恰好被使用一次。在某些特定的研究场景下可能会用到有放回采样 (Sampling with Replacement)，例如处理极度不均衡的数据集。
- **Iteration (迭代)**：指完成一次批次数据的前向传播、反向传播和参数更新的过程。这是参数更新的最小单位。
- **Epoch (轮次)**：指整个训练数据集中的所有样本都已经被模型处理了一遍。如果一个数据集有 1000 个样本，批次大小（Batch Size）为 100，那么一个 Epoch 就包含 10 次迭代。


### 批次的采样策略

在将数据集划分为多个批次时，标准的做法是 **无放回采样 (Sampling without Replacement)**，它保证每个样本在一个 Epoch 中恰好被使用一次。

1.  **随机打乱**：在一个 Epoch 开始时，将整个数据集的索引进行一次完全的随机打乱。
2.  **顺序切分**：然后，按照打乱后的新顺序，从头到尾依次将数据切分成一个个批次。

这个过程好比“洗一副牌，然后依次发牌”，确保了每个样本在一个轮次中都会被看到且仅看到一次，这能确保模型稳定、完整地学习整个数据集。

### 权重初始化

在训练开始前，模型的权重必须被初始化为一个初始值。这个看似简单的步骤对模型的收敛速度和最终性能至关重要。糟糕的初始化可能导致梯度消失或梯度爆炸。

- **为什么需要初始化？**：如果所有权重都初始化为 0，网络中的所有神经元将学习到完全相同的特征，这被称为“对称性”问题。随机初始化可以打破这种对称性。

- **Xavier (Glorot) 初始化**：一种经典的初始化方法。它的核心思想是使每一层的激活值的方差和梯度的方差在前向和反向传播中保持一致。这有助于防止梯度信号在网络中传播时变得过大或过小。它通常与 Sigmoid 和 Tanh 等激活函数配合使用效果较好。

- **He 初始化**：这是对 Xavier 初始化的改进，专门为 **ReLU** 及其变体设计。由于 ReLU 会将一半的输入置为零，He 初始化在计算方差时考虑了这一点，从而更适合 ReLU 激活函数，成为现代神经网络中非常常用的初始化方法。

# 模型推理

**模型推理（Inference）** 是指使用已经训练好的、参数固定不变的模型对新的、未见过的数据进行预测的过程。

与训练过程不同，推理过程要简单得多，它只包含 **前向传播（Forward Propagation）** 这一步。数据从输入层流向输出层，网络直接给出预测结果，过程中不计算损失，也没有反向传播和参数更新。

此外，在推理阶段，一些仅用于训练的特定技术需要被关闭或切换到“推理模式”，以确保预测结果的确定性和一致性：

-   **Dropout**：必须被关闭。在训练时，它随机丢弃神经元以防止过拟合；在推理时，所有神经元都必须参与计算，以发挥模型的最大能力。
-   **批归一化 (Batch Normalization)**：会切换到推理模式，使用在整个训练集上估算出的固定的均值和方差，而不是基于当前批次的数据。

训练和推理对比如下：

| 特征 | 训练 | 推理 |
| :--- | :--- | :--- |
| **过程** | 包括前向传播、计算损失、反向传播和参数更新。 | 只有前向传播。 |
| **数据** | 需要大量的**有标签**训练数据。 | 处理无标签的新数据。 |
| **计算成本**| 非常高，需要大量的计算资源和时间。 | 相对较低，速度快。 |
| **模型状态**| 模型参数在每次迭代中都会被修改。 | 模型参数是固定不变的。 |
| **特定技术**| Dropout、Batch Normalization 等技术处于“训练模式”，以帮助模型泛化。 | Dropout 被关闭，Batch Normalization 使用固定的统计值。 |
