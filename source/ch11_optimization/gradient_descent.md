# 梯度下降 (Gradient Descent)

梯度下降（Gradient Descent）是所有优化算法的基础。其核心思想是：**沿着梯度下降最快的方向（梯度的反方向）调整参数，从而逐步减小损失值。**

## 更新规则

参数更新规则如下：

$$ \text{新参数} = \text{旧参数} - \text{学习率} \times \text{梯度} $$

其中：
- **学习率 (Learning Rate)** 是一个超参数，它控制着每次参数更新的步长。学习率过大可能导致模型在最优点附近震荡甚至发散；学习率过小则会导致模型收敛速度过慢。
- **梯度** 是损失函数对参数的偏导数，表示损失函数在当前参数位置的变化率和方向。

## 梯度下降的直观理解

想象你站在一座山上，目标是走到山谷的最低点。你无法看到整个山谷的全貌，但你可以感受到当前位置的坡度（梯度）。梯度下降的策略就是：每次都朝着当前位置最陡峭的下坡方向迈一小步。重复这个过程，最终你就能到达山谷的最低点（或接近最低点）。

## 梯度下降的挑战

- **局部最优**：对于非凸损失函数，梯度下降可能会陷入局部最优解，而不是全局最优解。
- **收敛速度**：在平坦区域，梯度很小，收敛速度会非常慢；在陡峭区域，梯度很大，可能导致震荡。
- **超参数敏感**：学习率的选择对算法的性能至关重要。
