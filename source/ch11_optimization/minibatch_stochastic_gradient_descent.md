# 小批量随机梯度下降 (Mini-batch Stochastic Gradient Descent)

小批量随机梯度下降（Mini-batch Stochastic Gradient Descent）是批量梯度下降（Batch Gradient Descent）和随机梯度下降（Stochastic Gradient Descent, SGD）的折中方案，也是目前深度学习中最常用、最有效的优化算法。

## 核心思想

小批量 SGD 每次更新参数时，不使用整个训练集，也不只使用一个样本，而是使用训练集中的一小批（mini-batch）样本来计算梯度。

## 更新规则

对于每个小批量数据 $B = \{(x_j, y_j)\}_{j=1}^m$，其中 $m$ 是批量大小：

$$ \mathbf{w}_{new} = \mathbf{w}_{old} - \eta \frac{1}{m} \sum_{j=1}^m \nabla L(x_j, y_j; \mathbf{w}_{old}) $$

其中 $\eta$ 是学习率，$m$ 是批量大小。

## 优势

- **兼顾效率与稳定性**：
    - **计算效率**：相比批量梯度下降，每次更新的计算量大大减少，尤其是在大规模数据集上。
    - **更新稳定性**：相比纯粹的 SGD，使用小批量数据计算的梯度估计噪声更小，更新方向更稳定，损失函数波动更小。
- **利用并行计算**：现代深度学习框架和硬件（如 GPU）能够高效地并行处理小批量数据，进一步加速训练。
- **跳出局部最优**：小批量 SGD 仍然保留了一定的随机性，这有助于模型跳出局部最优解。

## 批量大小 (Batch Size) 的选择

批量大小是一个重要的超参数：
- **批量大小过小**：梯度估计噪声大，训练不稳定，但可能有助于跳出局部最优。
- **批量大小过大**：梯度估计更准确，训练更稳定，但可能收敛到尖锐的局部最优，且计算效率可能下降。

在实践中，批量大小通常选择 32、64、128 等 2 的幂次方。

小批量 SGD 结合了批量梯度下降的稳定性和 SGD 的效率，是深度学习模型训练的首选优化策略。
