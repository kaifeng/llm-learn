# Adagrad

Adagrad（Adaptive Gradient Algorithm）是一种自适应学习率优化算法，它根据参数的历史梯度平方和来调整每个参数的学习率。

## 核心思想

Adagrad 的核心思想是：对于不频繁更新的参数，使用较大的学习率；对于频繁更新的参数，使用较小的学习率。这使得模型能够更好地处理稀疏数据，因为稀疏特征的梯度通常不频繁。

## 更新规则

Adagrad 的更新规则如下：

1.  **累积梯度平方**：
    $$ s_t = s_{t-1} + (\nabla L(\mathbf{w}_t))^2 $$
    其中 $s_t$ 是从训练开始到当前时间步所有梯度平方的累积和。

2.  **更新参数**：
    $$ \mathbf{w}_{t+1} = \mathbf{w}_t - \frac{\eta}{\sqrt{s_t + \epsilon}} \nabla L(\mathbf{w}_t) $$
    其中 $\eta$ 是全局学习率，$\\epsilon$ 是一个很小的常数，用于防止除以零。

## 优势

- **自适应学习率**：无需手动调整每个参数的学习率，对稀疏数据表现良好。
- **适用于稀疏数据**：对于不频繁出现的特征，其梯度累积较小，因此学习率较大，有助于模型更快地学习这些特征。

## 劣势

- **学习率单调递减**：由于 $s_t$ 是单调递增的，学习率会不断减小，可能导致训练后期学习率过小，模型过早停止学习。
