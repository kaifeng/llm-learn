# Adam (Adaptive Moment Estimation)

Adam（Adaptive Moment Estimation）是目前最流行、最常用的优化算法之一。它结合了动量法（Momentum）和 RMSprop 的优点，能够为网络中的每一个参数计算自适应的学习率。

## 核心思想

Adam 算法的核心思想是：
1.  **利用梯度的指数加权移动平均**：类似于动量法，它计算梯度的**一阶矩估计**（即梯度的均值）。
2.  **利用梯度平方的指数加权移动平均**：类似于 RMSprop，它计算梯度的**二阶矩估计**（即梯度的未中心化方差）。

然后，Adam 利用这两个矩估计来调整每个参数的学习率。

## 更新规则

Adam 的更新规则相对复杂，但其核心步骤如下：

1.  **初始化**：$m_0 = 0, v_0 = 0$
2.  **计算一阶矩估计**：
    $$ m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L(\mathbf{w}_t) $$
3.  **计算二阶矩估计**：
    $$ v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L(\mathbf{w}_t))^2 $$
4.  **偏差修正**：由于 $m_t$ 和 $v_t$ 在训练初期会偏向于 0，需要进行偏差修正：
    $$ \hat{m}_t = \frac{m_t}{1 - \beta_1^t} $$
    $$ \hat{v}_t = \frac{v_t}{1 - \beta_2^t} $$
5.  **更新参数**：
    $$ \mathbf{w}_{t+1} = \mathbf{w}_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $$
    其中 $\eta$ 是学习率，$\beta_1$ 和 $\beta_2$ 是衰减率（通常设置为 0.9 和 0.999），$\epsilon$ 是一个很小的常数（如 $10^{-8}$），用于防止除以零。

## 优势

- **自适应学习率**：Adam 能够为每个参数提供独立的、自适应的学习率，这使得它在处理稀疏梯度和非平稳目标时表现出色。
- **快速收敛**：通常能够比其他优化算法更快地收敛。
- **对超参数不敏感**：对学习率和衰减率的选择不那么敏感，通常使用默认参数就能取得不错的效果。

Adam 算法因其出色的性能和鲁棒性，已成为深度学习模型训练中最常用的优化器之一。
