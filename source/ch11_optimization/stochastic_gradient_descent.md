# 随机梯度下降 (Stochastic Gradient Descent, SGD)

随机梯度下降（Stochastic Gradient Descent, SGD）是梯度下降的一种变体，旨在解决批量梯度下降（Batch Gradient Descent）在处理大规模数据集时效率低下的问题。

## 核心思想

与批量梯度下降每次更新都使用整个训练集的数据不同，SGD 每次更新仅使用训练集中的**一个样本**来计算梯度。

## 更新规则

对于每个训练样本 $(x_i, y_i)$：

$$ \mathbf{w}_{new} = \mathbf{w}_{old} - \eta \nabla L(x_i, y_i; \mathbf{w}_{old}) $$

其中 $\eta$ 是学习率，$\nabla L(x_i, y_i; \mathbf{w}_{old})$ 是损失函数在单个样本上的梯度。

## 优势

- **速度快**：由于每次只处理一个样本，计算速度非常快，尤其适用于大规模数据集。
- **跳出局部最优**：由于每次更新的梯度具有随机性，SGD 可能会在训练过程中产生更多的波动，这有助于模型跳出局部最优解，找到更好的全局最优解。

## 劣势

- **更新方向不稳定**：由于每次只使用一个样本，梯度估计的噪声较大，导致参数更新的方向不稳定，损失函数会剧烈波动。
- **收敛震荡**：在接近最优解时，SGD 可能会持续震荡，难以精确收敛。

尽管 SGD 的更新方向不稳定，但其计算效率和跳出局部最优的能力使其成为深度学习中非常重要的优化算法。
