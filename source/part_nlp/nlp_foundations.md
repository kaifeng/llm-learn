# 传统NLP基础

自然语言处理（NLP）是人工智能领域中一个至关重要的分支，它使计算机能够理解、解释和生成人类语言。

## 文本预处理

这是所有自然语言处理（NLP）任务的第一步，旨在将原始文本数据转换为模型可以理解和处理的格式。它通常包括以下几个阶段：

1. 分词 (Tokenization)：将文本分解成更小的单元，称为“词元”（tokens）。这些词元可以是单词、子词、字符或短语。
2. 词形还原与词干提取 (Lemmatization & Stemming)：旨在将单词还原到其基本形式。
3. 停用词移除 (Stop Word Removal)：移除文本中常见但对语义贡献不大的词语（如“的”、“是”、“一个”等）。
4. 文本清洗：包括移除标点符号、数字、特殊字符、HTML 标签，以及将文本转换为小写等操作。

## 文本表示

在进行自然语言处理（NLP）任务时，首要步骤就是将文本转换成机器可以理解的数值形式。这个过程称为文本表示（Text Representation）。

### 传统表示方法

在深度学习和词嵌入技术普及之前，文本通常通过统计学方法被转换为数值向量，以便机器学习模型能够处理。这些方法虽然简单，但在许多任务中仍然有效，并且是理解现代文本表示方法的基础。

1. 词袋模型 (Bag-of-Words, BoW)：词袋模型是最简单也是最常用的文本表示方法之一。它将文本（如句子或文档）视为一个无序的词语集合，忽略词语的语法和语序，只关注词语是否出现以及出现的频率。
2. TF-IDF (Term Frequency-Inverse Document Frequency)：TF-IDF 是一种统计方法，用于评估一个词语对于一个文档集或一个语料库中的其中一份文档的重要程度。它结合了词频（TF）和逆文档频率（IDF）两个概念。
3. 独热编码 (One-Hot Encoding)：最简单的文本表示方法之一是独热编码。首先，统计语料库中所有出现过的独立词语，构建一个词典。对于词典中的每一个词，创建一个长度等于词典大小的向量。这个向量在代表该词的位置上为 1，在所有其他位置上都为 0。独热编码的缺点：

  -   **高维稀疏**：如果词典中有成千上万个词，那么每个词的向量维度就会非常高，并且其中绝大多数元素都是 0，这在计算上是低效的。
  -   **无法表达语义相似性**：这是最致命的缺点。在独热编码中，任意两个不同词的向量都是正交的（它们的点积为 0）。这意味着，模型无法从向量本身得知 `猫` 和 `狗` 的关系比 `猫` 和 `鱼` 更近。词与词之间的语义关系完全丢失了。

### 现代表示方法：词嵌入 (Word Embeddings)

为了克服独热编码的缺点，研究者们提出了**分布式表示**，其中最著名的就是**词嵌入**。词嵌入不再使用稀疏的高维向量，而是将每个词表示为一个**低维、稠密的浮点数向量**（例如，一个长度为 100 或 300 的向量）。其核心思想基于**分布式假设（Distributional Hypothesis）**：上下文相似的词，其含义也相似。例如，“猫”和“狗”经常出现在相似的语境中，而“鱼”的上下文则可能很不一样。

词嵌入模型通过在大量文本上进行训练，将这种上下文关系编码到向量中。最终得到的词向量具有以下优良特性：

-   **语义相似性**：意义相近的词在向量空间中的距离也相近。
-   **语义关系**：向量之间甚至可以进行有意义的运算。最经典的例子是：`vector('King') - vector('Man') + vector('Woman') ≈ vector('Queen')`。

主流的词嵌入模型有：

-   **Word2Vec (Google, 2013)**：是词嵌入领域最经典的模型之一。它本质上是一个浅层神经网络，通过预测词与其上下文的关系来学习词向量。它包含两种主要的训练架构：
    -   **CBOW (Continuous Bag-of-Words)**：根据上下文词来预测中心词。
    -   **Skip-gram**：根据中心词来预测其上下文的词。通常在大型语料库上表现更好。

-   **GloVe (Stanford, 2014)**：全称为“Global Vectors for Word Representation”。它结合了两种方法的优点：既利用了全局的统计信息（词的共现矩阵），又利用了局部的上下文信息。GloVe 通过对一个巨大的“词-词共现矩阵”进行分解，来学习词向量。

词嵌入为神经网络模型提供了一种捕捉词语语义的有效方式，是后续理解句子、段落乃至文档语义的基础。

## 基本 NLP 任务与概念

自然语言处理（NLP）领域涵盖了多种任务和概念，它们是理解和构建更复杂 NLP 应用的基础。

1. N-gram 模型：一种在文本和语音处理中广泛使用的统计语言模型。它基于马尔可夫假设，即一个词的出现概率只依赖于它前面有限个词（N-1个词）。
2. 词性标注 (Part-of-Speech Tagging, POS Tagging)：识别句子中每个词语的语法类别（如名词、动词、形容词、副词等）的过程。
3. 命名实体识别 (Named Entity Recognition, NER)：是 NLP 中的一个子任务，旨在识别文本中具有特定意义或指代特定实体的词语或短语，并将其分类到预定义的类别中，如人名、地名、组织机构名、日期、时间等。
4. 统计语言模型 (Statistical Language Models)：在神经网络语言模型出现之前，统计语言模型是主流。它们通过计算词语序列的概率来预测下一个词，常用于语音识别、机器翻译等领域。
