# 注意力机制

注意力机制（Attention Mechanism）是深度学习领域最重要的思想之一，它不仅革新了自然语言处理（NLP），也为更强大的 Transformer 模型的诞生铺平了道路。其核心思想是：当模型处理信息时，并不会一次性处理所有信息，而是会将注意力集中在当前任务最相关的部分。

## 编码器-解码器架构中的信息瓶颈

在循环神经网络（RNN/LSTM）的时代，处理序列到序列（Sequence-to-Sequence, Seq2Seq）任务（如机器翻译）的标准范式是 **编码器-解码器（Encoder-Decoder）** 架构。

-   **编码器（Encoder）**：一个 RNN（或 LSTM），负责读取并理解整个输入序列（例如，一个英文句子）。它将整个序列的信息压缩成一个固定长度的向量，称为**上下文向量（Context Vector）**。这个向量通常是编码器最后一个时间步的隐藏状态。
-   **解码器（Decoder）**：另一个 RNN，它接收编码器生成的上下文向量，并逐词生成输出序列（例如，对应的法文句子）。

这种架构存在一个明显的信息瓶颈：无论输入序列有多长、多复杂，编码器都必须将其所有信息压缩到一个固定长度的向量中。对于长句子来说，这几乎是不可能完成的任务，模型会不可避免地丢失重要信息，尤其是句子开头的细节。

## 注意力机制的引入

为了解决这个瓶颈问题，注意力机制被引入。它允许解码器在生成每个词时，能够“回顾”并“关注”输入序列的所有部分，并根据当前要生成的词，动态地决定哪些输入词更重要。

**核心思想**：不再依赖于一个单一的、固定的上下文向量，而是在解码的每一步，都根据当前解码状态，重新计算一个**针对当前步骤的、动态的上下文向量**。这个动态的上下文向量是输入序列所有部分信息的加权平均，而“权重”就代表了模型对输入序列不同部分的“注意力”大小。

## 注意力机制的计算过程：Query, Key, Value

注意力机制的计算过程可以被优雅地抽象为对 **查询（Query）**、**键（Key）** 和 **值（Value）** 的操作。

-   **Query (Q)**：代表当前的需求。在 Seq2Seq 模型中，它通常是解码器在当前时间步的隐藏状态，表示“我正在寻找什么样的信息来生成下一个词？”
-   **Key (K)**：与输入序列的每个元素相关联的“标签”。它用于和 Query 进行匹配，以衡量输入元素的重要性。在基础的注意力模型中，它就是输入序列每个位置的编码器隐藏状态。
-   **Value (V)**：与 Key 关联的“内容”。它是输入序列每个元素的实际信息表示。在基础模型中，Value 通常也等于 Key（即编码器的隐藏状态）。

计算过程分为三步：

1.  **计算注意力分数（Attention Scores）**：
    对于当前的 Query，用它和**每一个** Key 进行相似度计算，得到一个分数。最常用的计算方法是点积（Dot-Product）。
    $$ \text{score}(Q, K_i) = Q \cdot K_i $$
    这个分数衡量了当前的查询 $Q$ 与输入序列第 $i$ 个位置的 Key $K_i$ 的相关性有多高。

2.  **分数归一化（Softmax）**：
    将上一步得到的所有分数通过一个 Softmax 函数进行归一化，得到**注意力权重（Attention Weights）** $\alpha_i$。Softmax 确保所有权重都是 0 到 1 之间的正数，并且它们的总和为 1，形成一个概率分布。
    $$ \alpha_i = \text{softmax}(\text{scores})_i = \frac{\exp(\text{score}(Q, K_i))}{\sum_j \exp(\text{score}(Q, K_j))} $$

3.  **计算上下文向量（Context Vector）**：
    将注意力权重 $\alpha_i$ 与对应的 Value $V_i$ 相乘，然后求和，得到最终的上下文向量。
    $$ \text{Context} = \sum_i \alpha_i V_i $$
    这个上下文向量是一个加权和，其中注意力权重高的 Value 贡献更大。它汇集了当前解码步骤最需要的来自输入序列的信息。

最后，解码器将这个动态生成的上下文向量与它自身的隐藏状态结合起来，用于预测下一个词。

## 自注意力（Self-Attention）

自注意力是注意力机制的一种特殊情况，其中查询（Query）、键（Key）和值（Value）均来自同一个地方——即上一层的输出。简单来说，序列中的每个词都会生成自己的 Q、K、V 向量，然后计算自己与其他所有词的注意力权重。

## 多头自注意力（Multi-Head Self-Attention）

与其只进行一次注意力计算，不如将 Q、K、V 向量在维度上拆分成多个“头（head）”，并让每个头并行地进行计算。这允许模型在不同的表示子空间中学习到不同方面的信息（例如，一个头可能关注句法关系，另一个头可能关注语义关联）。最后，所有头的输出会被拼接起来，并通过一次线性变换得到最终结果。

## 位置编码（Positional Encoding）

自注意力机制本身无法感知序列中词语的顺序，为了解决这个问题，模型必须引入一种能表达单词位置信息的方式。位置编码的核心思想是：为输入序列中的每个位置创建一个唯一的、固定维度的向量（位置向量），然后将这个位置向量与对应位置的词嵌入向量相加，得到一个既包含语义又包含位置信息的新向量，作为模型真正的输入。

### 正弦/余弦编码公式

原始论文提出了一种使用不同频率的正弦和余弦函数来生成位置向量的方案：
$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$
$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$
- `pos` 是单词在序列中的位置。
- `i` 是位置向量中的维度索引。
- `d_model` 是嵌入维度。

这个公式最巧妙的一点在于，它使得模型能够轻易地学习到**相对位置信息**。因为对于任何固定的偏移量 `k`，`PE(pos+k)` 都可以表示为 `PE(pos)` 的一个线性变换。这意味着模型可以学会识别“后面第 k 个词”这样的相对关系，而无论当前词的位置 `pos` 在哪里。

### 附录：位置编码的线性变换特性

这个结论之所以成立，是因为位置编码使用了**三角函数的和角公式**。

#### 1. 回顾公式

首先，我们有两个公式，分别用于计算位置向量中的偶数维度 (`2i`) 和奇数维度 (`2i+1`)：

-   $PE_{(pos, 2i)} = \sin(pos \cdot \omega_i)$
-   $PE_{(pos, 2i+1)} = \cos(pos \cdot \omega_i)$

其中 $\omega_i = \frac{1}{10000^{2i/d_{\text{model}}}} $。

#### 2. 证明线性关系

使用三角函数的**和角公式**：
-   $\sin(A+B) = \sin(A)\cos(B) + \cos(A)\sin(B)$
-   $\cos(A+B) = \cos(A)\cos(B) - \sin(A)\sin(B)$

代入 $A = pos \cdot \omega_i$ 和 $B = k \cdot \omega_i$ 后，可以得到：

$$\begin{bmatrix} PE_{(pos+k, 2i)} \ PE_{(pos+k, 2i+1)} \end{bmatrix} = \begin{bmatrix} \cos(k \cdot \omega_i) & \sin(k \cdot \omega_i) \\ -\sin(k \cdot \omega_i) & \cos(k \cdot \omega_i) \end{bmatrix} \begin{bmatrix} PE_{(pos, 2i)} \ PE_{(pos, 2i+1)} \end{bmatrix}$$

这个 2x2 的矩阵是一个**旋转矩阵**，它**只依赖于偏移量 `k`**，而与原始位置 `pos` 无关。这意味着 $PE_{pos+k} = M_k \cdot PE_{pos}$，这是一个线性变换。

#### 3. 这为什么重要？

这个特性意味着，对于模型来说，从位置 `pos` 移动到 `pos+k` 的“操作”是**一致的**，无论 `pos` 是 5 还是 50。模型可以非常容易地学会**只关注相对位置 `k`**，而不是绝对位置 `pos`。这使得模型具有了**位置不变性**，能够将学到的关于相对位置的知识应用到句子的任何地方。
