# 子词嵌入 (Subword Embeddings)

在自然语言处理中，词嵌入（Word Embeddings）将每个词映射到一个向量。然而，这种方法存在一些局限性：
- **词汇表外（Out-Of-Vocabulary, OOV）问题**：对于训练集中未出现过的词，模型无法为其生成词嵌入。
- **处理形态学**：对于具有丰富形态变化的语言（如德语、土耳其语），词嵌入无法捕捉词根和词缀之间的关系。

子词嵌入（Subword Embeddings）旨在解决这些问题。它的核心思想是将词分解为更小的、有意义的子单元（子词），然后为这些子词生成嵌入。

## 核心思想

- **词的分解**：将词分解为子词，例如，“unhappily”可以分解为“un”、“happi”和“ly”。
- **共享嵌入**：不同的词可以共享相同的子词，从而减少词汇表的大小，并处理 OOV 词。

## 常见的子词嵌入模型

- **Byte Pair Encoding (BPE)**：通过迭代地合并语料库中最频繁的字节对来构建子词词汇表。
- **WordPiece**：Google 在 BERT 中使用的子词分词算法，类似于 BPE，但选择合并的子词对是基于概率的。
- **SentencePiece**：一个语言无关的子词分词器，支持 BPE 和 WordPiece 算法。

## 优势

- **处理 OOV 词**：对于未见过的词，可以将其分解为已知的子词，从而生成其嵌入。
- **处理形态学**：能够捕捉词根和词缀之间的关系，对于形态丰富的语言尤其有效。
- **减少词汇表大小**：子词词汇表通常比词汇表小，从而减少了模型的参数量。

子词嵌入是现代大型预训练语言模型（如 BERT, GPT）中不可或缺的一部分。
