# 词嵌入的预训练

词嵌入的预训练是指在大规模无标签文本语料库上，通过无监督学习的方式，学习词语的向量表示（即词嵌入）。这些预训练好的词嵌入捕捉了词语的语义和语法信息，可以作为特征输入到各种下游的自然语言处理（NLP）任务中。

## 核心思想

预训练词嵌入的核心思想是**分布式假设（Distributional Hypothesis）**：上下文相似的词，其含义也相似。通过分析词语在大量文本中出现的上下文，模型可以学习到词语之间的语义关系。

## 常见的预训练模型

- **Word2Vec**：通过预测词与其上下文的关系来学习词向量，包括 CBOW 和 Skip-gram 两种架构。
- **GloVe**：结合了全局的共现统计信息和局部的上下文信息来学习词向量。
- **FastText**：在 Word2Vec 的基础上，引入了子词信息，能够处理词汇表外（OOV）问题和形态学丰富的语言。

## 预训练过程

1.  **数据准备**：收集大规模的无标签文本语料库。
2.  **模型训练**：使用 Word2Vec、GloVe 或 FastText 等模型在语料库上进行训练，学习每个词的向量表示。
3.  **词嵌入矩阵**：训练完成后，得到一个词嵌入矩阵，其中每一行代表一个词的向量。

## 优势

- **捕捉语义和语法信息**：预训练词嵌入能够捕捉词语之间的语义相似性、类比关系等。
- **解决数据稀疏问题**：对于下游任务，即使数据量较小，也可以利用预训练词嵌入来提高模型性能。
- **迁移学习**：预训练词嵌入可以作为特征输入到各种 NLP 模型中，实现知识迁移。

词嵌入的预训练是现代 NLP 的基石，为后续的预训练语言模型（如 BERT, GPT）奠定了基础。
