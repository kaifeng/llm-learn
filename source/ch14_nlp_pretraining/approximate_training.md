# 近似训练

在训练大型语言模型时，尤其是涉及到预测下一个词或进行多分类任务时，如果词汇表非常庞大（例如，几十万甚至数百万个词），那么在输出层计算每个词的概率并进行归一化（Softmax）会变得非常耗时。

近似训练（Approximate Training）技术旨在解决这个问题，通过避免在每个训练步骤中计算整个词汇表的 Softmax，从而显著加速训练过程。

## 核心思想

近似训练的核心思想是：在每个训练步骤中，我们不需要精确地计算所有词的概率，只需要计算正确词的概率，并从负样本中采样一些词来计算它们的概率。

## 常见的近似训练方法

- **层次 Softmax (Hierarchical Softmax)**：将词汇表组织成一个二叉树结构。在预测时，模型只需要遍历从根节点到目标词的路径，而不是计算所有词的概率。
- **负采样 (Negative Sampling)**：对于每个正样本（即目标词），从词汇表中随机采样一些负样本（即非目标词）。模型的目标是区分正样本和负样本，即最大化正样本的概率，最小化负样本的概率。
- **噪声对比估计 (Noise Contrastive Estimation, NCE)**：类似于负采样，但它将语言建模问题转化为一个二分类问题，即区分真实数据和噪声数据。

## 优势

- **加速训练**：显著减少了在大型词汇表上计算 Softmax 的计算量。
- **处理大规模词汇表**：使得训练具有巨大词汇表的语言模型成为可能。

近似训练是 Word2Vec 等词嵌入模型以及早期大型语言模型训练的关键技术。
