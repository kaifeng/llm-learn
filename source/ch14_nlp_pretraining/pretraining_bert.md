# 预训练 BERT

BERT（Bidirectional Encoder Representations from Transformers）的预训练是一个计算密集型但至关重要的过程，它使模型能够从大规模无标签文本数据中学习到通用的语言表示。

## 预训练任务

BERT 的预训练主要依赖于两个无监督任务：

1.  **掩码语言模型 (Masked Language Model, MLM)**：
    -   **目标**：预测输入序列中被随机掩盖的词元。
    -   **机制**：在输入序列中随机选择 15% 的词元进行掩盖，然后模型需要根据上下文预测这些被掩盖的词元。这种双向预测迫使模型学习到深度的上下文表示。

2.  **下一句预测 (Next Sentence Prediction, NSP)**：
    -   **目标**：预测两个句子是否在原始文本中是连续的。
    -   **机制**：模型接收一对句子 A 和 B，并预测 B 是否是 A 的下一句。这有助于模型理解句子之间的关系，这对于问答和自然语言推断等任务至关重要。

## 预训练过程

1.  **数据准备**：使用大规模的无标签文本语料库（如 BooksCorpus 和英文维基百科）。
2.  **输入表示**：将文本转换为 BERT 期望的输入格式，包括词元嵌入、段嵌入和位置嵌入。
3.  **模型架构**：使用 Transformer 编码器作为骨干网络。
4.  **损失函数**：结合 MLM 损失和 NSP 损失进行优化。
5.  **优化器**：通常使用 Adam 优化器。

## 预训练的意义

- **学习通用语言知识**：通过在大规模语料库上进行预训练，BERT 学习到了丰富的词汇、语法、语义和世界知识。
- **迁移学习**：预训练好的 BERT 模型可以作为特征提取器或通过微调应用于各种下游 NLP 任务，大大减少了对任务特定标注数据的需求。
- **双向上下文理解**：MLM 任务使得 BERT 能够学习到真正的双向上下文表示，这在之前的语言模型中是难以实现的。

预训练 BERT 是现代 NLP 发展的一个里程碑，它为后续的预训练语言模型和迁移学习范式奠定了基础。
