# 词嵌入 (Word2Vec)

Word2Vec 是词嵌入领域最经典的模型之一，由 Google 在 2013 年提出。它本质上是一个浅层神经网络，通过预测词与其上下文的关系来学习词向量。

## 核心思想：分布式假设

Word2Vec 的核心思想基于**分布式假设（Distributional Hypothesis）**：上下文相似的词，其含义也相似。例如，“猫”和“狗”经常出现在相似的语境中，而“鱼”的上下文则可能很不一样。

Word2Vec 模型通过在大量文本上进行训练，将这种上下文关系编码到向量中。最终得到的词向量具有以下优良特性：

-   **语义相似性**：意义相近的词在向量空间中的距离也相近。
-   **语义关系**：向量之间甚至可以进行有意义的运算。最经典的例子是：`vector('King') - vector('Man') + vector('Woman') ≈ vector('Queen')`。

## 两种主要的训练架构

Word2Vec 包含两种主要的训练架构，它们都通过预测词与其上下文的关系来学习词向量：

1.  **CBOW (Continuous Bag-of-Words)**：
    -   **思想**：根据上下文词来预测中心词。
    -   **工作原理**：给定一个词的上下文（即它周围的词），CBOW 模型的目标是预测这个上下文中间的词是什么。它将上下文词的词向量进行平均或求和，然后通过一个神经网络预测中心词。
    -   **特点**：在小型语料库上表现较好，训练速度相对较快。

2.  **Skip-gram**：
    -   **思想**：根据中心词来预测其上下文的词。
    -   **工作原理**：给定一个中心词，Skip-gram 模型的目标是预测它周围的上下文词。它将中心词的词向量作为输入，通过一个神经网络预测其上下文中的每个词。
    -   **特点**：通常在大型语料库上表现更好，尤其擅长处理不常见的词。

## 词嵌入的意义

词嵌入为神经网络模型提供了一种捕捉词语语义的有效方式，是后续理解句子、段落乃至文档语义的基础。它将离散的词语映射到连续的向量空间，使得词语之间的语义关系可以通过向量运算来体现。
