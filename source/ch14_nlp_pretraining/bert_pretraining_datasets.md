# 用于预训练 BERT 的数据集

BERT（Bidirectional Encoder Representations from Transformers）的强大性能很大程度上归功于其在大规模文本语料库上的预训练。这些数据集为 BERT 提供了丰富的语言知识，使其能够学习到通用的、高质量的语言表示。

## 核心要求

用于预训练 BERT 的数据集需要满足以下核心要求：
- **规模巨大**：为了捕捉语言的复杂模式，数据集需要包含数十亿甚至数万亿的词元。
- **多样性**：数据集应涵盖各种主题、风格和领域，以确保模型学习到通用的语言知识。
- **高质量**：数据集应尽可能地干净，减少噪声和错误。

## 常见数据集

- **BooksCorpus**：一个包含 11,038 本未出版书籍的数据集，总计约 8 亿词。
- **英文维基百科 (English Wikipedia)**：一个包含 25 亿词的数据集，涵盖了广泛的主题。
- **Common Crawl**：一个巨大的网络爬取数据集，包含数万亿的词元，但通常需要进行大量的清洗和过滤。
- **C4 (Colossal Clean Crawled Corpus)**：Google 在训练 T5 模型时使用的数据集，它是 Common Crawl 的一个经过严格清洗和过滤的版本。

## 数据预处理

在预训练 BERT 之前，通常需要对原始文本数据进行一系列预处理，包括：
- **分词 (Tokenization)**：将文本分解为词元，通常使用 WordPiece 分词器。
- **句子分割**：将文本分割成句子。
- **格式化**：将文本格式化为 BERT 期望的输入格式，包括添加特殊标记（如 `[CLS]`、`[SEP]`、`[MASK]`）。

这些大规模、高质量的数据集和精细的预处理过程，共同促成了 BERT 在自然语言理解任务上的巨大成功。
