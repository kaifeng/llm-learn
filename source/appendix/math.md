# 数学基础

理解机器学习和深度学习模型的工作原理，离不开扎实的数学基础。以下是几个最重要的数学领域及其在机器学习中的应用。

## 1. 线性代数 (Linear Algebra)

线性代数是机器学习的语言，几乎所有的数据和模型操作都可以用线性代数的概念来表达。

-   **核心概念**：
    -   **标量 (Scalar)**：单个数值。
    -   **向量 (Vector)**：一维数组，表示空间中的一个点或方向。在机器学习中，一个样本的特征通常表示为一个向量。
    -   **矩阵 (Matrix)**：二维数组，由行和列组成。数据集通常表示为一个矩阵，神经网络的权重也表示为矩阵。
    -   **张量 (Tensor)**：多维数组，是向量和矩阵的推广。深度学习中，数据和模型参数通常以张量的形式存在。
-   **基本运算**：
    -   **加法与减法**：向量和矩阵的对应元素相加减。
    -   **乘法**：
        -   **标量乘法**：标量与向量/矩阵的每个元素相乘。
        -   **点积 (Dot Product)**：两个向量的对应元素相乘再求和，结果是标量。用于计算相似度、投影等。
        -   **矩阵乘法**：矩阵之间按照特定规则相乘，结果是矩阵。神经网络中层与层之间的计算核心。
    -   **转置 (Transpose)**：矩阵的行和列互换。
    -   **逆 (Inverse)**：矩阵的逆用于求解线性方程组。
-   **应用**：数据表示、特征工程、主成分分析（PCA）、神经网络的层运算、损失函数计算。

### The Art of Linear Algebra

这份名为《线性代数的艺术》的笔记，是日本学者 Kenji Hiranabe 基于 MIT 教授 Gilbert Strang 的著作《每个人的线性代数》制作的。它将原著浓缩成仅 12 页的图解笔记，内容涵盖了从理解矩阵的四个视角、基本运算（如向量与矩阵乘法）、实用技巧到矩阵的五种分解方式。这份笔记因其精炼和可视化，获得了 Gilbert Strang 本人的高度肯定，并被收录进原书介绍页面的推荐链接中。

项目地址：https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra

参考链接：https://news.mit.edu/2023/gilbert-strang-made-linear-algebra-fun-0531


## 2. 微积分 (Calculus)

微积分是优化机器学习模型（尤其是深度学习模型）的关键工具，它告诉我们如何调整模型参数以最小化误差。

-   **核心概念**：
    -   **导数 (Derivative)**：衡量函数在某一点的变化率。在机器学习中，用于计算损失函数对单个参数的影响。
    -   **偏导数 (Partial Derivative)**：衡量多变量函数对其中一个变量的变化率，而保持其他变量不变。
    -   **梯度 (Gradient)**：由一个函数对所有自变量的偏导数组成的向量。梯度指向函数值增长最快的方向。在机器学习中，我们通常沿着梯度的反方向调整参数以最小化损失函数。
    -   **链式法则 (Chain Rule)**：用于计算复合函数的导数。在神经网络中，这是**反向传播（Backpropagation）**算法的核心，用于高效计算每一层参数的梯度。
-   **应用**：优化算法（如梯度下降）、反向传播、损失函数最小化。

## 3. 概率论与统计 (Probability and Statistics)

概率论和统计学提供了处理不确定性、理解数据分布和评估模型性能的工具。

-   **核心概念**：
    -   **概率 (Probability)**：事件发生的可能性。
    -   **随机变量 (Random Variable)**：其取值是随机事件结果的变量。
    -   **概率分布 (Probability Distribution)**：描述随机变量所有可能取值及其对应概率的函数。常见的有正态分布（高斯分布）、伯努利分布等。
    -   **期望 (Expectation)**：随机变量的平均值。
    -   **方差 (Variance)**：衡量随机变量与其期望值之间的离散程度。
    -   **协方差 (Covariance)**：衡量两个随机变量的联合变化程度。
-   **重要定理**：
    -   **大数定律 (Law of Large Numbers)**：随着样本数量的增加，样本均值会趋近于总体均值。
    -   **中心极限定理 (Central Limit Theorem)**：在适当条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。
    -   **贝叶斯定理 (Bayes' Theorem)**：用于更新事件的概率，是贝叶斯机器学习方法的基础。
-   **应用**：数据分析、模型评估（如准确率、召回率）、损失函数设计（如交叉熵）、贝叶斯推断、生成模型。

## 4. 优化理论 (Optimization Theory)

优化理论提供了寻找函数最小值（或最大值）的方法，这在机器学习中对应于寻找使模型损失最小化的参数。

-   **核心概念**：
    -   **目标函数 (Objective Function)**：在机器学习中通常指**损失函数（Loss Function）**，我们希望最小化它。
    -   **梯度下降 (Gradient Descent)**：最基本的优化算法，通过沿着损失函数梯度的反方向迭代调整参数来寻找最小值。
    -   **学习率 (Learning Rate)**：控制梯度下降每一步的步长。
    -   **收敛 (Convergence)**：优化算法达到或接近最优解的状态。
-   **应用**：训练神经网络、支持向量机等各种机器学习模型，寻找模型参数的最优解。

## 5. 交叉熵 (Cross-Entropy)

交叉熵源于信息论，在机器学习领域，它已成为最常用的分类问题损失函数。

-   **核心思想**：交叉熵衡量的是两个概率分布之间的“差异”或“距离”。在分类任务中，这两个分布分别是：
    1.  **真实分布 (p)**：即数据的真实标签，通常表示为一个 **独热编码（One-Hot Encoding）** 的向量。例如，在一个三分类（猫、狗、鸟）问题中，如果真实标签是“猫”，其分布就是 `[1, 0, 0]`。
    2.  **预测分布 (q)**：即模型对每个类别的预测概率，通常是 **Softmax** 层的输出。例如，模型可能预测为 `[0.7, 0.2, 0.1]`，表示它认为有 70% 的概率是猫，20% 是狗，10% 是鸟。

    交叉熵损失函数的目标，就是让模型的预测分布 `q` 尽可能地接近真实的标签分布 `p`。

-   **公式**：
    对于单个样本，交叉熵损失的计算公式为：
    $$ H(p, q) = - \sum_{i} p(i) \log(q(i)) $$
    其中，$p(i)$ 是真实分布中类别 $i$ 的概率（在 one-hot 向量中，它只在真实类别处为 1，其余为 0），$q(i)$ 是模型预测类别为 $i$ 的概率。

-   **简化与直觉**：
    由于真实分布 `p` 是 one-hot 的，上述求和公式可以大大简化。因为只有真实类别 $c$ 对应的 $p(c)$ 为 1，其余都为 0，所以公式等价于：
    $$ \text{Loss} = -\log(q(c)) $$
    其中 $q(c)$ 是模型对**正确类别**的预测概率。

    这个简化的公式非常直观：
    -   如果模型对正确类别的预测概率 $q(c)$ 很高（例如 0.99），那么 $-\log(0.99)$ 是一个很小的数，损失就很小。
    -   如果模型对正确类别的预测概率 $q(c)$ 很低（例如 0.01），那么 $-\log(0.01)$ 是一个很大的数，损失就很大。

    因此，**最小化交叉熵损失的过程，就是在驱动模型去最大化它对正确答案的预测概率**。