.. _glossary:

术语
====

.. glossary::

    机器学习
        人工智能的一个分支，它通过算法分析数据，从中学习模式，并利用这些模式对新数据进行预测或决策。

    DNN
        深度神经网络(Deep Neural Network)，包含多个隐藏层的神经网络。

    NLP
        Natural Language Processing，自然语言处理。

    指令微调
        Instruction Tuning

    线性回归
        Linear Regression，用于解决回归问题，即预测一个连续的数值输出。线性回归的目标是找到一条直线（或超平面），使其尽可能地拟合训练数据。

    逻辑回归
        Logistic Regression，用于解决分类问题。它通过一个 Sigmoid (Logistic) 函数将线性模型的输出映射到 (0, 1) 区间，表示属于某个类别的概率。

    MLP
        Multi-Layer Perceptron，多层感知机。

    生成式AI（Generative AI）
        机器产生复杂有结构的物件，尽乎无法穷举。如文章、图像、语音等。不是生成式AI：分类（如垃圾邮件分类、猫狗分类）。

    多模态（multi modality）
        模态指信息的类型或格式，如文本、图像、音频、视频等。多模态指 AI 模型能够同时理解、处理和关联多种不同模态信息的能力。GPT-4V 就是一个典型的多模态模型。

    AR
        自回归生成(Autoregressive Generation)。这是一种生成序列数据（如文本）的方式，即逐个生成序列中的元素（token），并且每生成一个新的元素，都要依赖于所有在它之前已经生成的元素。这就像我们写句子一样，下一个词总是基于前面已经写好的内容。GPT 系列模型就是典型的自回归模型。这种方式生成的文本质量高、连贯性好，但缺点是速度较慢，因为必须串行生成。

    NAR
        非自回归生成(Non-Autoregressive Generation)。与自回归相反，这种方式试图一次性或并行地生成整个序列的所有元素，而不是逐个生成。例如，模型可能先预测目标句子的长度，然后同时填充所有位置的词语。这种方式生成速度极快，但通常会牺牲一定的文本质量和连贯性，因为它在预测某个位置的词时，并不知道其他位置的词是什么。

    Transformer
        2017年在Attention Is All You Need论文中作为机器翻译的序列到序列模型提出的一种著名的深度学习模型。
        基于Transformer的预训练模型在各种自然语言处理任务上实现了最优性能，因此已经成为NLP中的主流架构。
        Transformer模型包括Encoder和Decoder两个部分。Encoder负责编码输入序列，Decoder负责生成输出序列。
        在编码过程中，每个词首先被转换为向量表示，然后通过多层的Encoder逐步传递信息，形成编码后的表示向量。
        在解码过程中，Decoder通过将编码后的表示向量与目标序列逐词匹配，生成输出序列。
        Transformer的核心技术是self-attention，它通过计算输入序列中不同位置之间的相关性，得到每个单词的权重，从而更好地捕捉输入序列中的重要信息。

    ChatGPT(Generative Pre-trained Transformer)
        由 OpenAI 开发的著名对话式 AI 模型。它本身属于“生成式”、“预训练”、“Transformer”模型，其核心架构是“仅解码器（Decoder-only）”，因此它的生成方式是“自回归（AR）”的。ChatGPT 的革命性之处不仅在于其巨大的模型规模，更在于它通过“指令微调”和“基于人类反馈的强化学习（RLHF）”等对齐技术，使其能更好地理解人类意图并进行流畅、有帮助的对话。同类：Google Bard, Anthropic Claude

    全连接层（Fully Connected Layer）
        简称 FC 层，也常被称为密集层（Dense Layer），是神经网络中最基本的一种层。在该层中，每一个输入神经元都与该层的所有输出神经元相连接。每个连接都有一个独立的权重。全连接层执行的操作本质上是一个线性变换（矩阵乘法），通常后面会跟着一个激活函数引入非线性。

    前馈神经网络（Feedforward Neural Network, FNN）
        指信息在网络中单向流动，从输入层经过隐藏层（如果有的话）到达输出层，没有任何循环或反馈连接。FNN可以包含各种类型的层，例如全连接层、卷积层、池化层等，只要信息流是单向的。它是一个非常通用的术语，描述了神经网络信息流动的基本方向。

    多层感知机(Multi-Layer Perceptron, MLP)
        是一种全连接的前馈神经网络。这意味着MLP中的每一层神经元都与前一层的所有神经元完全连接。MLP至少包含三层：一个输入层、一个或多个隐藏层和一个输出层。常在隐藏层中使用非线性激活函数，这使得MLP能够学习和表示复杂的非线性关系。最初的“感知机”是指一个单层网络，而“多层感知机”则扩展了这一概念，引入了隐藏层。

    神经架构搜索 (Neural Architecture Search, NAS)
        一种自动化机器学习（AutoML）技术，专注于自动设计神经网络的结构。传统上，网络结构（如层的类型、数量、连接方式）由人类专家手动设计，而 NAS 则通过算法来自动搜索一个在特定任务上表现最优的架构。它通常包含三大组件：定义所有可能架构的“搜索空间”，用于探索该空间的“搜索策略”（如强化学习、演化算法），以及用于评估每个被搜索到的架构性能的“评估策略”。

    监督学习 (Supervised Learning)
        模型从 有标签（labeled）的数据中学习。数据集中的每个样本都包含一个输入特征（input）和一个期望的输出标签（output/label）。
        学习一个从输入到输出的映射函数。模型通过比较自己的预测输出和真实标签之间的差异（即损失），来不断调整自身参数，直到能对未见过的新输入做出准确的预测。
        监督学习主要解决两大类问题：分类和回归。分类是预测一个离散的类别。例如：判断一封邮件是否是垃圾邮件（二分类），或识别一张图片中的动物是猫、狗还是鸟（多分类）。逻辑回归（Logistic Regression）是解决这类问题的经典算法之一。
        回归 (Regression)是预测一个连续的数值。例如：根据房屋的面积、位置等特征预测其价格。线性回归（Linear Regression）就是解决这类问题的经典算法之一。
        监督学习的算法非常丰富，包括
        用于分类的 **逻辑回归（Logistic Regression）**、**决策树（Decision Tree）**、支持向量机（SVM）、K近邻（KNN）等。
        用于回归的 **线性回归（Linear Regression）**、决策树回归、支持向量回归等。

    非监督学习 (Unsupervised Learning)
        模型从没有标签的数据中学习。它只能接触到输入数据，必须自己从中发现隐藏的结构、模式或关系。主要任务包括：

        -   **聚类 (Clustering)**：将相似的数据点分到同一个簇（cluster）中。例如：根据用户的购买行为将其划分为不同的客户群体。
        -   **降维 (Dimensionality Reduction)**：在保留数据主要信息的前提下，减少数据的特征数量。例如：主成分分析（PCA）。

    Semi-Supervised Learning (SSL)
        半监督学习，一种机器学习范式，它结合了少量有标签数据和大量无标签数据进行训练。当获取大量标注数据成本高昂时，半监督学习能够利用易于获取的无标签数据来提升模型的性能和泛化能力。常见的技术包括自训练、协同训练和一致性正则化等。

    自监督学习 (Self-Supervised Learning, SSL)
        近年来推动大语言模型发展的关键技术，可以看作是无监督学习的一种特殊形式。同样使用没有标签的海量数据，但它巧妙地从数据本身中自动创建伪标签，从而将问题转化为一个监督学习问题来进行训练。自监督学习使得从未经标注的、海量的互联网文本中学习通用语言知识成为可能。
        通过解决一个精心设计的“借口任务（pretext task）”，来让模型学习到关于数据内在结构的、有价值的表示。好比一个学生拿到一张被撕碎的报纸，他通过学习如何将碎片拼接回完整的报纸（借口任务），从而学会了语法、词汇和常识（学到了表示）。
        在 LLM 中的应用：

        - **掩码语言模型 (Masked Language Modeling)**：随机遮盖句子中的一些词，然后让模型预测被遮盖的词是什么（BERT 使用的方式）。
        - **下一词预测 (Next Token Prediction)**：根据一段文本的前半部分，让模型预测下一个词是什么（GPT 使用的方式）。

    RL
        Reinforcement Learning，强化学习。模型（称为智能体 Agent）通过与一个环境进行交互来学习。智能体在环境中做出动作，环境会相应地改变 **状态** 并反馈给智能体一个奖励或惩罚。
        模型学习一个最优策略，即在什么状态下应该采取什么动作，以最大化长期累积的总奖励。好比训练一只宠物。当它做出正确的动作时，给它零食（正奖励）；当它做出错误动作时，不给奖励或进行轻微的惩罚。
        :term:`RLHF` 是强化学习的一种。

    基于人类反馈的强化学习
        RLHF(Reinforcement Learning from Human Feedback)，这是对齐（Align）大语言模型、使其回答更有用、更无害的关键技术。人类对模型生成的多个回答进行偏好排序，这些排序被用来训练一个“奖励模型”，然后用这个奖励模型作为环境，通过强化学习来微调语言模型，使其更倾向于生成人类偏好的内容。
