.. _glossary:

术语
====

.. glossary::

    机器学习
        人工智能的一个分支，它通过算法分析数据，从中学习模式，并利用这些模式对新数据进行预测或决策。

    DNN
        深度神经网络(Deep Neural Network)，包含多个隐藏层的神经网络。

    NLP
        Natural Language Processing，自然语言处理。

    指令微调
        Instruction Tuning

    线性回归
        Linear Regression，用于解决回归问题，即预测一个连续的数值输出。线性回归的目标是找到一条直线（或超平面），使其尽可能地拟合训练数据。

    逻辑回归
        Logistic Regression，用于解决分类问题。它通过一个 Sigmoid (Logistic) 函数将线性模型的输出映射到 (0, 1) 区间，表示属于某个类别的概率。

    MLP
        Multi-Layer Perceptron，多层感知机。

    生成式AI（Generative AI）
        机器产生复杂有结构的物件，尽乎无法穷举。如文章、图像、语音等。不是生成式AI：分类（如垃圾邮件分类、猫狗分类）。

    多模态（multi modality）
        模态指信息的类型或格式，如文本、图像、音频、视频等。多模态指 AI 模型能够同时理解、处理和关联多种不同模态信息的能力。GPT-4V 就是一个典型的多模态模型。

    AR
        自回归生成(Autoregressive Generation)。这是一种生成序列数据（如文本）的方式，即逐个生成序列中的元素（token），并且每生成一个新的元素，都要依赖于所有在它之前已经生成的元素。这就像我们写句子一样，下一个词总是基于前面已经写好的内容。GPT 系列模型就是典型的自回归模型。这种方式生成的文本质量高、连贯性好，但缺点是速度较慢，因为必须串行生成。

    NAR
        非自回归生成(Non-Autoregressive Generation)。与自回归相反，这种方式试图一次性或并行地生成整个序列的所有元素，而不是逐个生成。例如，模型可能先预测目标句子的长度，然后同时填充所有位置的词语。这种方式生成速度极快，但通常会牺牲一定的文本质量和连贯性，因为它在预测某个位置的词时，并不知道其他位置的词是什么。

    Transformer
        2017年在Attention Is All You Need论文中作为机器翻译的序列到序列模型提出的一种著名的深度学习模型。
        基于Transformer的预训练模型在各种自然语言处理任务上实现了最优性能，因此已经成为NLP中的主流架构。
        Transformer模型包括Encoder和Decoder两个部分。Encoder负责编码输入序列，Decoder负责生成输出序列。
        在编码过程中，每个词首先被转换为向量表示，然后通过多层的Encoder逐步传递信息，形成编码后的表示向量。
        在解码过程中，Decoder通过将编码后的表示向量与目标序列逐词匹配，生成输出序列。
        Transformer的核心技术是self-attention，它通过计算输入序列中不同位置之间的相关性，得到每个单词的权重，从而更好地捕捉输入序列中的重要信息。

    ChatGPT(Generative Pre-trained Transformer)
        由 OpenAI 开发的著名对话式 AI 模型。它本身属于“生成式”、“预训练”、“Transformer”模型，其核心架构是“仅解码器（Decoder-only）”，因此它的生成方式是“自回归（AR）”的。ChatGPT 的革命性之处不仅在于其巨大的模型规模，更在于它通过“指令微调”和“基于人类反馈的强化学习（RLHF）”等对齐技术，使其能更好地理解人类意图并进行流畅、有帮助的对话。同类：Google Bard, Anthropic Claude

    全连接层（Fully Connected Layer）
        简称 FC 层，也常被称为密集层（Dense Layer），是神经网络中最基本的一种层。在该层中，每一个输入神经元都与该层的所有输出神经元相连接。每个连接都有一个独立的权重。全连接层执行的操作本质上是一个线性变换（矩阵乘法），通常后面会跟着一个激活函数引入非线性。

    前馈神经网络（Feedforward Neural Network, FNN）
        指信息在网络中单向流动，从输入层经过隐藏层（如果有的话）到达输出层，没有任何循环或反馈连接。FNN可以包含各种类型的层，例如全连接层、卷积层、池化层等，只要信息流是单向的。它是一个非常通用的术语，描述了神经网络信息流动的基本方向。

    多层感知机(Multi-Layer Perceptron, MLP)
        是一种全连接的前馈神经网络。这意味着MLP中的每一层神经元都与前一层的所有神经元完全连接。MLP至少包含三层：一个输入层、一个或多个隐藏层和一个输出层。常在隐藏层中使用非线性激活函数，这使得MLP能够学习和表示复杂的非线性关系。最初的“感知机”是指一个单层网络，而“多层感知机”则扩展了这一概念，引入了隐藏层。

    神经架构搜索 (Neural Architecture Search, NAS)
        一种自动化机器学习（AutoML）技术，专注于自动设计神经网络的结构。传统上，网络结构（如层的类型、数量、连接方式）由人类专家手动设计，而 NAS 则通过算法来自动搜索一个在特定任务上表现最优的架构。它通常包含三大组件：定义所有可能架构的“搜索空间”，用于探索该空间的“搜索策略”（如强化学习、演化算法），以及用于评估每个被搜索到的架构性能的“评估策略”。

    监督学习 (Supervised Learning)
        模型从 有标签（labeled）的数据中学习。数据集中的每个样本都包含一个输入特征（input）和一个期望的输出标签（output/label）。
        学习一个从输入到输出的映射函数。模型通过比较自己的预测输出和真实标签之间的差异（即损失），来不断调整自身参数，直到能对未见过的新输入做出准确的预测。
        监督学习主要解决两大类问题：分类和回归。分类是预测一个离散的类别。例如：判断一封邮件是否是垃圾邮件（二分类），或识别一张图片中的动物是猫、狗还是鸟（多分类）。逻辑回归（Logistic Regression）是解决这类问题的经典算法之一。
        回归 (Regression)是预测一个连续的数值。例如：根据房屋的面积、位置等特征预测其价格。线性回归（Linear Regression）就是解决这类问题的经典算法之一。
        监督学习的算法非常丰富，包括
        用于分类的 **逻辑回归（Logistic Regression）**、**决策树（Decision Tree）**、支持向量机（SVM）、K近邻（KNN）等。
        用于回归的 **线性回归（Linear Regression）**、决策树回归、支持向量回归等。

    非监督学习 (Unsupervised Learning)
        模型从没有标签的数据中学习。它只能接触到输入数据，必须自己从中发现隐藏的结构、模式或关系。主要任务包括：

        -   **聚类 (Clustering)**：将相似的数据点分到同一个簇（cluster）中。例如：根据用户的购买行为将其划分为不同的客户群体。
        -   **降维 (Dimensionality Reduction)**：在保留数据主要信息的前提下，减少数据的特征数量。例如：主成分分析（PCA）。

    Semi-Supervised Learning (SSL)
        半监督学习，一种机器学习范式，它结合了少量有标签数据和大量无标签数据进行训练。当获取大量标注数据成本高昂时，半监督学习能够利用易于获取的无标签数据来提升模型的性能和泛化能力。常见的技术包括自训练、协同训练和一致性正则化等。

    自监督学习 (Self-Supervised Learning, SSL)
        近年来推动大语言模型发展的关键技术，可以看作是无监督学习的一种特殊形式。同样使用没有标签的海量数据，但它巧妙地从数据本身中自动创建伪标签，从而将问题转化为一个监督学习问题来进行训练。自监督学习使得从未经标注的、海量的互联网文本中学习通用语言知识成为可能。
        通过解决一个精心设计的“借口任务（pretext task）”，来让模型学习到关于数据内在结构的、有价值的表示。好比一个学生拿到一张被撕碎的报纸，他通过学习如何将碎片拼接回完整的报纸（借口任务），从而学会了语法、词汇和常识（学到了表示）。
        在 LLM 中的应用：

        - **掩码语言模型 (Masked Language Modeling)**：随机遮盖句子中的一些词，然后让模型预测被遮盖的词是什么（BERT 使用的方式）。
        - **下一词预测 (Next Token Prediction)**：根据一段文本的前半部分，让模型预测下一个词是什么（GPT 使用的方式）。

    强化学习
        RL(Reinforcement Learning)。模型（称为智能体 Agent）通过与一个环境进行交互来学习。智能体在环境中做出动作，环境会相应地改变 **状态** 并反馈给智能体一个奖励或惩罚。
        模型学习一个最优策略，即在什么状态下应该采取什么动作，以最大化长期累积的总奖励。好比训练一只宠物。当它做出正确的动作时，给它零食（正奖励）；当它做出错误动作时，不给奖励或进行轻微的惩罚。
        :term:`基于人类反馈的强化学习` 是强化学习的一种。

    基于人类反馈的强化学习
        RLHF(Reinforcement Learning from Human Feedback)，这是对齐（Align）大语言模型、使其回答更有用、更无害的关键技术。人类对模型生成的多个回答进行偏好排序，这些排序被用来训练一个“奖励模型”，然后用这个奖励模型作为环境，通过强化学习来微调语言模型，使其更倾向于生成人类偏好的内容。

    Hugging Face
        一个专注于NLP和机器学习的社区和公司。它提供了著名的 `transformers` 开源库，其中包含大量预训练模型。Hugging Face Hub 也是一个模型、数据集和机器学习应用的共享平台。

    ASR
        Automatic Speech Recognition，自动语音识别。将语音信号转换为对应的文本内容的技术。Wav2Vec2 是一种流行的 ASR 模型架构。

    ViT
        Vision Transformer，一种将 Transformer 架构应用于计算机视觉任务的模型。ViT 将图像划分为固定大小的补丁（patches），并将这些补丁视为序列输入到 Transformer 中进行处理。ViT 在图像分类等任务上表现出色，

    ConvNeXT
        ConvNeXT 是一种现代卷积神经网络架构，结合了传统卷积网络和 Transformer 的优点。它通过引入更深的网络层次结构和改进的训练技术，实现了在图像分类任务上的卓越性能。

    DETR
        Detection Transformer，一种基于 Transformer 的目标检测模型。DETR 通过将目标检测任务转化为一个序列到序列的预测问题，利用 Transformer 的自注意力机制来捕捉图像中的全局上下文信息，从而实现高效的目标检测。

    Mask2Former
        Mask2Former 是一种用于图像分割任务的先进模型。它结合了掩码预测和 Transformer 架构，能够同时处理语义分割、实例分割和全景分割任务。Mask2Former 利用多尺度特征融合和自注意力机制，实现了对复杂场景中对象的精确分割。

    GLPN
        GLPN (Global-Local Path Network) 是一种用于深度估计的神经网络架构。它结合了全局和局部信息，通过多路径设计来捕捉图像中的细节和整体结构，从而实现高精度的深度预测。

    BERT
        Bidirectional Encoder Representations from Transformers，一种基于 Transformer 编码器的预训练语言模型。BERT 通过双向上下文学习，能够更好地理解词语在句子中的含义，在各种 NLP 任务中表现出色。

    GPT
        Generative Pre-trained Transformer，一种基于 Transformer 解码器的生成式语言模型。

    Top-K Sampling
        一种在生成文本时控制多样性的方法。在每一步生成词元时，模型会先计算出词汇表中所有词元的概率分布，然后只从概率最高的 K 个词元中进行抽样。例如，如果 K=10，模型就只会从最可能的 10 个词中选择下一个词。这种方法可以过滤掉非常不可能的词，但当 K 值设置不当，可能会限制模型的创造力。

    Top-P (Nucleus) Sampling
        另一种更动态的控制文本生成多样性的方法。它不是选择固定数量的词元，而是选择一个概率总和加起来刚好超过某个阈值 P (例如 P=0.95) 的最小词元集合。模型从这个“核心”集合中进行抽样。例如，如果 P=0.9，模型会按概率从高到低选择词元，直到这些词元的累积概率达到 0.9，然后从这个集合中抽样。这种方法比 Top-K 更灵活，当模型对下一个词非常确定时，核心集合会很小；当模型不确定时，核心集合会变大，从而允许更多可能性。

    BART
        Bidirectional and Auto-Regressive Transformers，一种结合了 BERT 和 GPT 优点的序列到序列预训练模型。BART 通过编码器-解码器架构，能够高效地处理文本生成和理解任务。

    Batch Normalization (BN)
        一种在神经网络训练中用于加速收敛、稳定训练过程的技术。它对一个 mini-batch 内的所有样本，在每一个特征维度上进行归一化，使其均值为0，方差为1。这种方法严重依赖于 batch size，在 batch 较小时效果不佳，且在训练和推理时行为不一致，因此在 Transformer 模型中较少使用。

    Layer Normalization (LN)
        另一种归一化技术，与 Batch Normalization 的主要区别在于它的归一化维度。它对单个样本的所有特征（即一个层的所有神经元）进行归一化。这种方式与 batch size 无关，在训练和推理时行为一致，因此特别适用于处理变长序列的 RNN 和 Transformer 模型。

    SSM
        State Space Models (状态空间模型)。Transformer架构的自注意力机制，计算和内存的复杂度会随着序列长度的增加而呈二次方增长，使得它在处理超长序列时效率低下。
        状态空间模型是一种新兴的序列模型架构，在保持强大性能的同时实现更高的计算效率。
        Mamba 是 SSM 架构的代表，通过引入选择性机制和硬件感知的算法设计，实现了对长序列数据的高效处理。Mamba 能够根据输入内容动态地决定哪些信息是重要的，应该被保留在状态中；哪些信息是次要的，可以被忽略。这使得模型能更灵活地压缩和记忆序列信息。Mamba 的计算复杂度随序列长度呈线性增长。这意味着在处理长文本、高分辨率图像或音频等长序列数据时，Mamba 相比 Transformer 具有显著的速度和内存优势。

    Vibe Coding
        氛围驱动编程，是一种直观、即兴的开发方式。开发者或 AI Agent 根据一个相对模糊的目标或“感觉”（Vibe）开始工作，没有详尽的前期规划，
        通过与 LLM 的连续对话、不断的试错和快速迭代来逐步构建和修正解决方案。它依赖于 LLM 的“创造力”和上下文理解能力来“即兴发挥”。
        优点是能够快速适应需求的变化，在问题边界不清晰或需要快速验证想法时非常有效。但其结果难以保证，对于复杂的、需要高可靠性的任务，Vibe Coding 的成功率较低。由于缺少清晰的计划和步骤，当出现问题时，很难定位错误的根源。

    Spec-Driven Development
        规范驱动开发。人类开发者与 AI 一起创建一份详细的需求文档，明确任务的目标、功能、输入/输出、关键组件、约束条件和验收标准。AI Agent 根据规范生成一个分步的、可执行的行动计划。
        Agent 严格按照计划执行每一步（如生成代码、运行命令、执行测试），并持续验证中间结果是否符合规范。清晰的规范和计划大大降低了任务失败的风险，确保最终产出符合预期。整个开发过程有条不紊，每一步的进展都清晰可见。

    Vision-Language Models
        视觉语言模型，是多模态领域最主要的发展方向，它旨在让 LLM 具备强大的图像理解能力。其核心架构通常是将一个强大的图像编码器与一个预训练好的大语言模型连接起来。
        图像编码器（如 ViT, Vision Transformer）负责将输入的图像转换成一系列的特征向量，这些向量可以被看作是描述图像内容的图像词元 (Image Tokens)。
        这些图像词元与用户的文本提示词元一起被送入 LLM。LLM 在其统一的表示空间中同时处理这两种模态的信息，从而实现对图像内容的理解，并生成相关的文本描述、回答或分析。
        代表模型有：

        - **GPT-4V(ision)**：展现了强大的零样本图像理解能力，能够完成复杂的视觉推理任务。
        - **LLaVA (Large Language and Vision Assistant)**：一个著名的开源项目，通过在图像-文本对数据上进行指令微调，将 ViT 和 Vicuna (一个 LLM) 连接起来。
