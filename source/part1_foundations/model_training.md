# 模型训练

神经网络的学习过程，即 **训练**，本质上是一个参数优化的过程，目标是找到一组最佳的权重和偏置，使得网络对给定输入的预测结果与真实标签之间的损失最小。

这个过程通常通过 **梯度下降（Gradient Descent）** 的算法来迭代完成，其核心循环包括四个关键步骤：

1.  **前向传播（Forward Propagation）**：将一批训练数据输入网络，数据从输入层流向输出层，最终得到预测结果。
2.  **计算损失（Loss Calculation）**：使用一个**损失函数**（如均方误差或交叉熵）来量化预测结果与真实标签之间的差距。
3.  **反向传播（Backpropagation）**：这是学习过程的核心。算法会从输出层开始，反向计算损失函数相对于网络中每一个权重和偏置的梯度（偏导数）。
4.  **参数更新（Weight Update）**：使用优化器（如 Adam、SGD）根据计算出的梯度来更新网络中的所有权重和偏置，以期在下一步减小损失。

## 训练过程的关键概念

- **Batch (批次)**：由于整个数据集可能非常大，一次性将其全部加载到内存中并计算梯度是不现实的。因此通常将数据集分成若干个小的子集，这些子集被称为批次。
- **Iteration (迭代)**：指完成一次批次数据的前向传播、反向传播和参数更新的过程。这是参数更新的最小单位。
- **Epoch (轮次)**：指整个训练数据集中的所有样本都已经被模型“过”了一遍。例如，如果一个数据集有 1000 个样本，批次大小（Batch Size）为 100，那么一个 Epoch 就包含 10 次迭代（1000 / 100 = 10）。

### 批次的采样策略

在将数据集划分为多个批次时，标准的做法是 **无放回采样 (Sampling without Replacement)**，它保证每个样本在一个 Epoch 中恰好被使用一次。

1.  **随机打乱**：在一个 Epoch 开始时，将整个数据集的索引进行一次完全的随机打乱。
2.  **顺序切分**：然后，按照打乱后的新顺序，从头到尾依次将数据切分成一个个批次。

这个过程好比“洗一副牌，然后依次发牌”，确保了每个样本在一个轮次中都会被看到且仅看到一次，这能确保模型稳定、完整地学习整个数据集。

## 权重初始化 (Weight Initialization)

在训练开始前，模型的权重必须被初始化为一个初始值。这个看似简单的步骤对模型的收敛速度和最终性能至关重要。糟糕的初始化可能导致梯度消失或梯度爆炸。

- **为什么需要初始化？**：如果所有权重都初始化为 0，网络中的所有神经元将学习到完全相同的特征，这被称为“对称性”问题。随机初始化可以打破这种对称性。

- **Xavier (Glorot) 初始化**：一种经典的初始化方法。它的核心思想是使每一层的激活值的方差和梯度的方差在前向和反向传播中保持一致。这有助于防止梯度信号在网络中传播时变得过大或过小。它通常与 Sigmoid 和 Tanh 等激活函数配合使用效果较好。

- **He 初始化**：这是对 Xavier 初始化的改进，专门为 **ReLU** 及其变体设计。由于 ReLU 会将一半的输入置为零，He 初始化在计算方差时考虑了这一点，从而更适合 ReLU 激活函数，成为现代神经网络中非常常用的初始化方法。